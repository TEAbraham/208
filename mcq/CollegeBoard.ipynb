{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "362d579e-f950-463b-b779-51339f6709a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "def get_total_question_count(soup):\n",
    "    try:\n",
    "        count_text = soup.select_one(\"div.h-\\\\[32px\\\\]\").text.strip()\n",
    "        match = re.search(r\"of\\s+(\\d+)\", count_text)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to get total question count: {e}\")\n",
    "    return None\n",
    "\n",
    "def wrap_latex_if_needed(text):\n",
    "    \"\"\"\n",
    "    Wrap a math expression in $...$ if it's inline, or $$...$$ if it's on its own line.\n",
    "    \"\"\"\n",
    "    stripped = text.strip()\n",
    "    if re.match(r\"^[-+*/()=0-9a-zA-Z.\\\\^ ]+$\", stripped):\n",
    "        return f\"$$ {stripped} $$\"\n",
    "    return f\"$ {stripped} $\"\n",
    "\n",
    "def convert_mml_to_latex(text):\n",
    "    \"\"\"\n",
    "    Convert embedded MathJax or MathML alttext to LaTeX format with appropriate wrapping.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    for math in soup.select(\"mjx-assistive-mml\"):\n",
    "        if math.has_attr(\"alttext\"):\n",
    "            latex = wrap_latex_if_needed(math[\"alttext\"].strip())\n",
    "            math.insert_before(latex)\n",
    "            math.decompose()\n",
    "    return soup.get_text(\" \", strip=True)\n",
    "\n",
    "def scrape_all_questions(driver, total_questions):\n",
    "    i = 1\n",
    "    unit_question_counts = {}\n",
    "    \n",
    "    while total_questions is None or i <= total_questions:\n",
    "        print(f\"\\nüß† Scraping {question_id} of {total_questions if total_questions else '???'}\")        \n",
    "        \n",
    "        # Determine unit number from current page\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        unit_tag = soup.find(\"h3\", string=\"Unit And Topic\")\n",
    "        if not unit_tag:\n",
    "            print(\"‚ö†Ô∏è Unit not found, skipping.\")\n",
    "            break\n",
    "\n",
    "        unit_list = unit_tag.find_next(\"ul\")\n",
    "        unit_items = unit_list.find_all(\"li\")\n",
    "        unit_text = unit_items[0].text.strip() if unit_items else \"unitX\"\n",
    "        unit_match = re.search(r\"(\\d+)\", unit_text)\n",
    "        unit_folder = f\"unit{unit_match.group(1)}\" if unit_match else \"unitX\"\n",
    "\n",
    "        # Count existing JSON files to resume numbering\n",
    "        if unit_folder not in unit_question_counts:\n",
    "            os.makedirs(unit_folder, exist_ok=True)\n",
    "            image_folder = os.path.join(unit_folder, \"images\")\n",
    "            os.makedirs(image_folder, exist_ok=True)\n",
    "            existing_files = [f for f in os.listdir(unit_folder) if f.endswith(\".json\") and f.startswith(\"q\")]\n",
    "            existing_ids = [int(re.search(r\"q(\\d+).json\", f).group(1)) for f in existing_files if re.search(r\"q(\\d+).json\", f)]\n",
    "            unit_question_counts[unit_folder] = max(existing_ids) + 1 if existing_ids else 1\n",
    "\n",
    "        question_num = unit_question_counts[unit_folder]\n",
    "        question_id = f\"q{question_num:04d}\"\n",
    "\n",
    "        success = scrape_question(driver, question_id)\n",
    "        if not success:\n",
    "            print(\"‚úÖ Finished scraping all questions or hit end.\")\n",
    "            break\n",
    "            \n",
    "        unit_question_counts[unit_folder] += 1\n",
    "        i += 1\n",
    "        \n",
    "def scrape_question(driver, question_id):\n",
    "    output = {\n",
    "        \"question_id\": question_id,\n",
    "        \"title\": \"\",\n",
    "        \"difficulty\": \"\",\n",
    "        \"question_text\": \"\",\n",
    "        \"image_files\": [],\n",
    "        \"choices\": [],\n",
    "        \"correct_answer\": \"\",\n",
    "        \"solution\": \"\",\n",
    "        \"distractor_explanations\": {},\n",
    "        \"tags\": [],\n",
    "        \"standards\": [],\n",
    "        \"unit\": \"\",\n",
    "        \"topic\": \"\",\n",
    "        \"stimulus_type\": \"\",\n",
    "        \"table_html\": \"\"\n",
    "    }\n",
    "\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    time.sleep(1.5)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        title_tag = soup.select_one(\"h2 span.question-text\")\n",
    "        if title_tag:\n",
    "            output[\"title\"] = title_tag.text.strip().replace(\"Q: \", \"\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        unit_topic_block = soup.find(\"h3\", string=\"Unit And Topic\")\n",
    "        if unit_topic_block:\n",
    "            unit_list = unit_topic_block.find_next(\"ul\")\n",
    "            unit_items = unit_list.find_all(\"li\")\n",
    "            if unit_items:\n",
    "                output[\"unit\"] = unit_items[0].text.strip()\n",
    "                if len(unit_items) > 1:\n",
    "                    output[\"topic\"] = unit_items[1].text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to extract unit/topic: {e}\")\n",
    "\n",
    "    try:\n",
    "        tag_lis = soup.select(\".ItemMetadata__tags li\")\n",
    "        for li in tag_lis:\n",
    "            if \":\" in li.text:\n",
    "                key = li.text.split(\":\")[0].strip()\n",
    "                output[\"tags\"].append(key)\n",
    "\n",
    "            if \"Question Difficulty\" in li.text:\n",
    "                difficulty_value = li.text.split(\":\")[-1].strip()\n",
    "                if difficulty_value == \"Emerging\":\n",
    "                    output[\"difficulty\"] = \"Easy\"\n",
    "                elif difficulty_value == \"Proficient\":\n",
    "                    output[\"difficulty\"] = \"Moderate\"\n",
    "                elif difficulty_value == \"Advanced\":\n",
    "                    output[\"difficulty\"] = \"Difficult\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to extract tags: {e}\")\n",
    "\n",
    "    try:\n",
    "        stim_label = soup.find(\"label\", string=re.compile(\"Stimulus Type\"))\n",
    "        if stim_label:\n",
    "            parent_li = stim_label.find_parent(\"li\")\n",
    "            if parent_li:\n",
    "                stim_type = parent_li.text.split(\":\")[-1].strip()\n",
    "                output[\"stimulus_type\"] = stim_type\n",
    "\n",
    "                unit_num = re.search(r\"(\\d+)\", output[\"unit\"])\n",
    "                unit_folder = f\"unit{unit_num.group(1)}\" if unit_num else \"unitX\"\n",
    "                image_folder = os.path.join(unit_folder, \"images\")\n",
    "                os.makedirs(image_folder, exist_ok=True)\n",
    "\n",
    "                def download_image(img_url, suffix):\n",
    "                    ext = os.path.splitext(urlparse(img_url).path)[1] or \".png\"\n",
    "                    local_filename = f\"{question_id}-{suffix}{ext}\"\n",
    "                    local_path = os.path.join(image_folder, local_filename)\n",
    "                    if not os.path.exists(local_path):\n",
    "                        try:\n",
    "                            img_data = requests.get(img_url).content\n",
    "                            with open(local_path, \"wb\") as f:\n",
    "                                f.write(img_data)\n",
    "                            print(f\"üñº Downloaded image: {local_filename}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"‚ö†Ô∏è Failed to download image: {e}\")\n",
    "                    else:\n",
    "                        print(f\"üñº Image already exists: {local_filename}\")\n",
    "                    return f\"images/{local_filename}\"\n",
    "\n",
    "                if stim_type.lower() == \"table\":\n",
    "                    table_wrapper = soup.find(\"div\", class_=\"table_wrapper\")\n",
    "                    if table_wrapper:\n",
    "                        output[\"table_html\"] = str(table_wrapper)\n",
    "\n",
    "                elif stim_type.lower() == \"graph\":\n",
    "                    image_tag = soup.select_one(\".standalone_image img\")\n",
    "                    if image_tag and image_tag.has_attr(\"src\"):\n",
    "                        img_path = download_image(image_tag[\"src\"], \"graph\")\n",
    "                        output[\"image_files\"].append(img_path)\n",
    "\n",
    "                elif stim_type.lower() == \"plot\":\n",
    "                    stimulus_block = soup.select_one(\".lrn_stimulus_content\")\n",
    "                    if stimulus_block:\n",
    "                        image_tag = stimulus_block.find(\"img\")\n",
    "                        if image_tag and image_tag.has_attr(\"src\"):\n",
    "                            img_path = download_image(image_tag[\"src\"], \"plot\")\n",
    "                            output[\"image_files\"].append(img_path)\n",
    "\n",
    "                elif stim_type.lower() == \"computer output\":\n",
    "                    table_wrapper = soup.find(\"div\", class_=\"table_wrapper\")\n",
    "                    if table_wrapper:\n",
    "                        output[\"table_html\"] = str(table_wrapper)\n",
    "                    else:\n",
    "                        image_tag = soup.select_one(\".stimulus_reference img\")\n",
    "                        if image_tag and image_tag.has_attr(\"src\"):\n",
    "                            img_path = download_image(image_tag[\"src\"], \"computer-output\")\n",
    "                            output[\"image_files\"].append(img_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to extract stimulus/table/graph/plot/computer output: {e}\")\n",
    "\n",
    "    try:\n",
    "        question_block = soup.select_one(\".lrn_stimulus_content\")\n",
    "        if question_block:\n",
    "            output[\"question_text\"] = convert_mml_to_latex(str(question_block))\n",
    "\n",
    "        choice_blocks = soup.select(\".lrn-mcq-option\")\n",
    "        for choice in choice_blocks:\n",
    "            label = choice.select_one(\".lrn-possible-answer\")\n",
    "            if label:\n",
    "                mml = label.select_one(\"mjx-assistive-mml\")\n",
    "                if mml and mml.has_attr(\"alttext\"):\n",
    "                    latex = wrap_latex_if_needed(mml[\"alttext\"].strip())\n",
    "                    output[\"choices\"].append(latex)\n",
    "                    if \"--correct\" in choice.get(\"class\", []):\n",
    "                        output[\"correct_answer\"] = latex\n",
    "                else:\n",
    "                    text = label.get_text(strip=True)\n",
    "                    output[\"choices\"].append(text)\n",
    "                    if \"--correct\" in choice.get(\"class\", []):\n",
    "                        output[\"correct_answer\"] = text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to extract question/choices/answer: {e}\")\n",
    "\n",
    "    try:\n",
    "        sol_block = soup.select_one(\".LearnosityDistractor.--valid .content\")\n",
    "        if sol_block:\n",
    "            output[\"solution\"] = convert_mml_to_latex(str(sol_block))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to extract solution: {e}\")\n",
    "\n",
    "    unit_num = re.search(r\"(\\d+)\", output[\"unit\"])\n",
    "    unit_folder = f\"unit{unit_num.group(1)}\" if unit_num else \"unitX\"\n",
    "    os.makedirs(unit_folder, exist_ok=True)\n",
    "    out_path = os.path.join(unit_folder, f\"{question_id}.json\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Saved {question_id} to {out_path}\")\n",
    "\n",
    "    try:\n",
    "        next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'button[data-cy=\"next-button\"]')))\n",
    "        next_button.click()\n",
    "        print(\"‚û°Ô∏è Clicked Next\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to click Next button: {e}\")\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b08b7d52-9b86-46da-b99e-6a0617ce4476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Log in manually and navigate to the first question. Press ENTER to begin scraping... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total questions to scrape: 967\n",
      "\n",
      "üß† Scraping question-001 of 967\n",
      "‚úÖ Saved question-001 to unit5/question-001.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-002 of 967\n",
      "‚úÖ Saved question-002 to unit8/question-002.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-003 of 967\n",
      "‚úÖ Saved question-003 to unit2/question-003.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-004 of 967\n",
      "‚úÖ Saved question-004 to unit8/question-004.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-005 of 967\n",
      "‚úÖ Saved question-005 to unit1/question-005.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-006 of 967\n",
      "‚úÖ Saved question-006 to unit6/question-006.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-007 of 967\n",
      "‚úÖ Saved question-007 to unit7/question-007.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-008 of 967\n",
      "‚úÖ Saved question-008 to unit1/question-008.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-009 of 967\n",
      "‚úÖ Saved question-009 to unit9/question-009.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-010 of 967\n",
      "üñº Downloaded graph image: question-010-graph.png\n",
      "‚úÖ Saved question-010 to unit2/question-010.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-011 of 967\n",
      "‚úÖ Saved question-011 to unit7/question-011.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-012 of 967\n",
      "‚úÖ Saved question-012 to unit3/question-012.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-013 of 967\n",
      "‚úÖ Saved question-013 to unit5/question-013.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-014 of 967\n",
      "‚úÖ Saved question-014 to unit8/question-014.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-015 of 967\n",
      "‚úÖ Saved question-015 to unit4/question-015.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-016 of 967\n",
      "‚úÖ Saved question-016 to unit7/question-016.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-017 of 967\n",
      "‚úÖ Saved question-017 to unit6/question-017.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-018 of 967\n",
      "‚úÖ Saved question-018 to unit9/question-018.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-019 of 967\n",
      "‚úÖ Saved question-019 to unit6/question-019.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-020 of 967\n",
      "‚úÖ Saved question-020 to unit6/question-020.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-021 of 967\n",
      "‚úÖ Saved question-021 to unit7/question-021.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-022 of 967\n",
      "‚úÖ Saved question-022 to unit2/question-022.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-023 of 967\n",
      "‚úÖ Saved question-023 to unit4/question-023.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-024 of 967\n",
      "‚úÖ Saved question-024 to unit6/question-024.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-025 of 967\n",
      "‚úÖ Saved question-025 to unit2/question-025.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-026 of 967\n",
      "‚úÖ Saved question-026 to unit5/question-026.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-027 of 967\n",
      "‚úÖ Saved question-027 to unit9/question-027.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-028 of 967\n",
      "‚úÖ Saved question-028 to unit3/question-028.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-029 of 967\n",
      "‚úÖ Saved question-029 to unit1/question-029.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-030 of 967\n",
      "‚úÖ Saved question-030 to unit3/question-030.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-031 of 967\n",
      "‚úÖ Saved question-031 to unit3/question-031.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-032 of 967\n",
      "‚úÖ Saved question-032 to unit3/question-032.json\n",
      "‚û°Ô∏è Clicked Next\n",
      "\n",
      "üß† Scraping question-033 of 967\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m question_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müß† Scraping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_questions\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mtotal_questions\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m???\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m success \u001b[38;5;241m=\u001b[39m scrape_question(driver, question_id)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Finished scraping all questions or hit end.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 41\u001b[0m, in \u001b[0;36mscrape_question\u001b[0;34m(driver, question_id)\u001b[0m\n\u001b[1;32m     22\u001b[0m output \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: question_id,\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable_html\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m }\n\u001b[1;32m     40\u001b[0m wait \u001b[38;5;241m=\u001b[39m WebDriverWait(driver, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1.5\u001b[39m)\n\u001b[1;32m     42\u001b[0m html \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n\u001b[1;32m     43\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Setup Selenium\n",
    "options = Options()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "driver = webdriver.Chrome(service=Service(), options=options)\n",
    "\n",
    "# MANUALLY LOG IN FIRST (Google login + navigate to first question)\n",
    "\n",
    "input(\"Log in manually and navigate to the first question. Press ENTER to begin scraping...\")\n",
    "\n",
    "\n",
    "# Step 1: Get total number of questions from header\n",
    "try:\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    total_questions = get_total_question_count(soup)\n",
    "    print(f\"üìä Total questions to scrape: {total_questions}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Failed to extract total question count: {e}\")\n",
    "    total_questions = None\n",
    "\n",
    "scrape_all_question(driver, total_questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72392df6-0f20-427e-81d1-ecc9df0a072c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
