{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4dc16a7-4968-4dcc-81e6-1fcec9cb7a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import html\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "tom = [\"Easy\", \"Medium\", \"Hard\"]\n",
    "albert = [\"Easy\", \"Moderate\", \"Difficult\"]\n",
    "difficulty = dict(zip(albert, tom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ed349a6-9bb7-4504-8eda-431a04fafff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up browser\n",
    "# options = Options()\n",
    "# options.add_argument(\"--start-maximized\")\n",
    "# driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# # Step 1: Visit a site and log in manually if needed\n",
    "# driver.get(\"https://albert.io/log-in\")  # or any target domain\n",
    "\n",
    "# input(\"üîê Log in manually and press ENTER here when done...\")\n",
    "\n",
    "# # Step 2: Save cookies to a file\n",
    "# cookies = driver.get_cookies()\n",
    "\n",
    "# with open(\"cookies_albert.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(cookies, f, indent=2)\n",
    "\n",
    "# print(\"‚úÖ Cookies saved to cookies_albert.json\")\n",
    "\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89e496b2-8ca4-4d01-84bc-ad48e786f062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional: Set up options\n",
    "options = Options()\n",
    "options.add_argument('--start-maximized')\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get('https://albert.io')  # Must visit domain before adding cookies\n",
    "\n",
    "# Load and clean cookies\n",
    "with open('cookies_albert.json', 'r') as f:\n",
    "    cookies = json.load(f)\n",
    "\n",
    "for cookie in cookies:\n",
    "    # Clean invalid sameSite values\n",
    "    if 'sameSite' in cookie and cookie['sameSite'] not in ['Strict', 'Lax', 'None']:\n",
    "        del cookie['sameSite']  # or cookie['sameSite'] = 'Lax'\n",
    "\n",
    "    # Remove unsupported fields\n",
    "    cookie.pop('hostOnly', None)\n",
    "    cookie.pop('storeId', None)\n",
    "    cookie.pop('session', None)\n",
    "\n",
    "    # Fix expiry format if present\n",
    "    if 'expiry' in cookie and not isinstance(cookie['expiry'], int):\n",
    "        del cookie['expiry']\n",
    "\n",
    "    # Ensure cookie has required keys\n",
    "    if 'name' in cookie and 'value' in cookie:\n",
    "        try:\n",
    "            driver.add_cookie(cookie)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Skipping cookie '{cookie.get('name')}' ‚Üí {e}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Invalid cookie format skipped: {cookie}\")\n",
    "\n",
    "# Reload page with cookies applied\n",
    "driver.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ddb26-683d-4981-b53a-bbd2d03eaf80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wrap_latex_if_needed(text):\n",
    "    stripped = text.strip()\n",
    "    if re.match(r\"^[-+*/()=0-9a-zA-Z.\\\\^ ]+$\", stripped):\n",
    "        return f\"$$ {stripped} $$\"\n",
    "    return f\"$ {stripped} $\"\n",
    "\n",
    "def convert_mml_to_latex(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    for math in soup.select(\"mjx-assistive-mml\"):\n",
    "        if math.has_attr(\"alttext\"):\n",
    "            latex = wrap_latex_if_needed(math[\"alttext\"].strip())\n",
    "            math.insert_before(latex)\n",
    "            math.decompose()\n",
    "    return soup.get_text(\" \", strip=True)\n",
    "\n",
    "def scrape_question(driver, wait):\n",
    "    output = {\n",
    "        \"question_id\": \"\",\n",
    "        \"unit\": \"\",\n",
    "        \"topics\":[],\n",
    "        \"title\": \"\",\n",
    "        \"difficulty\": \"\",\n",
    "        \"question_text\": \"\",\n",
    "        \"image_files\": [],\n",
    "        \"tables\": [],\n",
    "        \"choices\": [],\n",
    "        \"correct_answer\": \"\",\n",
    "        \"solution\": \"\",\n",
    "        \"explanations\": [],\n",
    "        \"tags\": [],\n",
    "    }\n",
    "\n",
    "    show_btn = wait.until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[data-testid='practice-view__toggle-solution-btn']\"))\n",
    "    )\n",
    "    show_btn.click()\n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \".markdown-renderer-v2\")))\n",
    "    print(\"‚úÖ Solution revealed.\")\n",
    "    \n",
    "    # Get unit info\n",
    "    unit_el = driver.find_element(By.CSS_SELECTOR, \".pv-title-bar__title .paragraph\")\n",
    "    unit_text = unit_el.text.strip()\n",
    "    match = re.search(r\"Unit (\\d+)\", unit_text)\n",
    "    if match:\n",
    "        unit_num = match.group(1)\n",
    "        output[\"unit\"] = unit_text\n",
    "        folder = f\"unit{unit_num}\"\n",
    "        image_folder = os.path.join(folder, \"images\")\n",
    "        os.makedirs(image_folder, exist_ok=True)\n",
    "        \n",
    "    num = len([f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))])\n",
    "\n",
    "    question_id = f\"question-{num:03d}\"\n",
    "    output['question_id'] = question_id\n",
    "    output[\"title\"] = driver.find_element(By.CSS_SELECTOR, \"h1.u-mar_0\").text\n",
    "    output[\"difficulty\"] = difficulty[driver.find_element(By.CSS_SELECTOR, \"span.a-text.content-primary.a-text--size-xs\").text]\n",
    "    \n",
    "    question_body = driver.find_element(By.CSS_SELECTOR, \"div.question-wrapper__body\")\n",
    "    markdown = question_body.find_element(By.CSS_SELECTOR, \"div.markdown-renderer-v2\")\n",
    "    output[\"question_text\"] = markdown.text\n",
    "    \n",
    "    img_tags = driver.find_elements(By.CSS_SELECTOR, \"img.image-supplement__image\")\n",
    "    for i, img in enumerate(img_tags):\n",
    "        img_url = img.get_attribute(\"src\")\n",
    "        ext = os.path.splitext(urlparse(img_url).path)[1] or \".png\"\n",
    "        local_filename = f\"{question_id}-img{i+1}{ext}\"\n",
    "        local_path = os.path.join(image_folder, local_filename)\n",
    "\n",
    "        try:\n",
    "            img_data = requests.get(img_url).content\n",
    "            with open(local_path, \"wb\") as f:\n",
    "                f.write(img_data)\n",
    "            output[\"image_files\"].append(f\"images/{local_filename}\")\n",
    "            print(f\"üñº Downloaded: {local_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Image download failed: {e}\")\n",
    "    \n",
    "    tables = markdown.find_elements(By.CSS_SELECTOR, \"div.markdown-table-wrapper\")\n",
    "    for table in tables:\n",
    "        output[\"tables\"].append(table.get_attribute(\"innerHTML\"))\n",
    "    \n",
    "    choice_elements = driver.find_elements(By.CSS_SELECTOR, \"label.mcq-option\")\n",
    "    for el in choice_elements:\n",
    "        letter = el.find_element(By.CSS_SELECTOR, \".mcq-option__letter\").text.strip()\n",
    "        content_blocks = el.find_elements(By.CSS_SELECTOR, \".mcq-option__content .paragraph\")\n",
    "        content = \"\\n\".join(p.text for p in content_blocks)\n",
    "        output[\"choices\"].append({\"letter\": letter, \"text\": content})\n",
    "\n",
    "\n",
    "    \n",
    "    explanation_heading = driver.find_element(By.XPATH, \"//h2[contains(text(), 'Explanation')]\")\n",
    "    explanation_block = explanation_heading.find_element(By.XPATH, \"following-sibling::div[contains(@class, 'markdown-renderer-v2')]\")\n",
    "    paragraphs = explanation_block.find_elements(By.CSS_SELECTOR, \"div.paragraph\")\n",
    "\n",
    "    correct_answer = \"\"\n",
    "    solution = []\n",
    "    distractors = {}\n",
    "\n",
    "    section = None\n",
    "\n",
    "    for i, p in enumerate(paragraphs):\n",
    "        inner_html = p.get_attribute(\"innerHTML\").strip().lower()\n",
    "\n",
    "        if i == 0:\n",
    "            correct_answer = p.text\n",
    "\n",
    "        elif \"<u>solution</u>\" in inner_html:\n",
    "            section = \"solution\"\n",
    "\n",
    "        elif \"<u>explanation of distractors</u>\" in inner_html:\n",
    "            section = \"distractors\"\n",
    "\n",
    "        elif section == \"solution\":\n",
    "            solution.append(p.text)\n",
    "\n",
    "        elif section == \"distractors\":\n",
    "            plain = p.text.strip()\n",
    "            match = re.search(r\"choice\\\\s+'([a-e])'\", plain.lower())\n",
    "            if match:\n",
    "                letter = match.group(1).upper()\n",
    "                distractors[letter] = p.text\n",
    "\n",
    "    output[\"correct_answer\"] = correct_answer\n",
    "    output[\"solution\"] = \"\\n\\n\".join(solution)\n",
    "    output[\"explanations\"] = distractors\n",
    "    \n",
    "    tag_section = driver.find_elements(By.CSS_SELECTOR, \"div.tags-standards-list.u-mar-b_2\")\n",
    "    tag_links = tag_section[0].find_elements(By.CSS_SELECTOR, \"a.a-chip\")\n",
    "    output[\"tags\"] = [t.text.strip() for t in tag_links]\n",
    "    if len(tag_section) > 1:\n",
    "        top_links = tag_section[1].find_elements(By.CSS_SELECTOR, \"a.a-chip\")\n",
    "        output[\"topics\"] = [t.text.strip() for t in top_links]\n",
    "    \n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    out_path = f\"{folder}/{question_id}.json\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Saved {question_id} to {out_path}\")\n",
    "    \n",
    "    if num % 20 == 0:\n",
    "        time.sleep(5)\n",
    "        print(\"I waited\")\n",
    "    \n",
    "    buttons = driver.find_elements(By.CSS_SELECTOR, \"button\")\n",
    "    next_chevron = next(\n",
    "        btn for btn in buttons if \"fa-chevron-right\" in btn.get_attribute(\"innerHTML\") and \"--disabled\" not in btn.get_attribute(\"class\")\n",
    "    )\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_chevron)\n",
    "    wait.until(EC.element_to_be_clickable(next_chevron))\n",
    "    next_chevron.click()\n",
    "    wait.until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"div.question-wrapper__body\"))\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "\n",
    "        print(\"‚û°Ô∏è Clicked chevron-right to load next question.\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not click chevron-right: {e}\")\n",
    "        return False\n",
    "\n",
    "        \n",
    "pages = ['//*[@id=\"-unit-1-exploring-onevariable-data\"]','//*[@id=\"-unit-2-exploring-twovariable-data\"]', '//*[@id=\"-unit-3-collecting-data\"]', '//*[@id=\"-unit-4-probability-random-variables-and-probability-distributions\"]', '//*[@id=\"-unit-5-sampling-distributions\"]', '//*[@id=\"-unit-6-inference-for-categorical-data-proportions\"]', '//*[@id=\"-unit-7-inference-for-quantitative-data-means\"]', '//*[@id=\"-unit-8-inference-for-categorical-data-chisquare\"]', '//*[@id=\"-unit-9-inference-for-quantitative-data-slopes\"]']\n",
    "\n",
    "for page in pages:\n",
    "    driver.get('https://albert.io/ap-statistics')\n",
    "    wait = WebDriverWait(driver, 5)\n",
    "    select = wait.until(EC.element_to_be_clickable((By.XPATH, page)))\n",
    "    select.click()\n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"span.questions-list-header__heading\")))\n",
    "    \n",
    "    total_text = driver.find_element(By.CSS_SELECTOR, \"span.questions-list-header__heading\").text\n",
    "    match = re.search(r\"(\\d+)\\s+questions\", total_text.lower())\n",
    "    total_questions = int(match.group(1)) if match else None\n",
    "    total_questions = total_questions\n",
    "    print(f\"üìä Total questions found: {total_questions}\")\n",
    "    \n",
    "    i = 1\n",
    "    while total_questions is None or i <= total_questions:\n",
    "\n",
    "        print(f\"\\nüß† Scraping {i} of {total_questions if total_questions else '???'}\")\n",
    "        success = scrape_question(driver, wait)\n",
    "\n",
    "        if not success:\n",
    "            print(\"‚úÖ Finished scraping all questions or hit end.\")\n",
    "            break\n",
    "\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bff88b-9fb4-4b5e-8f1a-ae44da0d24b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
