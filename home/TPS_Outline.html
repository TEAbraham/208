<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mr.&nbsp;Abraham">

<title>AP Statistics Outline</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="TPS_Outline_files/libs/clipboard/clipboard.min.js"></script>
<script src="TPS_Outline_files/libs/quarto-html/quarto.js"></script>
<script src="TPS_Outline_files/libs/quarto-html/popper.min.js"></script>
<script src="TPS_Outline_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="TPS_Outline_files/libs/quarto-html/anchor.min.js"></script>
<link href="TPS_Outline_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="TPS_Outline_files/libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="TPS_Outline_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="TPS_Outline_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="TPS_Outline_files/libs/bootstrap/bootstrap-c0367b04c37547644fece4185067e4a7.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">AP Statistics Outline</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Mr.&nbsp;Abraham </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="unit-1-exploring-one-variable-data" class="level2">
<h2 class="anchored" data-anchor-id="unit-1-exploring-one-variable-data">UNIT 1 Exploring One-Variable Data</h2>
<section id="part-i-data-analysis" class="level3">
<h3 class="anchored" data-anchor-id="part-i-data-analysis">Part I Data Analysis</h3>
<p>Introduction Section 1A Statistics: The Language of Variation<br>
Section 1B Displaying and Describing Categorical Data<br>
Section 1C Displaying Quantitative Data with Graphs<br>
Section 1D Describing Quantitative Data with Numbers<br>
Unit 1, Part I Review</p>
</section>
<section id="part-ii-modeling-distributions-of-quantitative-data" class="level3">
<h3 class="anchored" data-anchor-id="part-ii-modeling-distributions-of-quantitative-data">Part II Modeling Distributions of Quantitative Data</h3>
<p>Introduction<br>
Section 1E Describing Position and Transforming Data<br>
Section 1F Normal Distributions<br>
Unit 1, Part II Review</p>
</section>
</section>
<section id="unit-2-exploring-two-variable-data" class="level2">
<h2 class="anchored" data-anchor-id="unit-2-exploring-two-variable-data">UNIT 2 Exploring Two-Variable Data</h2>
<p>Introduction<br>
Section 2A Relationships Between Two Categorical Variables<br>
Section 2B Relationships Between Two Quantitative Variables<br>
Section 2C Linear Regression Models<br>
Section 2D Analyzing Departures from Linearity<br>
Unit 2 Review</p>
</section>
<section id="unit-3-collecting-data" class="level2">
<h2 class="anchored" data-anchor-id="unit-3-collecting-data">UNIT 3 Collecting Data</h2>
<p>Introduction<br>
Section 3A Introduction to Data Collection<br>
Section 3B Sampling and Surveys<br>
Section 3C Experiments<br>
Unit 3 Review</p>
</section>
<section id="unit-4-probability-random-variables-and-probability-distributions" class="level2">
<h2 class="anchored" data-anchor-id="unit-4-probability-random-variables-and-probability-distributions">UNIT 4 Probability, Random Variables, and Probability Distributions</h2>
<section id="part-i-probability" class="level3">
<h3 class="anchored" data-anchor-id="part-i-probability">Part I Probability</h3>
<p>Introduction<br>
Section 4A Randomness, Probability, and Simulation<br>
Section 4B Probability Rules<br>
Section 4C Conditional Probability and Independent Events<br>
Unit 4, Part I Review</p>
</section>
<section id="part-ii-random-variables-and-probability-distributions" class="level3">
<h3 class="anchored" data-anchor-id="part-ii-random-variables-and-probability-distributions">Part II Random Variables and Probability Distributions</h3>
<p>Introduction<br>
Section 4D Introduction to Discrete Random Variables<br>
Section 4E Transforming and Combining Random Variables<br>
Section 4F Binomial and Geometric Random Variables<br>
Unit 4, Part II Review</p>
</section>
</section>
<section id="unit-5-sampling-distributions" class="level2">
<h2 class="anchored" data-anchor-id="unit-5-sampling-distributions">UNIT 5 Sampling Distributions</h2>
<p>Introduction<br>
Section 5A Normal Distributions, Revisited<br>
Section 5B What is a Sampling Distribution?<br>
Section 5C Sample Proportions<br>
Section 5D Sample Means<br>
Unit 5 Review</p>
</section>
<section id="unit-6-inference-for-categorical-data-proportions" class="level2">
<h2 class="anchored" data-anchor-id="unit-6-inference-for-categorical-data-proportions">UNIT 6 Inference for Categorical Data: Proportions</h2>
<section id="part-i-inference-for-one-sample" class="level3">
<h3 class="anchored" data-anchor-id="part-i-inference-for-one-sample">Part I: Inference for One Sample</h3>
<p>Introduction<br>
Section 6A Confidence Intervals: The Basics<br>
Section 6B Confidence Intervals for a Population Proportion<br>
Section 6C Significance Tests: The Basics<br>
Section 6D Significance Tests for a Population Proportion<br>
Unit 6, Part I Review</p>
</section>
<section id="part-ii-inference-for-two-samples" class="level3">
<h3 class="anchored" data-anchor-id="part-ii-inference-for-two-samples">Part II: Inference for Two Samples</h3>
<p>Introduction<br>
Section 6E Confidence Intervals for a Difference in Population Proportions<br>
Section 6F Significance Tests for a Difference in Population Proportions<br>
Unit 6, Part II Review</p>
</section>
</section>
<section id="unit-7-inference-for-quantitative-data-means" class="level2">
<h2 class="anchored" data-anchor-id="unit-7-inference-for-quantitative-data-means">UNIT 7 Inference for Quantitative Data: Means</h2>
<p>Introduction<br>
Section 7A Confidence Intervals for a Population Mean or Mean Difference<br>
Section 7B Significance Tests for a Population Mean or Mean Difference<br>
Section 7C Confidence Intervals for a Difference in Population Means<br>
Section 7D Significance Tests for a Difference in Population Means<br>
Unit 7 Review</p>
</section>
<section id="unit-8-inference-for-categorical-data-chi-square" class="level2">
<h2 class="anchored" data-anchor-id="unit-8-inference-for-categorical-data-chi-square">UNIT 8 Inference for Categorical Data: Chi-Square</h2>
<p>Section 8A Chi-Square Tests for Goodness of Fit<br>
Section 8B Chi-Square Tests for Independence or Homogeneity<br>
Unit 8 Review</p>
</section>
<section id="unit-9-inference-for-quantitative-data-slopes" class="level2">
<h2 class="anchored" data-anchor-id="unit-9-inference-for-quantitative-data-slopes">UNIT 9 Inference for Quantitative Data: Slopes</h2>
<p>Introduction<br>
Section 9A Confidence Intervals for the Slope of a Population Regression Line<br>
Section 9B Significance Tests for the Slope of a Population Regression Line<br>
Unit 9 Review</p>
</section>
<section id="unit-1-exploring-one-variable-data-1" class="level1">
<h1>UNIT 1: Exploring One-Variable Data</h1>
<section id="part-i-data-analysis-1" class="level2">
<h2 class="anchored" data-anchor-id="part-i-data-analysis-1">Part I: Data Analysis</h2>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>We live in a world of <em>data</em>. Every day, the media report poll results, outcomes of medical studies, and analyses of data on everything from gasoline prices to elections to weather to the latest technology. These data are trying to tell us a story. To understand what the data are saying, we use <strong>statistics</strong>.</p>
<p>A solid understanding of statistics will help you make informed decisions based on data in your daily life.</p>
</section>
<section id="section-1a-statistics-the-language-of-variation" class="level3">
<h3 class="anchored" data-anchor-id="section-1a-statistics-the-language-of-variation">Section 1A: Statistics: The Language of Variation</h3>
<ul>
<li><strong>Statistics</strong> is the science and art of collecting, analyzing and drawing conclusions from data</li>
<li>A data set contains information about a number of <strong>individuals</strong>. Individuals may be people, animals, or things. For each individual, the data give values for one or more <strong>variables</strong>. A variable is a characteristic that can take different values for different individuals.</li>
<li>A <strong>categorical variable</strong> takes values that are labels, which place each individual into a particular group, called a category. A <strong>quantitative variable</strong> takes numerical values that count or measure some characteristic of each individual.</li>
<li>The <strong>distribution</strong> of a variable describes what values the variable takes and how often it takes each value.</li>
<li>To summarize the distribution of a variable, you can use a <strong>frequency table</strong> that shows the number of individuals having each value or a <strong>relative frequency table</strong> that shows the proportion or percentage of individuals having each value. You can use a frequency table or a relative frequency table to help describe the distribution of a variable.</li>
</ul>
</section>
<section id="section-1b-displaying-and-describing-categorical-data" class="level3">
<h3 class="anchored" data-anchor-id="section-1b-displaying-and-describing-categorical-data">Section 1B: Displaying and Describing Categorical Data</h3>
<ul>
<li>A <strong>bar graph</strong> or <strong>pie chart</strong> can be used to display the distribution of a categorical variable. When examining any graph, ask yourself, “What do I see?”</li>
<li>You can use a <strong>side-by-side bar graph</strong> to compare the distribution of one categorical variable for two or more groups. Be sure to use relative frequencies when comparing groups of different sizes.<br>
</li>
<li>Beware of graphs that mislead the eye. Look at the scales to see if they have been distorted to create a particular impression. Avoid making graphs that replace the bars of a graph with pictures whose height and width both change.</li>
</ul>
</section>
<section id="section-1c-displaying-quantitative-data-with-graphs" class="level3">
<h3 class="anchored" data-anchor-id="section-1c-displaying-quantitative-data-with-graphs">Section 1C: Displaying Quantitative Data with Graphs</h3>
<ul>
<li>There are two types of quantitative variables: discrete and continuous. A <strong>discrete variable</strong> can take a countable set of possible numeric values with gaps between them on the number line. A <strong>continuous variable</strong> can take any value in an interval on the number line. Discrete variables usually result from counting something; continuous variables usually result from measuring something.</li>
<li>You can use a <strong>dotplot</strong>, <strong>stemplot</strong>, or <strong>histogram</strong> to display the distribution of a quantitative variable. A dotplot displays individual data values on a number line. Stemplots separate each data value into a stem and a one-digit leaf. Histograms plot the frequencies (counts) or relative frequencies (proportions or percentages) of values in equal-width intervals.</li>
<li>When examining any graph of quantitative data, look for an <em>overall pattern</em> and for clear <em>departures</em> from that pattern. <strong>Shape</strong>, <strong>center</strong>, and <strong>variability</strong> describe the overall pattern of the distribution of a quantitative variable. <strong>Outliers</strong> are observations that lie outside the overall pattern of a distribution.<br>
</li>
<li>Some distributions have simple shapes, such as <strong>roughly symmetric</strong>, <strong>skewed to the left</strong>, or <strong>skewed to the right</strong>. A distribution in which the frequency (relative frequency) of each possible value is about the same is <strong>approximately uniform</strong>.<br>
</li>
<li>The number of peaks is another aspect of overall shape. So are distinct clusters and gaps. A single-peaked graph is sometimes called <em>unimodal</em>, and adouble-peaked graph is sometimes called <em>bimodal</em>.</li>
<li>Dotplots, back-to-back stemplots, and histograms can also be used to compare distributions of quantitative data. When comparing the distribution of a quantitative variable in two or more groups, be sure to compare shape, outliers, center, and variability.</li>
<li>Histograms are for quantitative data; bar graphs are for categorical data. In both types of graphs, be sure to use relative frequencies when comparing data sets of different sizes.</li>
</ul>
</section>
<section id="section-1d-displaying-quantitative-data-with-numbers" class="level3">
<h3 class="anchored" data-anchor-id="section-1d-displaying-quantitative-data-with-numbers">Section 1D: Displaying Quantitative Data with Numbers</h3>
<ul>
<li>A numerical summary of a distribution of quantitative data should include measures of <em>center</em> and <em>variability</em>.</li>
<li>The <strong>mean</strong> and the <strong>median</strong> measure the center of a distribution in different ways. The median is the midpoint of the distribution, the number such that about half of the observations are smaller and half are larger. The mean is the average of the observations. In symbols, the sample mean is given by <span class="math inline">\(\bar{x} = \frac{\sum{x_i}}{n}\)</span>.</li>
<li>A <strong>statistic</strong> is a number that describes a sample. A <strong>parameter</strong> is a number that describes a population. We often use statistics (like the sample mean <span class="math inline">\(\bar{x}\)</span>) to estimate parameters (like the population mean <span class="math inline">\(\mu\)</span>).</li>
<li>The simplest measure of variability for a distribution of quantitative data is the <strong>range</strong>, which is the distance from the minimum value to the maximum value.</li>
<li>When you use the mean to describe the center of a distribution, use the <strong>standard deviation</strong> to measure variability. The standard deviation gives the typical distance of the values in a distribution from the mean. In symbols, the sample standard deviation is given by <span class="math inline">\(s_x = \sqrt{\frac{\sum{(x_i-\bar{x})^2}}{n-1}}\)</span>. The value obtained before taking the square root is known as the <em>sample variance</em>, denoted by <span class="math inline">\(s_x^2\)</span>. The standard deviation <span class="math inline">\(s_x\)</span> is <span class="math inline">\(0\)</span> when there is no variability and gets larger as variability from the mean increases.</li>
<li>When you use the median to describe the center of a distribution, use the <strong>interquartile range (IQR)</strong> to describe the distribution’s variability. The <strong>first quartile <span class="math inline">\(Q_1\)</span></strong> has about one-fourth of the individual data values at or below it, and the <strong>third quartile <span class="math inline">\(Q_3\)</span></strong> has about three-fourths of the individual data values at or below it. The interquartile range measures variability in the middle half of the distribution and is found by calculating <span class="math inline">\(IQR = Q_3 - Q_1\)</span>.</li>
<li>The median is a <strong>resistant</strong> measure of center because it is relatively unaffected by extreme values. The mean is not resistant. Among measures of variability, the <span class="math inline">\(IQR\)</span> is resistant, but the range and standard deviation are not.</li>
<li>The mean and standard deviation are good descriptions for roughly symmetric distributions with no outliers. The median and <span class="math inline">\(IQR\)</span> are a better description for skewed distributions or distributions with outliers.</li>
<li>The most common method of identifying outliers in a distribution of quantitative data is the <strong><span class="math inline">\(1.5 \times IQR\)</span> rule</strong>. According to this rule, an individual data value is an outlier if it is less than <span class="math inline">\(Q_1 - 1.5 \times IQR\)</span> or greater than <span class="math inline">\(Q_3 + 1.5 \times IQR\)</span>. Another method for identifying outliers is the <span class="math inline">\(2 \times SD\)</span> rule, which says that any data value more than <span class="math inline">\(2\)</span> standard deviations from the mean of the distribution is an outlier.</li>
<li>The <strong>five-number summary</strong> of a distribution consists of the minimum, <span class="math inline">\(Q_1\)</span>, the median, <span class="math inline">\(Q_3\)</span>, and the maximum. A <strong>boxplot</strong> displays the five-number summary, marking outliers with a special symbol. The box shows the variability in the middle half of the distribution. The median is marked within the box. Lines extend from the box to the smallest and largest observations that are not outliers. Boxplots are helpful for comparing the center (median) and variability (range, <span class="math inline">\(IQR\)</span>) of multiple distributions. Boxplots aren’t as useful for identifying the shape of a distribution because they do not display peaks, clusters, gaps, and other interesting features.</li>
</ul>
</section>
<section id="unit-1-part-i-review" class="level3">
<h3 class="anchored" data-anchor-id="unit-1-part-i-review">Unit 1, Part I Review</h3>
<section id="section-1a-statistics-the-language-of-variation-1" class="level4">
<h4 class="anchored" data-anchor-id="section-1a-statistics-the-language-of-variation-1">Section 1A: Statistics; The Language of Variation</h4>
<p>In this brief section, you learned how to identify the <strong>individuals</strong> and <strong>variables</strong> in a data set. You also learned how to distinguish between <strong>categorical variables</strong> and <strong>quantitative variables</strong>, as well as how to summarize the <strong>distribution</strong> of a variable with a <strong>frequency table</strong> or <strong>relative frequency table</strong>.</p>
</section>
<section id="section-1b-displaying-and-describing-categorical-data-1" class="level4">
<h4 class="anchored" data-anchor-id="section-1b-displaying-and-describing-categorical-data-1">Section 1B: Displaying and Describing Categorical Data</h4>
<p>In this section, you learned how to display the distribution of a categorical variable with a <strong>bar graph</strong> or a <strong>pie chart</strong>, and what to look for when describing these graphs. Next, you learned how to compare the distribution of a categorical variable in two or more groups using <strong>side-by-side bar graphs</strong>. Remember to properly label your graphs! Poor labeling is an easy way to lose credit on the AP Statistics exam. You should also be able to recognize misleading graphs and be careful to avoid making misleading graphs yourself.</p>
</section>
<section id="section-1c-displaying-quantitative-data-with-graphs-1" class="level4">
<h4 class="anchored" data-anchor-id="section-1c-displaying-quantitative-data-with-graphs-1">Section 1C: Displaying Quantitative Data with Graphs</h4>
<p>In this section, you learned how to make three different types of graphs for displaying quantitative data: <strong>dotplots</strong>, <strong>stemplots</strong>, and <strong>histograms</strong>. Each of the graphs has distinct benefits, but all of them are good tools for examining the distribution of a quantitative variable. Dotplots and stemplots are handy for smaller sets of data. Histograms are the best choice when there are a large number of data values. On the AP Statistics exam, you will be expected to make, interpret, and describe each of these types of graphs.</p>
<p>When you are describing the distribution of a quantitative variable, you should look at a graph of the data to determine the overall pattern (<strong>shape</strong>, <strong>center</strong>, <strong>variability</strong>) and striking departures from that pattern (<strong>outliers</strong>). When comparing distributions of quantitative data, you should include explicit comparison words for center and variability such as “is greater than” or “is approximately the same as.” When asked to compare distributions, a very common mistake on the AP Statistics exam is describing the characteristics of each distribution separately without making these explicit comparisons.</p>
</section>
<section id="section-1d-describing-quantitative-data-with-numbers" class="level4">
<h4 class="anchored" data-anchor-id="section-1d-describing-quantitative-data-with-numbers">Section 1D: Describing Quantitative Data with Numbers</h4>
<p>To measure the <em>center</em> of a distribution of quantitative data, you learned how to calculate the <strong>median</strong> and the <strong>mean</strong> of a distribution. You also learned that the median is a <strong>resistant</strong> measure of center, but the mean isn’t resistant because it can be greatly affected by skewness or outliers.</p>
<p>To measure the <em>variability</em> of a distribution of quantitative data, you learned how to calculate the <strong>range</strong>, <strong>standard deviation</strong>, and <strong>interquartile range (<span class="math inline">\(IQR\)</span>)</strong>. The range measures the distance from the minimum value to the maximum value, while the standard deviation measures the typical distance of values in the distribution from the mean. The range and standard deviation are not resistant–both are heavily affected by extreme values. The interquartile range (<span class="math inline">\(IQR\)</span>) is a resistant measure of variability because it ignores the upper <span class="math inline">\(25\%\)</span> and lower <span class="math inline">\(25\%\)</span> of the distribution.</p>
<p>To identify outliers in a distribution of quanititative data, you learned the <strong><span class="math inline">\(1.5 \times IQR\)</span> rule</strong> and the less common <em><span class="math inline">\(2 \times SD\)</span> rule</em>. You also learned that <strong>boxplots</strong> are a great way to visually summarize a distribution of quantitative data. Boxplots are helpful for comparing the center (Median) and the variability (range, <span class="math inline">\(IQR\)</span>) for multiple distributions. Boxplots aren’t as useful for identifying the shape of a distribution because they do not display peaks, clusters, gaps, and other interesting features.</p>
</section>
</section>
</section>
<section id="part-ii-modelling-distributions-of-quantitative-data" class="level2">
<h2 class="anchored" data-anchor-id="part-ii-modelling-distributions-of-quantitative-data">Part II: Modelling Distributions of Quantitative Data</h2>
<section id="section-1e-describing-position-and-transforming-data" class="level3">
<h3 class="anchored" data-anchor-id="section-1e-describing-position-and-transforming-data">Section 1E: Describing Position and Transforming Data</h3>
<ul>
<li>Two ways of describing an individual value’s position in a distribution of quantitative data are <strong>percentiles</strong> and <strong>standardized scores (z-scores)</strong>.
<ul>
<li>An individual’s percentile is the percentage of values in a distribution that are less than or equal to the individual’s data value.</li>
<li>To standardize any data value, subtract the mean of the distribution and then divide the difference by the standard deviation. The resulting standardized score (z-score) <span class="math display">\[z = \frac{\text{value} -\text{mean}}{\text{standard deviation}}\]</span> measures how many standard deviations the data value lies above or below the mean of the distribution.</li>
</ul></li>
<li>We can use percentiles and z-scores to compare the relative positions of individuals in one or more distributions of quantitative data.</li>
<li>A <strong>cumulative relative frequency graph</strong> allows you to estimate the percentile for a specific value and the value corresponding to a given percentile in a distribution of quantitative data.</li>
<li>It is necessary to <strong>transform data</strong> when changing units of measurement.
<ul>
<li>When you add a positive constant <em>a</em> to (or subtract <em>a</em> from) all the values in a quantitative data set, measures of center increase (decrease) by <em>a</em>. Measures of variability do not change, nor does the shape of the distribution.</li>
<li>When you multiply (or divide) all the values ina a quantitative data set by a positive constant <em>b</em>, measures of center and variability are multiplied (divided) by <em>b</em>. However, the shape of the distribution does not change.<br>
</li>
<li>A common transformation is to standardize all the values in a distribution: for each value, subtract the mean of the distribution and then divide the difference by the standard deviation to get its z-score. The distribution of standardized scores (z-scores) has the same shape as the original distribution, a mean of <span class="math inline">\(0\)</span>, and a standard deviation of <span class="math inline">\(1\)</span>.</li>
</ul></li>
</ul>
</section>
<section id="section-1f-normal-distributions" class="level3">
<h3 class="anchored" data-anchor-id="section-1f-normal-distributions">Section 1F: Normal Distributions</h3>
<ul>
<li><p>Some distributions of quantitative data can be modeled by symmetric, single-peaked, mound-shaped <strong>normal curves</strong>.</p></li>
<li><p>Any <strong>normal distribution</strong> is completely specified by two numbers: its mean and its standard deviation. The mean <span class="math inline">\(\mu\)</span> is the center of the distribution and the standard deviation <span class="math inline">\(\sigma\)</span> is the distance from <span class="math inline">\(\mu\)</span> to the change-of-curvature points on either side.</p></li>
<li><p>An area under a normal curve above any interval of values on the horizontal axis estimates the proportion of values in the distribution that fall within that interval.</p></li>
<li><p>The <strong>empirical rule</strong> describes what percentage of observations in any normal distribution fall within <span class="math inline">\(1\)</span>, <span class="math inline">\(2\)</span>, and <span class="math inline">\(3\)</span> standard deviations of the mean: about <span class="math inline">\(68\%\)</span>, <span class="math inline">\(95\%\)</span>, and <span class="math inline">\(99.7\%\)</span>, respectively.<br>
</p></li>
<li><p>To assess normality for a distribution of quantitative data, we first observe the shape of a dotplot, stemplot, or histogram. Then we check how well the data fit the empirical rule.</p></li>
<li><p>All normal distributions are the same when values are standardized. If a quantitative variable can be modeled by a normal distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, we can standardize using <span class="math display">\[z = \frac{\text{value} -\text{mean}}{\text{standard deviation}} = \frac{x - \mu}{\sigma}\]</span> The standardized values can be modeled using the <strong>standard normal distribution</strong> with mean <span class="math inline">\(0\)</span> and standard deviation <span class="math inline">\(1\)</span>.</p></li>
<li><p>You can use technology or Table A in the back of the book to find areas or percentiles in any normal distribution. Table A gives percentiles for the standard normal distribution.<br>
</p></li>
<li><p>To find the area in a normal distribution corresponding to a given interval of values:</p>
<p><strong>Step 1: Draw a normal distribution</strong> with the horizontal axis labeled and scaled using the mean and standard deviation, the boundary value(s) clearly identified, and the area of interest shaded.<br>
<strong>Step 2: Perform calculations–show your work!</strong> Do one of the following:<br>
(i) Standardize each boundary value and use technology or Table A to find the desired area under the standard normal curve; or<br>
(ii) Use technology to find the desired area without standardizing. Label the inputs you used for the calculator command.<br>
Be sure to answer the question that was asked.<br>
</p></li>
<li><p>To find the value in a normal distribution corresponding to a given area (percentile):</p>
<p><strong>Step 1: Draw a normal distribution</strong> with the horizontal axis labeled and scaled using the mean and standard deviation, the area of interest shaded and labeled, and the unknown boundary value clearly marked.<br>
<strong>Step 2: Perform calculations–show your work!</strong> Do one of the following:<br>
(i) Use technology or Table A to find the value of <span class="math inline">\(z\)</span> with the indicated area under the standard normal curve, then “unstandardize” to transform back to the original distribution; or<br>
(ii) Use technology to find the desired area without standardizing. Label the inputs you used for the calculator command.<br>
Be sure to answer the question that was asked.<br>
</p></li>
<li><p>You can find the mean or standard deviation of a normal distribution using one or more percentiles by solving for the missing value in the z-score formula.</p></li>
</ul>
</section>
<section id="unit-1-part-ii-review" class="level3">
<h3 class="anchored" data-anchor-id="unit-1-part-ii-review">Unit 1, Part II Review</h3>
<section id="section-1e-describing-position-and-transforming-data." class="level4">
<h4 class="anchored" data-anchor-id="section-1e-describing-position-and-transforming-data.">Section 1E: Describing Position and Transforming Data.</h4>
<p>In this section, you learned two different ways to describe the <em>position</em> of individuals in a distribution of quantitative data: <strong>percentiles</strong> and <strong>standardized scores (z-scores)</strong>. Percentiles describe the position of an individual value in a distribution by measuring what percentage of the observations are less than or equal to that value. Standardized scores (z-scores) describe the position of an individual value in a distribution by measuring how many standard deviations the value is above or below the mean. To find the standardized score for a particular observation, transform the value by subtracting the mean and then dividing the difference by the standard deviation. Besides describing the position of an individual in a distribution, you cal also use percentiles and z-scores to compare the relative positions of individual values in one or more distributions of quantitative data.</p>
<p>A <strong>cumulative relative frequency graph</strong> is a handy tool for displaying percentiles in the distribution of a quantitative variable. You can use it to estimate the percentile for a particular value of the variable or to estimate the value of the variable at a particular percentile.</p>
<p>You also learned to describe the effects of <em>transforming data</em> on the shape, center, and variability of a distribution of quantitative data. Adding a positive constant to (or subtracting it from) each value in a data set changes measures of center, but not the shape or variability of the distribution. Multiplying or dividing each value in a data set by a positive costant changes measures of center and variability, but not the shape of the distribution.</p>
</section>
<section id="section-1f-normal-distribution" class="level4">
<h4 class="anchored" data-anchor-id="section-1f-normal-distribution">Section 1F: Normal Distribution</h4>
<p>In this section, you learned how a <strong>normal curve</strong> can be used to model some distributions of quantitative data. A <strong>normal distribution</strong> is symmetric, single-peaked, and mound-shaped with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. For a quantitative variable that can be modeled by a normal distribution, the area under the normal curve above an interval of values onthe horizontal axis estimates the proportion or percentage of data values in that interval. You learned how to draw a normal curve with the mean and the points that are <span class="math inline">\(1\)</span>, <span class="math inline">\(2\)</span>, and <span class="math inline">\(3\)</span> standard deviations from the mean labeled.</p>
<p>For any distribution that is approximately normal, about <span class="math inline">\(68\%\)</span> of the values will be within <span class="math inline">\(1\)</span> standard deviation of the mean, about <span class="math inline">\(95\%\)</span> of the values will be within <span class="math inline">\(2\)</span> standard deviations of the mean, and about <span class="math inline">\(99.7\%\)</span> of the values will be within <span class="math inline">\(3\)</span> standard deviations of the mean. This handy result is known as the <strong>empirical rule</strong>. You learned how to determine whether a distribution of quantitative data is approximately normal using graphs (dotplots, stemplots, histograms) and the empirical rule.</p>
<p>When values in a distribution do not fall exactly <span class="math inline">\(1\)</span>, <span class="math inline">\(2\)</span>, or <span class="math inline">\(3\)</span> standard deviations from the mean, you learned how to use technology or Table A to calculate the proportion of values in any specified interval under a normal curve. This includes areas to the left, areas to the right, and areas between two values in a normal distribution. You also learned how to determine the value of an individual that falls at a specified percentile in a normal distribution, and how to find the mean or standard deviation of a normal distribution given the value of a percentile.</p>
<p>On the AP Statistics exam, it is extremely important that you clearly communicate your methods when answering free-response questions that involve a normal distribution. Start by drawing a normal distribution with the horizontal axis labeled and scaled using the mean and standard deviation, the boundary value(s) clearly identified, and the area of interest shaded. Then perform calculations, being sure to show your work. If you use technology without standardizing as your method, be sure to label the inputs of your calculator command.</p>
</section>
</section>
</section>
</section>
<section id="unit-2-exploring-two-variable-data-1" class="level1">
<h1>UNIT 2: Exploring Two-Variable Data</h1>
<section id="introduction-1" class="level3">
<h3 class="anchored" data-anchor-id="introduction-1">Introduction</h3>
<p>Although we can gain many insights into the world around us by analyzing one variable at a time, investigating relationships between variables is central to what we do in statistics. When we understand the relationship between two variables, we can use the value of one variable to help us make predictions about the other variable. In this unit, we’ll start by exploring relationships between two <em>categorical</em> variables, such as membership in an environmental club and snowmobile use by visitors to Yellowstone National Park. In Sections 2B-2D, we investigate relationships between two <em>quantitative</em> variables, such as the number of miles a used car has been driven and its price.</p>
</section>
<section id="section-2a-relationships-between-two-categorical-variables" class="level3">
<h3 class="anchored" data-anchor-id="section-2a-relationships-between-two-categorical-variables">Section 2A: Relationships Between Two Categorical Variables</h3>
<ul>
<li>A <strong>response variable</strong> measures an outcome of a study. An <strong>explanatory variable</strong> may help predict or explain changes in a response variable.</li>
<li>A <strong>two-way table</strong> is a table of counts or relative frequencies that summarizes data on the relationship between two categorical variables for some group of individuals.</li>
<li>You can use a two-way table to calculate three types of relative frequencies:
<ul>
<li>A <strong>marginal relative frequency</strong> gives the percentage or proportion of individuals that have a specific value for one categorical variable. A marginal relative frequency is calculated by dividing a row or column total by the total for the entire two-way table.</li>
<li>A <strong>joint relative frequency</strong> gives the percentage or proportion of individuals that have a specific value for one categorical variable and a specific value for another categorical variable. A joint relative frequency is calculated by dividing the value in one cell by the total for the entire two-way table.</li>
<li>A <strong>conditional relative frequency</strong> gives the percentage or proportiona of individuals that have a specific value for one categorical variable among a group of individuals who share the same value of another categorical variable (the condition). A conditional relative frequency is calculated by dividing the value in one cell of a two-way table by the total for the appropriate row or column.</li>
</ul></li>
<li>Use a <strong>side-by-side bar graph</strong>, a <strong>segmented bar graph</strong>, or a <strong>mosaic plot</strong> to display the relationship between two categorical variables or compare the distribution of a categorical variable for two or more groups.</li>
<li>There is an <strong>association</strong> between two variables if knowing the value of one variable helps us to predict the value of the other. If knowing the value of one variable does not help us to predict the value of the other, then there is no association between the variables.</li>
</ul>
</section>
<section id="section-2b-relationships-between-two-quantitative-variables" class="level3">
<h3 class="anchored" data-anchor-id="section-2b-relationships-between-two-quantitative-variables">Section 2B: Relationships Between Two Quantitative Variables</h3>
<ul>
<li>A <strong>scatterplot</strong> shows the relationship between two quantitative variables measured for the same individuals. Each individual in the data set appears as a point in the graph. The values of the explanatory variable appear on the horizontal axis, and the values of the response variable appear on the vertical axis. If there is no explanatory variable, either variable can appear on the horizontal axis.</li>
<li>When describing a scatterplot, address the overall pattern (direction, form, strength) and departures from the pattern (unusual features) in context using the names of the variables.
<ul>
<li><strong>Direction</strong>: A relationship has a <strong>positive association</strong> when the values of one variable tend to increase as the values of the other variable increase, a <strong>negative association</strong> when the values of one variable tend to decrease at the values of the other variable increase, and <strong>no association</strong>, when knowing the value of one variable doesn’t help predict the value of the other variable.</li>
<li><strong>Form</strong>: A scatterplot can show a linear form or a non-linear form. The form is linear if the overall pattern follows a straight line. Otherwise, the form is non-linear.</li>
<li><strong>Strength</strong>: A scatter plot can show a weak, moderate, or strong association. And association is strong if the points closely follow a specific form. And association is weak if the points deviate quite a bit from the form identified.</li>
<li><strong>Unusual features</strong>: Look for individual points that fall outside the pattern and distinct clusters of points.</li>
</ul></li>
<li>The <strong>correlation <span class="math inline">\(r\)</span></strong> gives the direction and measures the strength of the linear association between two quantitative variables.
<ul>
<li>The value of the correlation is always between negative <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span> (<span class="math inline">\(-1\le r\le1\)</span>).</li>
<li>Correlation indicates the direction of a linear relationship by its sign: <span class="math inline">\(r &gt; 0\)</span> for a positive association and <span class="math inline">\(r &lt; 0\)</span> for a negative association.</li>
<li>strong, linear associations, have values of <span class="math inline">\(r\)</span> closer to <span class="math inline">\(1\)</span> or <span class="math inline">\(-1\)</span>. Correlations of <span class="math inline">\(r=1\)</span> and <span class="math inline">\(r=-1\)</span> occur only when the points on a scatterplot lie exactly on a straight line.</li>
<li>Correlation doesn’t measure form and should be used only to describe linear relationships.</li>
</ul></li>
<li>Correlation does not imply causation.</li>
<li>Calculate <span class="math inline">\(r\)</span> using technology or the following formula: <span class="math display">\[r = \frac{1}{n - 1} \sum\left(\frac{x_i-\bar{x}}{s_x}\right)\left(\frac{y_i-\bar{y}}{s_y}\right) = \frac{\sum{z_{x_i},z_{y_i}}}{n-1}\]</span></li>
<li>The correlation isn’t affected by reversing the variables or changing the units of either variable. The correlation has no units.</li>
</ul>
</section>
<section id="section-2c-linear-regression-models" class="level3">
<h3 class="anchored" data-anchor-id="section-2c-linear-regression-models">Section 2C: Linear Regression Models</h3>
<ul>
<li>The <strong>regression line</strong> <span class="math inline">\(\hat{y} = a + bx\)</span> models how a response variable, <span class="math inline">\(y\)</span> changes as an explanatory variable <span class="math inline">\(x\)</span> changes. The symbol, <span class="math inline">\(\hat{y}\)</span> indicates that the line gives the predicted value of <span class="math inline">\(y\)</span> for a particular value of <span class="math inline">\(x\)</span>, not the actual value of <span class="math inline">\(y\)</span>.</li>
<li>Avoid <strong>extrapolation</strong>, which is using a regression line to make predictions with values of the explanatory variable outside the interval of values used to calculate the line.</li>
<li>The <strong>slope <span class="math inline">\(b\)</span></strong> of a regression line <span class="math inline">\(\hat{y} = a + bx\)</span> describes how the predicted value of <span class="math inline">\(y\)</span> changes for each increase of one unit in <span class="math inline">\(x\)</span>.</li>
<li>The <strong>y intercept <span class="math inline">\(a\)</span></strong> of a regression line <span class="math inline">\(\hat{y} = a + bx\)</span> is the predicted value of <span class="math inline">\(y\)</span> when the explanatory variable <span class="math inline">\(x\)</span> equals <span class="math inline">\(0\)</span>. This prediction does not have a logical interpretation unless <span class="math inline">\(x\)</span> can actually take values near <span class="math inline">\(0\)</span>.</li>
<li>The <strong>least-squares regression line</strong> is the line that minimizes the sum of the squares of the vertical distances of the observed points from the line. Calculate the equation of the least-squares regression line using technology or with formulas.</li>
<li>You can examine the fit of a regression model by analyzing the <strong>residuals</strong>, which are the differences between the actual values of <span class="math inline">\(y\)</span> and the predicted values of <span class="math inline">\(y\)</span>: residual <span class="math inline">\(= y - \hat{y}\)</span>.</li>
<li>A <strong>residual plot</strong> is a scatterplot that plots the residuals on the vertical axis and the values of the explanatory variable (or the predicted <span class="math inline">\(y\)</span>–values) on the horizontal axis.
<ul>
<li>Random scatter in a residual plot, indicates that the regression model used to calculate the residuals is appropriate.</li>
<li>A leftover curved pattern in the residual plot indicates that the model isn’t appropriate.</li>
</ul></li>
<li>The <strong>coefficient of determination <span class="math inline">\(r^2\)</span></strong> is the percentage of the variation in the response variable that is accounted for by the least-squares regression line using a particular explanatory variable.</li>
<li>The <strong>standard deviation of the residuals <span class="math inline">\(s\)</span></strong> measures the typical size of a residual when using the least-squares regression line.</li>
</ul>
</section>
<section id="section-2d-analyzing-departures-from-linearity" class="level3">
<h3 class="anchored" data-anchor-id="section-2d-analyzing-departures-from-linearity">Section 2D: Analyzing Departures from Linearity</h3>
<ul>
<li><strong>Influential points</strong> can greatly affect the equation of the least-squares regression line and other summary statistics such as the correlation.
<ul>
<li>Points with <span class="math inline">\(x\)</span>-values far from <span class="math inline">\(\bar{x}\)</span> have <strong>high leverage</strong> and can be very influential.</li>
<li>Points with large residuals are called <strong>outliers</strong> and can also affect correlation and regression calculations.</li>
</ul></li>
<li>Nonlinear associations between two quantitative variables can sometimes be changed into linear associations by transforming one or both variables. Once we transform the data to achieve linearity, we can fit a least-squares regression line to the transformed data and use this linear model to make predictions.</li>
<li>To decide between competing models, choose the model with the most randomly scattered residual plot. If it is difficult to determine which residual plot is the most randomly scattered, choose the model with the largest value of <span class="math inline">\(r^2\)</span>.</li>
</ul>
</section>
<section id="unit-2-review" class="level3">
<h3 class="anchored" data-anchor-id="unit-2-review">Unit 2, Review</h3>
<section id="section-2a-relationships-between-two-categorical-variables-1" class="level4">
<h4 class="anchored" data-anchor-id="section-2a-relationships-between-two-categorical-variables-1">Section 2A: Relationships Between Two Categorical Variables</h4>
<p>In this section, you learned how to distinguish an <strong>explanatory variable</strong> from a <strong>response variable</strong>. Then you learned how to investigate the relationship between two categorical variables. Using a <strong>two-way table</strong>, you learned how to calculate and display <strong>marginal relative frequencies</strong> and <strong>joint relative frequencies</strong>. Calculating <strong>conditional relative frequencies</strong> and constructing <strong>segmented bar graphs</strong> or <strong>mosaic plots</strong> allow you to look for an <strong>association</strong> between the variables. If there is no association between the two variables, the distribution of the response variable will be the same for each value of the explanatory variable. However, if there are differences in the corresponding conditional relative frequencies, there is an association between the two variables. In other words, knowing the value of one variable helps you predict the value of the other variable.</p>
</section>
<section id="section-2b-relationships-between-two-quantitative-variables-1" class="level4">
<h4 class="anchored" data-anchor-id="section-2b-relationships-between-two-quantitative-variables-1">Section 2B: Relationships Between Two Quantitative Variables</h4>
<p>In this section, you learned how to explore the relationship between two quantitative variables. As with univariate data, the first step when working with bivariate data is to make a graph. A <strong>scatterplot</strong> is the appropriate type of graph to investigate relationships between two quantitative variables. To describe a scatterplot, be sure to discuss four characteristics: direction, form, strength, and unusual features. The <strong>direction</strong> of a relationship might be described as a <strong>positive association</strong>, a <strong>negative association</strong>, or <strong>no association</strong>. The <strong>form</strong> of a relationship can be linear or nonlinear. The <strong>strength</strong> of a relationship is strong if it closely follows a specific form. Finally, <strong>unusual features</strong> include distinct clusters of points and points that clearly fall outside the pattern of the rest of the data.</p>
<p>The <strong>correlation <span class="math inline">\(r\)</span></strong> is a numerical summary for linear relationships that describes the direction and strength of the association. When <span class="math inline">\(r \gt 0\)</span>, the association is positive; when <span class="math inline">\(r \lt 0\)</span>, the association is negative. The correlation will always take values between <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span>, with <span class="math inline">\(r = -1\)</span> and <span class="math inline">\(r=1\)</span> indicating a perfectly linear relationship. Strong linear relationships have correlations near <span class="math inline">\(1\)</span> or <span class="math inline">\(-1\)</span>, while weak linear relationships have correlations near <span class="math inline">\(0\)</span>. It isn’t possible to determine the form of a relationships by using the correlation. Strong nonlinear relationships can have a correlation close to <span class="math inline">\(1\)</span> or a correlation close to <span class="math inline">\(0\)</span>. You also learned that <strong>correlation does not imply causation</strong>. That is, we can’t assume that changes in one variable cause changes in the other variable, just because the variables have a correlation close to <span class="math inline">\(1\)</span> or <span class="math inline">\(-1\)</span>. Finally, you should calculate <span class="math inline">\(r\)</span> with technology whenever possible, but there is a formula just in case.</p>
</section>
<section id="section-2c-linear-regression-models-1" class="level4">
<h4 class="anchored" data-anchor-id="section-2c-linear-regression-models-1">Section 2C: Linear Regression Models</h4>
<p>In this section, you learned how to use <strong>regression lines</strong> as models for relationships between two quantitative variables that have a linear association. To emphasize that the model provides only predicted values, regression lines are always expressed in terms of <span class="math inline">\(\hat{y}\)</span> instead of <span class="math inline">\(y\)</span>. Likewise, when you are interpreting the <span class="math inline">\(y\)</span> intercept or slope of a least-squares regression line, make sure you are describing the <em>predicted</em> value of <span class="math inline">\(y\)</span>. The <strong>slope</strong> describes how the predicted value of <span class="math inline">\(y\)</span> changes for each one-unit increase in <span class="math inline">\(x\)</span>. The <strong><span class="math inline">\(y\)</span> intercept</strong> is the predicted value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x=0\)</span>.</p>
<p>The difference between the actual value of <span class="math inline">\(y\)</span> and the predicted value of <span class="math inline">\(y\)</span> is called a <strong>residual</strong>. Residuals are the key to understanding almost everything in this section. To find the equation of the <strong>least-squares regression line</strong>, find the line that minimizes the sum of the squared residuals. To see if a linear model is appropriate, make a <strong>residual plot</strong>. If there is no leftover curved pattern in the residual plot, you know the model is appropriate. To assess how well a line fits the data, use two values based on the sum of squared residuals. The <strong>coefficient of detmination <span class="math inline">\(r^2\)</span></strong> measures the percentage of the variation in the response variable that is accounted for by the least-squares regression line that uses the explanatory variable. The <strong>standard deviation of the residuals <span class="math inline">\(s\)</span></strong> estimates the size of a typical prediction error.</p>
<p>It is best to get the equation of a least-squares regression line from technology. Make sure you can obtain the slope and <span class="math inline">\(y\)</span> intercept from statistical software output. You can also find the equation with your calculator if you have raw data or with formulas that use the means and stanard deviations of the two variables and their correlation.</p>
</section>
<section id="section-2d-analyzing-departures-from-linearity-1" class="level4">
<h4 class="anchored" data-anchor-id="section-2d-analyzing-departures-from-linearity-1">Section 2D: Analyzing Departures from Linearity</h4>
<p>Unusual points can greatly influence the equation of the least squares regression line and other summary statistics such as the correlation <span class="math inline">\(r\)</span>, the coefficient of determination <span class="math inline">\(r^2\)</span>, and the standard deviation of the residuals <span class="math inline">\(s\)</span>. <strong>Outliers</strong> are points with large residuals. <strong>High-leverage points</strong> have x-values that are far from <span class="math inline">\(\bar{x}\)</span> relative to other points.</p>
<p>When the association between two variables is nonlinear, <strong>transforming</strong> one or both variables can result in a linear association. Once you have achieved the linearity, calculate the equation of the least-squares regression line using the transformed data. Remember to include the transformed variables when you are writing the equation of the line. Likewise, when using the line to make predictions,make sure that the prediction is in the original units of <span class="math inline">\(y\)</span>. If you transformed the <span class="math inline">\(y\)</span> variable, you will need to undo the transformation after using the least-squares regression line.</p>
<p>To decide which of two or more models is most appropriate, choose the one that produces the most linear association and whose residual plot has the most random scatter. If more than one residual plot is randomly scattered, choose the model with the value of <span class="math inline">\(r^2\)</span> closest to 1.</p>
</section>
</section>
</section>
<section id="unit-3-collecting-data-1" class="level1">
<h1>Unit 3: Collecting Data</h1>
<section id="introduction-2" class="level3">
<h3 class="anchored" data-anchor-id="introduction-2">Introduction</h3>
<p>In Unit 1, you learned how to analyze distributions of categorical and quantitative data. In Unit 2, you continued to develop your data analysis skills by studying relationships between two categorical variables and relationships between two quantitative variables. In Unit 3, we focus on two different AP Statistics course skill categories: selecting statistical methods and statistical argumentation. You’ll learn how to describe appropriate methods for collecting data, along with the advantages and disadvantages of each method. You’ll also learn how the method of data collection determines the type of conclusions we can draw from the data.</p>
</section>
<section id="section-3a-introduction-to-data-collection" class="level3">
<h3 class="anchored" data-anchor-id="section-3a-introduction-to-data-collection">Section 3A: Introduction to Data Collection</h3>
<ul>
<li>A <strong>census</strong> collects data from every individual in the <strong>population</strong>.</li>
<li>A <strong>sample survey</strong> selects a <strong>sample</strong> from the population of all individuals about which we desire information. The goal of a sample survey is to draw conclusions about the population based on data from the sample.</li>
<li>A <strong>random sample</strong> consists of individuals from the population who are selected for the sample using a chance process. We can use the data collected from a random sample to make inferences about the population from which the sample was selected.</li>
<li>An <strong>observational study</strong> observes individuals and measures variables of interest but does not attempt to influence the responses. Observational studies that examine existing data for a sample of individuals are called <strong>retrospective</strong>. Observational studies that track individuals into the future are called <strong>prospective</strong>.</li>
<li>An <strong>experiment</strong> deliberately impose his treatments (conditions) on experimental units (individuals) to measure their responses.</li>
<li>Cause-and-effect relationships are very difficult to establish from observational studies. However, well-designed experiments allow for inference about cause and effect.</li>
</ul>
</section>
<section id="section-3b-sampling-and-surveys" class="level3">
<h3 class="anchored" data-anchor-id="section-3b-sampling-and-surveys">Section 3B: Sampling and Surveys</h3>
<ul>
<li>A <strong>simple random sample (SRS)</strong> gives every possible sample of a given size the same chance to be chosen. Choose an SRS by labeling the members of the population and using slips of paper, a table of random digits, or technology to select the sample. Make sure to use <strong>sampling without replacement</strong> when selecting an SRS.</li>
<li>To select a <strong>stratified random sample</strong>, divide the population into non-overlapping groups of individuals (<strong>strata</strong>) that are similar in someway that might affect their responses. Then choose a separate SRS from each Stratham and combine these SRS’s to form the sample. When strata are “similar (homogenous) within a difference between,” stratified random sampling tends to give more precise estimates of unknown population values than does simple random sampling.</li>
<li>To select a <strong>cluster sample</strong>, divide the population into non-overlapping groups of individuals that are located near each other, called <strong>clusters</strong>. Randomly select some of these clusters. All the individuals in the chosen clusters are included in the sample. Ideally, clusters are “different (heterogenous) within but similar between.” Cluster sampling saves time and money by collecting data from entire groups of individuals that are close together.</li>
<li>To select a <strong>systematic random sample</strong>, select a value of <span class="math inline">\(k\)</span> based on the population size and desired sample size, randomly selected value from <span class="math inline">\(1\)</span> to <span class="math inline">\(k\)</span> to identify the first individual in the sample, and choose every <span class="math inline">\(k^{th}\)</span> individual thereafter. If there are patterns in the way the population is ordered that coincide with the value of <span class="math inline">\(k\)</span>, the sample may not be representative of the population. Otherwise, systematic random sampling can be easier to conduct than other sampling methods.</li>
<li>The design of a statistical study shows <strong>bias</strong> if it is very likely to underestimate or very likely to overestimate the value you want to know.
<ul>
<li>The members of a <strong>convenience sample</strong> are those individuals who are easiest to reach. The members of a <strong>voluntary response sample</strong> are those individuals who choose to join in response to an open invitation. Convenience sampling and voluntary response sampling often lead to bias because the members of the sample are not representative of the population.</li>
<li><strong>Undercoverage</strong> occurs when some members of the population are less likely to be chosen or can’t be chosen for the sample. Sampling methods that suffer from undercoverage can show bias if the individuals less likely to be included in the sample differ in relevant ways from the other members of the population.</li>
<li><strong>Nonresponse</strong> occurs when an individual chosen for the sample can’t be contacted or refuses to participate. Sampling methods that suffer from nonresponse can show bias at the individuals who don’t respond differ in relevant ways from the other members of the population.</li>
<li><strong>Response bias</strong> occurs when there is a consistent pattern of inaccurate responses to a survey question. This kind of bias can be caused by the wording of questions, characteristics of the interviewer, lack of anonymity, and other factors.</li>
</ul></li>
</ul>
</section>
<section id="section-3c-experiments" class="level3">
<h3 class="anchored" data-anchor-id="section-3c-experiments">Section 3C: Experiments</h3>
<ul>
<li>A <strong>response variable</strong> measures an outcome of the study. An <strong>explanatory variable</strong> may help explain or predict changes in a response variable.</li>
<li>Two variables are <strong>confounded</strong> when their effects on a response variable can’t be distinguished from each other. It is difficult to make cause-and-effect conclusions from observational studies because of confounding.</li>
<li>In an experiment, we impose one or more <strong>treatments</strong> on a group of <strong>experimental units</strong> (sometimes called <strong>subjects</strong> if they are human). Each treatment is a combination of the <strong>levels</strong> of the factors (explanatory variables).</li>
<li>The <strong>placebo effect</strong> describes the fact that some subjects in an experiment will respond favorably to any treatment, even an inactive treatment. A <strong>placebo</strong> is a treatment that has no active ingredient but is otherwise like other treatments.</li>
<li>In a <strong>double-blind</strong> experiment, neither the subjects nor those who interact with them and measure the response variable know which treatment a subject received. In a <strong>single-blind</strong> experiment, either the subjects or the people who interact with them and measure the response variable don’t know which treatment a subject is receiving.</li>
<li>In an experiment, a <strong>control group</strong> is used to provide a baseline for comparing the effects of other treatments. Depending on the purpose of the experiment, a control group may be given a placebo, and active treatment, or no treatment at all.</li>
<li>The basic principles of experimental design are:
<ul>
<li><strong>Comparison</strong>: Use a design that compares to or more treatments.</li>
<li><strong>Random Assignment</strong>: Use a chance process to assign treatments to experimental units (or experimental units to treatments). This helps create roughly equivalent groups before treatments are imposed.</li>
<li><strong>Replication</strong>: Use each treatment with enough experimental units so that the effects of the treatments can be distinguished from chance differences between the groups.</li>
<li><strong>Control</strong>: Keep other variables the same for all groups. Control helps avoid confounding and reduces the variation in the response variable, making it easier to decide if a treatment is effective.</li>
</ul></li>
<li>In a <strong>completely randomized design</strong>, the experimental units are assigned to the treatments completely at random.</li>
<li>A <strong>randomized block design</strong> forms groups (<strong>blocks</strong>) of experimental units that are similar with respect to a variable that is expected to affect the response. Treatments are assigned at random within each block. Responses are than compared within each block and combined with the responses of other blocks after accounting for the differences between the blocks. When blocks are chosen wisely, it is easier to determine if one treatment is more effective than another.</li>
<li>A <strong>matched pairs design</strong> is a common form of randomized block design for comparing two treatments. In some matched pairs designs, each subject receives both treatments in a random order. In others, two very similar subjects are paired, and the two treatments are randomly assigned within each pair.</li>
<li>When an observed difference in responses between the groups in an experiment is so large that it is unlikely to be explained by chance variation in the random assignment, the results are called <strong>statistically significant</strong>.</li>
<li>The <strong>scope of inference</strong> for a study describes the types of conclusions we can make based on how the data were collected.
<ul>
<li><strong>Inference about a population</strong> requires the individuals are randomly selected from the population.</li>
<li><strong>Inference about cause and effect</strong> requires a well-designed experiment with random assignment of treatments and statistically significant results.</li>
</ul></li>
<li>(Not required for the AP Statistics exam) Studies involving humans must be screened in advance by an <strong>institutional review board</strong>. All participants must give their <strong>informed consent</strong> before taking part. Any information about the individuals in the study must be kept <strong>confidential</strong>.</li>
</ul>
</section>
<section id="unit-3-review" class="level3">
<h3 class="anchored" data-anchor-id="unit-3-review">Unit 3, Review</h3>
<section id="section-3a-introduction-to-data-collection-1" class="level4">
<h4 class="anchored" data-anchor-id="section-3a-introduction-to-data-collection-1">Section 3A: Introduction to Data Collection</h4>
<p>In this section, you learned that a <strong>population</strong> is the group of all individuals that we want information about. A <strong>sample</strong> is the subset of the population that we use to gather this information. The goal of most <strong>sample surveys</strong> is to use information from the sample to draw conclusions about the population. <strong>Inference about a population</strong> is justified when the sample is selected at random from that population.</p>
<p>You also learned about the difference between observational studies and experiments. <strong>Experiments</strong> deliberately impose treatments on experimental units to see if there is a cause-and-effect relationship between two variables. <strong>Observational studies</strong> loot at relationships between two variables by using existing data (<strong>retrospective</strong>) or by followiing individuals into the future (<strong>prospective</strong>). <strong>Inference about cause and effect</strong> is very challenging with observational studies.</p>
</section>
<section id="section-3b-sampling-and-surveys-1" class="level4">
<h4 class="anchored" data-anchor-id="section-3b-sampling-and-surveys-1">Section 3B: Sampling and Surveys</h4>
<p>Selecting people for a sample because they are easy to location and letting people choose whether to be in the sample are poor ways to select a sample. Because <strong>convenience sampling</strong> and <strong>voluntary response sampling</strong> will produce estimates that are likely to underestimate or likely to overestimate the value you want to know, these methods of choosing a sample are <strong>biased</strong>.</p>
<p>To avoid bias in the way the sample is formed, the members of the sample should be chosen at random. One way to do this is with a <strong>simple random sample (SRS)</strong>, which is equivalent to selecting well-mixed slips of paper from a hat without replacement. It is often more convenient to select a SRS using technology or a table or random digits.</p>
<p>Three other random sampling methods are stratified sampling, cluster sampling, and systematic sampling. To obtain a <strong>stratified random sample</strong>, divide the population into non-overlapping groups (<strong>strata</strong>) of individuals that are likely to have similar responses, select an SRS from each stratum, and combine the chosen individuals to form the sample. Stratified random samples can produce estimates with much greater precision than is possible with simple random samples. To obtain a <strong>cluster sample</strong>, divide the population into non-overlapping groups (<strong>clusters</strong>) of individuals that are similar in locations, randomly select clusters, and use every individual in the chosen clusters. Cluster samples are easier to obtain than simple random samples or stratified random samples, but they may not produce very precise estimates. To obtain a <strong>systematic random sample</strong>, choose a value <span class="math inline">\(k\)</span> based on the population size and desired sample size. Randomly select a number from <span class="math inline">\(1\)</span> to <span class="math inline">\(k\)</span> to determine which member of the population to survey first, and then survey every <span class="math inline">\(k^{th}\)</span> member thereafter. Systematic random samples can be easier to obtain that other types of random samples.</p>
<p>Several additional sources of bias can affect estimates from a sample. <strong>Undercoverage</strong> occurs when the sampling method systematically underrepresents one part of the population. <strong>Nonresponse</strong> describes when answers cannot be obtained from some people who were chosen to be in the sample. Bias can also occur when some people in the sample don’t give accurate responses due to question wording, interviewer characteristics, or other factors (<strong>response bias</strong>).</p>
</section>
<section id="section-3c-experiments-1" class="level4">
<h4 class="anchored" data-anchor-id="section-3c-experiments-1">Section 3C: Experiments</h4>
<p>It is difficult to justify a cause-and-effect relationship between an <strong>explanatory variable</strong> and a <strong>response variable</strong> in an observational study because of confounding. Variables are <strong>confounded</strong> when it is impossible to determine which of the variables is causing a change in the response variable. A well-designed experiment can help avoid confounding.</p>
<p>A common type of comparative experiment uses a <strong>completely randomized design</strong>. In this type of design, the experimental units are assigned to the treatments completely at random. With <strong>random assignment</strong>, the treatment groups should be roughly equivalent at the beginning of the experiment. <strong>Replication</strong> means giving each treatment to as many experimental units as possible. This makes it easier to see the effects of the treatments because the effects of other variables are more likely to be balanced among the treatment groups.</p>
<p>During an experiment, it is important that other variables be <strong>controlled</strong> (kept the same) for each experimental unit. Doing so helps avoid confounding and removes a possible source of variation in the response variable. Also, beware of the <strong>placebo effect</strong>–the tendency for people to improve because they expect to, not because of the treatment they are receiving. One way to make sure that all experimental units have the same expectations is to make them <strong>blind</strong>–unaware of which tratment they are receiving. When the people interacting with the subjects and measuring the response variable are also blind, the experiment is called <strong>double-blind</strong>. Many experiments include a <strong>control group</strong> to provide a baseline for measuring the effects of the other treatments.</p>
<p>Blocking in experiments is similar to stratifying in sampling. To form <strong>blocks</strong>, group together experimental units that are similar with respect to a variable that is associated with the response. Then randomly assign the treatments within each block. A <strong>randomized block design</strong> that uses blocks with two experimental units is called a matched pairs design. Blocking helps us estimate the effects of the treatments more precisely because we can account for the variability introduced by the variables used to form the blocks.</p>
<p>The results of an experiment are <strong>statistically significant</strong> if they are too unusual to occur by chance alone. When results from an experiment with random assignment of treatments are statistically significant, we can justify <strong>inference about cause and effect</strong>. In some cases, making a cause-and-effect conclusion is difficult because it is impossible or unethical to perform certain types of experiments. Good daa ethics requires that studies are approved by an institutional review board, subjects give informed consent, and individual data are kept confidential.</p>
</section>
</section>
</section>
<section id="unit-4-probability-random-variables-and-probability-distributions-1" class="level1">
<h1>Unit 4: Probability, Random Variables, and Probability Distributions</h1>
<section id="part-i-probability-1" class="level2">
<h2 class="anchored" data-anchor-id="part-i-probability-1">Part I: Probability</h2>
<section id="introduction-3" class="level3">
<h3 class="anchored" data-anchor-id="introduction-3">Introduction</h3>
<p>Chance is all around us. You and a friend play rock-paper-scissors to determine who gets the last slice of pizza. A coin toss decides which team gets to receive the ball first in a football game. Many adults regularly play the lottery, hoping to win a big jackpot with a few lucky numbers. People of all ages play games of chance involving cards, dice, or spinners. Chance also plays a large role in the genetic traits that children inherit from their birth parents, such as hair and eye color, blood type, handedness, dimples, or whether or not they can roll their tongues.</p>
<p>The mathematics of chance behavior is called <em>probability</em>. Probability is the topic of Part I of this unit. Here is an activity that gives you some idea of what lies ahead.</p>
</section>
<section id="section-4a-randomness-probability-and-simulation" class="level3">
<h3 class="anchored" data-anchor-id="section-4a-randomness-probability-and-simulation">Section 4A: Randomness, Probability, and Simulation</h3>
<ul>
<li>A <strong>random process</strong> generates outcomes that are determined purely by chance. Random behavior is unpredictable in the short run but shows a regular and predictable pattern in the long run.</li>
<li>The <strong>probability</strong> of an outcome is the long-run relative frequency of the outcome after many trials of a random process. A probability is a number between <span class="math inline">\(0\)</span> (never occurs) and <span class="math inline">\(1\)</span> (always occurs).</li>
<li>The <strong>law of large numbers</strong> says that in many trials of the same random process, the proportion of times that a particular outcome occurs will approach its probability.</li>
<li><strong>Simulation</strong> can be used to imitate a random process and to estimate probabilities. To perform a simulation:
<ol type="1">
<li>Describe how to set up and use a random process to perform one trial of the simulation. Identify what you will record at the end of each trial.</li>
<li>Perform many trials.</li>
<li>Use the results of your simulation to answer the question of interest.</li>
</ol></li>
</ul>
</section>
<section id="section-4b-probability-rules" class="level3">
<h3 class="anchored" data-anchor-id="section-4b-probability-rules">Section 4B: Probability Rules</h3>
<ul>
<li>A <strong>probability model</strong> describes a random process by listing all possible outcomes in the <strong>sample space</strong> and giving the probability of each outcome. A valid probability model requires that all possible outcomes have probabilities between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, and that the probabilities add to <span class="math inline">\(1\)</span>.</li>
<li>An <strong>event</strong> is a set of possible outcomes from the sample space. The probability of any event is a number between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, inclusive.</li>
<li>The event “A or B” is known as the <strong>union</strong> of A and B, denoted A <span class="math inline">\(\cup\)</span> B. It consists of all outcomes in event A, event B, or both.</li>
<li>The event “A and B” is known as the <strong>intersection</strong> of A and B, denoted A <span class="math inline">\(\cap\)</span> B. It consists of all outcomes that are common to both events.</li>
<li>To find the probability that an event occurs, we use some basic rules:
<ul>
<li>If all outcomes in the sample space are equally likely, <span class="math display">\[P(A) = \frac{\text{number of outcomes in event }A}{\text{total number of outcomes in sample space}}\]</span></li>
<li><strong>Complement rule</strong>: <span class="math inline">\(P(A^C) = 1 - P(A)\)</span>, where <span class="math inline">\(A^C\)</span> is the <strong>complement</strong> of event <span class="math inline">\(A\)</span>–that is, the event that <span class="math inline">\(A\)</span> does not happen.</li>
<li><strong>General addition rule</strong>: For any two events A and B, <span class="math display">\[P(A \text{ or } B) = P(A \cup B) = P(A) + P(B) - P(A \cap B)\]</span></li>
<li><strong>Addition rule for mutually exclusive events</strong>: Events A and B are mutually exclusive (disjoint) if they have no outcomes in common. If A and B are mutually exclusive, <span class="math inline">\(P(A \text{ or } B) = P(A) + P(B)\)</span>.</li>
</ul></li>
<li>A two-way table or <strong>Venn diagram</strong> can be used to display the sample space and to help find probabilities for a random process involving two events.</li>
</ul>
</section>
<section id="section-4c-conditional-probability-and-independent-events" class="level3">
<h3 class="anchored" data-anchor-id="section-4c-conditional-probability-and-independent-events">Section 4C: Conditional Probability and Independent Events</h3>
<ul>
<li>A <strong>conditional probability</strong> describes the probability that one event happens given that another event is already known to have happened.</li>
<li>One way to calculate a conditional probability is to use the formula <span class="math display">\[P(A|B) = \frac{P(A\text{ and }B)}{P(B)} = \frac{P(A \cap B)}{P(B)} = \frac{P(\text{both events occur})}{P(\text{given event occurs})}\]</span></li>
<li>When knowing whether or not one event has occurred does not change the probability that another event happens, we say that the two events are <strong>independent</strong>. Events A and B are independent if <span class="math display">\[P(A|B) = P(A|B^C) = P(A)\]</span> or, alternatively, if <span class="math display">\[P(B|A) = P(B|A^C) = P(B)\]</span></li>
<li>Use the <strong>general multiplication rule</strong> to calculate the probability that events A and B both occur: <span class="math display">\[P(A\text{ and }B) = P(A \cap B) = P(A) \cdot P(B|A)\]</span></li>
<li>When a random process involves multiple stages, a <strong>tree diagram</strong> can be used to display the sample space and to help calculate probabilities.</li>
<li>In the special case of independent events, the multiplication rule becomes <span class="math display">\[P(A\text{ and }B) = P(A \cap B) = P(A) \cdot P(B)\]</span> This formula gives us another way to determine whether events A and B are independent.</li>
</ul>
</section>
<section id="unit-4-part-i-review" class="level3">
<h3 class="anchored" data-anchor-id="unit-4-part-i-review">Unit 4, Part I Review</h3>
<section id="section-4a-randomness-probability-and-simulation-1" class="level4">
<h4 class="anchored" data-anchor-id="section-4a-randomness-probability-and-simulation-1">Section 4A: Randomness, Probability, and Simulation</h4>
<p>In this section, you learned about the idea of <em>probability</em>. The <strong>law of large numbers</strong> says that when you repeat a <strong>random process</strong> many, many times, the relative frequency of an outcome will approach a single number. This single number is called the <strong>probability</strong> of the outcome–how often we expect the outcome to occur in a very large number of trials of the random process. Be sure to remember the “large” part of the law of large numbers. Although clear patterns emerge in a largue number of trials, we shouldn’t expect such regularity in a small number of trials.</p>
<p><strong>Simulation</strong> is a powerful tool that we can use to imitate a random process and estimate a probability. To conduct a simulation, describe how to set up and use a random process to perform one trial of the simulation. Identify what you will record at the end of each trial. Then perform many trials, and use the results of your simulation to answer the question of interest. If you are using random numbers to perform yoru simulation, be sure to consider whether numbers can be repeated within each trial.</p>
</section>
<section id="section-4b-probability-rules-1" class="level4">
<h4 class="anchored" data-anchor-id="section-4b-probability-rules-1">Section 4B: Probability Rules</h4>
<p>In this section, you learned that random behavior can be described by a <strong>probability model</strong>. Probability models have two parts, a list of possible outcomes (the <strong>sample space</strong>) and a probability for each outcome. The probability of each outcome in a probability model must be between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> (inclusive), and the probabilities of all the outcomes in the sample space must add to <span class="math inline">\(1\)</span>.</p>
<p>An <strong>event</strong> is a subset of possible outcomes from the sample space. The <strong>complement rule</strong> says the probability that an event doesn’t occur is <span class="math inline">\(1\)</span> minus the probability that an event does occur. In symbols, the complement rule says that <span class="math inline">\(P(A^C) = 1 - P(A)\)</span>. Given two events A and B from the same random process, use the <strong>general addition rule</strong> to find the probability that event A or event B occurs: <span class="math display">\[P(A \text{ or } B) = P(A \cup B) = P(A) + P(B) - P(A \cap B)\]</span></p>
<p>If the events A and B have no outcomes in common, use the addition rule for <strong>mutually exclusive</strong> events: <span class="math inline">\(P(A \cup B) = P(A) + P(B)\)</span>.</p>
<p>Finally, you learned how to use two-way tables to display the sample space for a random process involving two events. Using a two-way table (or a <strong>Venn diagram</strong>) is a helpful way to organize information and calculate probabilities, including those involving the <strong>union</strong> (<span class="math inline">\(A \cup B\)</span>) and the <strong>intersection</strong> (<span class="math inline">\(A \cap B\)</span>) of two events.</p>
</section>
<section id="section-4c-conditional-probability-and-independent-events-1" class="level4">
<h4 class="anchored" data-anchor-id="section-4c-conditional-probability-and-independent-events-1">Section 4C: Conditional Probability and Independent Events</h4>
<p>In this section, you learned that a <strong>conditional probability</strong> describes the probability of an event occuring given that another event is known to have already occurred. To calculate the probability that event A occurs given that event B has occurred, use the conditional probability formula: <span class="math display">\[P(A|B) = \frac{P(A\text{ and }B)}{P(B)} = \frac{P(A \cap B)}{P(B)} = \frac{P(\text{both events occur})}{P(\text{given event occurs})}\]</span></p>
<p>Two-way tables and <strong>tree diagrams</strong> are useful ways to organize the information provided in a conditional probability problem. Two-way tables are best when the problem describes the number or proportion of casees with certain characteristics. Tree diagrams are best when the problem describes a random process with multiple stages.</p>
<p>Use the <strong>general multiplication rule</strong> for calculating the probability that event A and event B both occur: <span class="math display">\[P(A\text{ and }B) = P(A \cap B) = P(A) \cdot P(B|A)\]</span></p>
<p>If knowing whether or not event B occurs doesn’t change the probability that event A occurs, then events A and B are <strong>independent</strong>. That is, events A and B are independent if <span class="math inline">\(P(A|B) = P(A|B^C) = P(A)\)</span>. If events A and B are independent, use the <strong>multiplication rule for independent events</strong> to find the probability that events A and B both occur: <span class="math inline">\(P(A \cap B) = P(A) \cdot P(B)\)</span>.</p>
</section>
</section>
</section>
<section id="part-ii-random-variables-and-probability-distributions-1" class="level2">
<h2 class="anchored" data-anchor-id="part-ii-random-variables-and-probability-distributions-1">Part II: Random Variables and Probability Distributions</h2>
<section id="introduction-4" class="level3">
<h3 class="anchored" data-anchor-id="introduction-4">Introduction</h3>
<p>In Part 1 of Unit 4, you learned several methods for calculating probabilities. The following activity gives you a preview of what you’ll learn about in Part II.</p>
</section>
<section id="section-4d-introduction-to-discrete-random-variables" class="level3">
<h3 class="anchored" data-anchor-id="section-4d-introduction-to-discrete-random-variables">Section 4D: Introduction to Discrete Random Variables</h3>
<ul>
<li>A <strong>random variable</strong> takes numerical values determined by the outcome of a random process there are two types of random variables: <em>discrete</em> and <em>continuous</em>.</li>
<li>A <strong>discrete random variable</strong> has a countable set of possible values with gaps between them on a number line.</li>
<li>The <strong>probability distribution</strong> of a discreet random variable gives its possible values and their probabilities.
<ul>
<li>A valid probability distribution assigns each of its values a probability between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> such that the sum of all the probabilities is exactly <span class="math inline">\(1\)</span>.</li>
<li>The probability of any event is the sum of the probabilities of all the values that make up the event.</li>
<li>We can display the probability distribution as a histogram, with the values of the brand variable on the horizontal axis and the probabilities on the vertical axis.</li>
</ul></li>
<li>We can describe the <em>shape</em> of a probability distribution graph in the same way as we did a distribution of quantitative data – by identifying symmetry or skewness and any clear peaks.</li>
<li>Use the mean to summarize the <em>center</em> of a probability distribution. The <strong>mean</strong>, or <strong>expected value</strong>, <em>of a</em> <strong>random variable</strong> is the balance point of the probability distribution’s graph.
<ul>
<li>The mean, or expected value, is the long-run average value of the variable after many, many trials of the random process. It is denoted by <span class="math inline">\(\mu_X\)</span> or <span class="math inline">\(E(X)\)</span>.</li>
<li>If <span class="math inline">\(X\)</span> is a discrete random variable, the mean (expected value) is the average of the values of <span class="math inline">\(X\)</span>, each weighted by its probability: <span class="math display">\[\mu_X = E(X) = \sum x_iP(x_i) = x_1P(x_1) + x_2P(x_2) + x_3P(x_3) + ...\]</span></li>
</ul></li>
<li>Use the standard deviation to summarize the <em>variability</em> of a probability distribution. The <strong>standard deviation</strong> <em>of a</em> <strong>random variable</strong> <span class="math inline">\(\sigma_x\)</span> measures how much the values of the variable typically vary from the mean in many, many trials of the process.
<ul>
<li>If X is a discrete random variable, the standard deviation of X is <span class="math display">\[\sigma_X = \sqrt{\sum \left(x_i-\mu_X\right)^2P\left(x_i\right)} = \sqrt{\left(x_1-\mu_X\right)^2P\left(x_1\right) + \left(x_2-\mu_X\right)^2P\left(x_2\right) + \left(x_3-\mu_X\right)^2P\left(x_3\right) + ...}\]</span></li>
<li>The value obtained before taking the square root is the <em>variance</em> <span class="math inline">\(\sigma^2_X\)</span></li>
</ul></li>
</ul>
</section>
<section id="section-4e-transforming-and-combining-random-variables" class="level3">
<h3 class="anchored" data-anchor-id="section-4e-transforming-and-combining-random-variables">Section 4E: Transforming and Combining Random Variables</h3>
<ul>
<li>Adding a positive constant <span class="math inline">\(a\)</span> to or subtracting <span class="math inline">\(a\)</span> from a random variable increases or decreases measures of center (mean, median) by <span class="math inline">\(a\)</span>, but does not affect measures of variability (range, <span class="math inline">\(IQR\)</span>, standard deviation) or the shape of its probability distribution.</li>
<li>Multiplying or dividing a random variable by a positive constant <span class="math inline">\(b\)</span> multiplies or divides measures of center (mean, median) by <span class="math inline">\(b\)</span> and multiplies or divides measures of variability (range, <span class="math inline">\(IQR\)</span>, standard deviation) by <span class="math inline">\(b\)</span>, but does not change the shape of its probability distribution.</li>
<li>If <span class="math inline">\(Y = a + bX\)</span> is a <em>linear transformation</em> of the random variable X with <span class="math inline">\(b \gt 0\)</span>,
<ul>
<li>The probability distribution of <span class="math inline">\(Y\)</span> has the same shape as the probability distribution of <span class="math inline">\(X\)</span>.</li>
<li><span class="math inline">\(\mu_Y = a + b\mu_X\)</span></li>
<li><span class="math inline">\(\sigma_Y = b\sigma_X\)</span></li>
</ul></li>
<li>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <em>any</em> two random variables,
<ul>
<li><span class="math inline">\(\mu_{X+Y} = \mu_X + \mu_Y\)</span>: the mean of the sum of two random variables is the sum of their means.</li>
<li><span class="math inline">\(\mu_{X-Y} = \mu_X - \mu_Y\)</span>: the mean of the difference of two random variables is the difference of their means.</li>
</ul></li>
<li>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>independent random variables</strong>, then knowing the value of one variable does not change the probability distribution of the other variable. In that case, variances add:
<ul>
<li><span class="math inline">\(\sigma^2_{X+Y} = \sigma^2_X + \sigma^2_Y\)</span>: the variance of the sum of two independent random variables is the sum of their variances.</li>
<li><span class="math inline">\(\sigma^2_{X-Y} = \sigma^2_X - \sigma^2_Y\)</span>: the variance of the difference of two independent random variables is the difference of their variances.<br>
</li>
</ul></li>
<li>To get the standard deviation of the sum or difference of two independent random variables, calculate the variance and then take the square root: <span class="math display">\[\sigma_{X+Y} = \sigma_{X-Y} = \sqrt{\sigma^2_X + \sigma^2_Y}\]</span></li>
<li>If <span class="math inline">\(aX +bY\)</span> is a <em>linear combination</em> of the random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>,
<ul>
<li>Its mean is <span class="math inline">\(a\mu_X + b\mu_Y\)</span>.</li>
<li>Its standard deviation is <span class="math inline">\(\sqrt{a^2\sigma^2_X + b^2\sigma^2_Y}\)</span> if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.</li>
</ul></li>
</ul>
</section>
<section id="section-4f-binomial-and-geometric-random-variables" class="level3">
<h3 class="anchored" data-anchor-id="section-4f-binomial-and-geometric-random-variables">Section 4F: Binomial and Geometric Random Variables</h3>
<ul>
<li>A <strong>binomial setting</strong> arises when we perform <span class="math inline">\(n\)</span> independent trials of the same random process and count the number of times that a particular outcome (a “success”) occurs. The following conditions must be met to have a binomial setting:
<ul>
<li><strong>B</strong>inary? The possible outcomes of each trial can be classidied as “success” or “failure.”</li>
<li><strong>I</strong>ndependent? Trials must be independent. That is, knowing the result of one trial must not tell us anything about the result of any other trial.</li>
<li><strong>N</strong>umber? The number of trials <span class="math inline">\(n\)</span> of the random process must be fixed in advance.</li>
<li><strong>S</strong>ame probability? There is the same probability of success <span class="math inline">\(p\)</span> on each trial.<br>
Remember to check the BINS!</li>
</ul></li>
<li>The count X of successes in a binomial setting is a special type of discrete random variable known as a <strong>binomial random variable</strong>. Its probability distribution is a <strong>binomial distribution</strong>. Any binomial distribution is completely specified by two numbers: the number of trials <span class="math inline">\(n\)</span> of the random process and the probability of success <span class="math inline">\(p\)</span> on any trial. The possible values of <span class="math inline">\(X\)</span> are the whole numbers <span class="math inline">\(0, 1, 2, ..., n\)</span>.</li>
<li>Use the binomial probability formula to calculate the probability of getting exactly <span class="math inline">\(x\)</span> successes in <span class="math inline">\(n\)</span> trials:<br>
<span class="math display">\[P(X = x) = \binom{n}{x}p^x(1-p)^{n-x}\]</span>
<ul>
<li>The <strong>binomial coefficient</strong> <span class="math display">\[\binom{n}{x} = \frac{n!}{x!(n-x)!}\]</span> counts the number of ways <span class="math inline">\(x\)</span> successes can be arranged among <span class="math inline">\(n\)</span> trials.</li>
<li>The factorial of <span class="math inline">\(n\)</span> is <span class="math display">\[n!=n(n-1)(n-2)\cdot ... \cdot(3)(2)(1)\]</span> for positive whole numbers <span class="math inline">\(n\)</span>, where <span class="math inline">\(0! = 1\)</span>.</li>
</ul></li>
<li>You can also use technology to calculate binomial probabilities. The TI-83/84 command binompdf(n, p, x) computes <span class="math inline">\(P(X = x)\)</span>. The TI-83/84 command binomcdf(n, p, x) computes the cumulative probability <span class="math inline">\(P(X \le x)\)</span>.<br>
</li>
<li>A binomial distribution can have a shape that is roughly symmetric, skewed to the right, or skewed to the left depending on the values of <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>.<br>
</li>
<li>The mean and standard deviation of a binomial random variable <span class="math inline">\(X\)</span> are <span class="math display">\[\mu_X = np \text{ and } \sigma_X = \sqrt{np(1-p)}\]</span><br>
</li>
<li>The binomial distribution with <span class="math inline">\(n\)</span> trials and probability of success <span class="math inline">\(p\)</span> gives a good approximation of the count of successes in a random sample of size <span class="math inline">\(n\)</span> selected without replacement from a large population containing <span class="math inline">\(p\)</span> of successess. This is true as long as the sample size <span class="math inline">\(n\)</span> is less than <span class="math inline">\(10\%\)</span> of the population size <span class="math inline">\(N\)</span>. We refer to this as the <strong><span class="math inline">\(10\%\)</span> condition</strong>. When the <span class="math inline">\(10\%\)</span> condition is met, we can view individual observations as independent.</li>
<li>A <strong>geometric setting</strong> consists of repeated trials of the same random process in which the probability <span class="math inline">\(p\)</span> of success is the same for each trial, and the goal is to count the number of trials it takes to get one success. If <span class="math inline">\(X=\)</span> the number of trials required to obtain the first success, then <span class="math inline">\(X\)</span> is a <strong>geometric random variable</strong>. Its probability distribution is called a <strong>geometric distribution</strong>.</li>
<li>If <span class="math inline">\(X\)</span> has the geometric distribution with probability of success <span class="math inline">\(p\)</span>, the possible values of <span class="math inline">\(X\)</span> are the positive integers <span class="math inline">\(1, 2, 3, ...\)</span>. The probability that it takes exactly <span class="math inline">\(x\)</span> trials to get the first success is given by <span class="math display">\[P(X = x) = (1-p)^{x-1}p\]</span></li>
<li>You can also use technology to calculate geometric probabilities. The TI-83/84 command geometpdf(p,x) computes <span class="math inline">\(P(X = x)\)</span>. The TI-83/84 command geometcdf(p,x) computes the cumulative probability <span class="math inline">\(P(X \le x)\)</span>.</li>
<li>The mean and standard deviation of a geometric random variable X are <span class="math display">\[\mu_X = \frac{1}{p} \text{ and } \sigma_X = \frac{\sqrt{1 - p}}{p}\]</span></li>
<li>When calculating probabilities involving binomial or geometric random variables, follow this two-step process:<br>
<strong>Step 1</strong>: Define the random variable of interest, state how it is distributed, and identify the values of interest.<br>
<strong>Step 2</strong>: Perform calculations–show your work! If you use technology, be sure to label the inputs you used for your calculator command.<br>
Be sure to answer the question that was asked.</li>
</ul>
</section>
<section id="unit-4-part-ii-review" class="level3">
<h3 class="anchored" data-anchor-id="unit-4-part-ii-review">Unit 4, Part II Review</h3>
<section id="section-4d-introduction-to-discrete-random-variables-1" class="level4">
<h4 class="anchored" data-anchor-id="section-4d-introduction-to-discrete-random-variables-1">Section 4D: Introduction to Discrete Random Variables</h4>
<p>A <strong>random variable</strong> assigns numerical values to the outcomes of a random process. The <strong>probability distribution</strong> of a random variable describes its possible values and their probabilities. There are two types of random variables: discrete and continuous. <strong>Discrete random variables</strong> take a countable (finite or infinite) set of possible values with gaps between them on the number line.</p>
<p>As with distributions of quantitative data described in Unit 1, we are often interested in the shape, center, and variability of a probability distribution. The shape of a discrete probability distribution can be identified by graphing a probability histogram, with the height of each bar representing the probability of a single value. The center is usually identified by the <strong>mean</strong>, or <strong>expected value</strong>, <strong>of the random variable</strong>, which is the average value of the random variable if the random process is repeated many, many times. The variability of a probability distribution is usually identified by the <strong>standard deviation of the random variable</strong>, which describes how much the values of the random variable typically vary from the mean value, in many, many trials of the random process.</p>
</section>
<section id="section-4e-transforming-and-combining-random-variables-1" class="level4">
<h4 class="anchored" data-anchor-id="section-4e-transforming-and-combining-random-variables-1">Section 4E: Transforming and Combining Random Variables</h4>
<p>In this section, you learned how <em>linear transformations</em> of a random variable affect the shape, center, and variability of its probability distribution. Similar to what you learned in Section 1E, adding a positive constant to (or subtracting it from) each value of a random variable changes the measures of center (mean, median), but not the shape or variability of the probability distribution. Multiplying or dividing each value of a random variable by a positive constant changes the measure of center (mean, median) and variability (standard deviation, range, interquartile range), but not the shape of the probability distribution. For a linear transformation <span class="math inline">\(Y = a + bX\)</span> (with <span class="math inline">\(b \gt 0\)</span>), the mean and standard deviation of <span class="math inline">\(Y\)</span> are <span class="math display">\[\mu_Y = a + b\mu_X \text{  and  }\sigma_Y = b\sigma_X\]</span> You also learned how to calculate the mean and standard deviation for a <em>linear combination</em> of random variables. The mean of a sum or difference of any two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is given by <span class="math display">\[\mu_{X+Y} = \mu_X + \mu_Y \text{  and  } \mu_{X-Y} = \mu_X - \mu_Y\]</span> If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>independent random variables</strong>, their variances add: <span class="math display">\[\sigma^2_{X+Y} = \sigma^2_X + \sigma^2_Y \text{  and  }
\sigma^2_{X-Y} = \sigma^2_X - \sigma^2_Y\]</span> To find the standard deviation, just take the square root of the variance. Recall that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent if knowning the value of one variable does not change the probability distribution of the other variable.</p>
<p>The linear combination <span class="math inline">\(aX +bY\)</span> has mean <span class="math inline">\(a\mu_X + b\mu_Y\)</span>. Its standard deviation is <span class="math inline">\(\sqrt{a^2\sigma^2_X + b^2\sigma^2_Y}\)</span> if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.</p>
</section>
<section id="section-4f-binomial-and-geometric-random-variables-1" class="level4">
<h4 class="anchored" data-anchor-id="section-4f-binomial-and-geometric-random-variables-1">Section 4F: Binomial and Geometric Random Variables</h4>
<p>In this section, you learned about two common types of discrete random variables: binomial and geometric. <strong>Binomial random variables</strong> count the number of successes in a fixed number of trials (<span class="math inline">\(n\)</span>) of the same random process, whereas <strong>geometric random variables</strong> count the number of trials needed to get one success. Otherwise, <strong>binomial settings</strong> and <strong>geometric settings</strong> have the same conditions: there must be two possible outcomes for each trial (success or failure), the trials must be independent, and the probability of success <span class="math inline">\(p\)</span> must stay the same throughout all trials.</p>
<p>To calculate probabilities for a <strong>binomial distribution</strong> with <span class="math inline">\(n\)</span> trials and probability of success <span class="math inline">\(p\)</span> on each trial, use technology or the binomial probability formula <span class="math display">\[P(X = x) = \binom{n}{x}p^x(1-p)^{n-x}\]</span> Be sure to check the <strong><span class="math inline">\(10\%\)</span> condition</strong> (<span class="math inline">\(n \lt 0.10N\)</span>) when sampling is done without replacement before using a binomial distribution.</p>
<p>The shape of a binomial distribution depends on both the number of trials <span class="math inline">\(n\)</span> and the probability of success <span class="math inline">\(p\)</span>. The mean and standard deviation of a binomial random variable <span class="math inline">\(X\)</span> are <span class="math display">\[\mu_X = np \text{ and } \sigma_X = \sqrt{np(1-p)}\]</span><br>
Finally, to calculate probabilities for a <strong>geometric distribution</strong> with probability of success <span class="math inline">\(p\)</span> on each trial, use technology or the geometric probability formula <span class="math display">\[P(X = x) = (1-p)^{x-1}p\]</span></p>
<p>A geometric distribution is always skewed to the right and unimodal, with a single peak at 1. The mean and standard deviation of a geometric random variable <span class="math inline">\(X\)</span> are <span class="math display">\[\mu_X = \frac{1}{p} \text{ and } \sigma_X = \frac{\sqrt{1 - p}}{p}\]</span></p>
</section>
</section>
</section>
</section>
<section id="unit-5-sampling-distributions-1" class="level1">
<h1>Unit 5: Sampling Distributions</h1>
<section id="introduction-5" class="level3">
<h3 class="anchored" data-anchor-id="introduction-5">Introduction</h3>
<p>In this unit, we will return to a key idea about statistical inference from Unit 3–making conclusions about a population based on data from a sample. Here are a few examples of statistical inference in practice:</p>
<ul>
<li>Each month, the Current Population Survey (CPS) interviews a random sample of individuals in about 60,000 U.S. households. The CPS uses the proportion of unemployed people in the sample <span class="math inline">\(\hat{p}\)</span> to estimate the national unemployment rate <span class="math inline">\(p\)</span>.</li>
<li>To estimate how much gasoline prices vary in a large city, a reporter records the price per gallon of regular unleaded gasoline at a random sample of 10 gas stations in the city. The range (maximum-minimum) of the prices in the sample is 25 cents. What can the reporter say about the range of gas prices for all the city’s stations?</li>
<li>A battery manufacturer wants to make sure that the AAA batteries it produces meet certain standards, including how long the batteries last. Quality control inspectors collect data about the lifetime of each battery in a random sample of 100 AAA batteries produced during one hour and use the sample mean lifetime <span class="math inline">\(\bar{x}\)</span> to estimate the unknown population mean lifetime <span class="math inline">\(\mu\)</span> for all batteries produced in that hour.</li>
</ul>
<p>Let’s look at the battery example a little more closely. To make an inference about the batteries produced in the given hour, we need to know how close the sample mean <span class="math inline">\(\bar{x}\)</span> is likely to be to the population mean <span class="math inline">\(\mu\)</span> and what values of <span class="math inline">\(\bar{x}\)</span> should be considered unusual. After all, different random samples of 100 batteries from the same hour of production would yield different values of <span class="math inline">\(\bar{x}\)</span>.</p>
<p>How can we describe this <em>sampling distribution</em> ofpossible <span class="math inline">\(\bar{x}\)</span> values? We can think of <span class="math inline">\(\bar{x}\)</span> as a random variable because it takes numerical values that describe the outcomes of a random sampling process. As a result, we can use therules for means and standard deviations from Unit 4 to determine the center and variability of the distribution of <span class="math inline">\(\bar{x}\)</span>. Furthermore, if the distribution of <span class="math inline">\(\bar{x}\)</span> is approximately normal, we can do probability calculations using what we learned about normal distributions in Unit 1.</p>
</section>
<section id="section-5a-normal-distributions-revisited" class="level3">
<h3 class="anchored" data-anchor-id="section-5a-normal-distributions-revisited">Section 5A: Normal Distributions, Revisited</h3>
<ul>
<li>A <strong>continuous random variable</strong> can take any value in a specific interval on the number line.<br>
</li>
<li>The probability distribution of a continuous random variable is described by a <strong>density curve</strong> that is always on or above the horizontalaxis and has a total area <span class="math inline">\(1\)</span> underneath it.<br>
</li>
<li>The area under the density curve and above any specified interval of values on the horizontal axis gives the probability that the random variable falls within that interval.</li>
<li>A <strong>normal random variable</strong> is a continuous random variable whose probability distribution is described by a normal curve.<br>
</li>
<li>When calculating probabilities involving normal random variables, follow this process:
<ul>
<li><strong>Step 1</strong>: Define the random variable of interest, state how it is distributed, and identify the values of interest. <em>This can be done verbally or with a well-labeled sketch of a normal curve that includes the variable name, mean, standard deviation, boundary value(s), and area(s) of interest</em>.</li>
<li><strong>Step 2</strong>: Perform calculations–show your work! Be sure to answer the question that was asked.</li>
</ul></li>
<li>A linear combination of independent, normal random variables is a normal random variable.</li>
</ul>
</section>
<section id="section-5b-what-is-a-sampling-distribution" class="level3">
<h3 class="anchored" data-anchor-id="section-5b-what-is-a-sampling-distribution">Section 5B: What is a Sampling Distribution?</h3>
<ul>
<li>A <strong>parameter</strong> is a number that describes some characteristic of a population. A statistic is a number that describes some characteristic of a sample, we use statistics to estimate parameters.</li>
<li>The <strong>sampling distribution</strong> of a statistic describes the values of the statistic in all possible samples of the same size from the same population.</li>
<li>To determine a sampling distribution, list all possible samples of a particular size, calculate the value of the statistic for each sample, and graph the distribution of the statistic. If there are many possible samples, approximate the sampling distribution: repeatedly select (or simulate) random samples of a particular size, calculate the value of the statistic for each sample, and graph the distribution of the statistic.</li>
<li>We can use sampling distributions to evaluate a claim by determining which values of a statistic are likely to happen by chance alone.</li>
<li>A statistic used to estimate a parameter is an <strong>unbiased estimator</strong> if the mean of its sampling distribution is equal to the value of the parameter being estimated. That is, the statistic doesn’t consistently overestimate or consistently underestimate the value of the parameter.<br>
</li>
<li>The sampling distribution of any statistic will have less variability when the sample size is larger. That is, the statistic will be more precise estimator of the parameter with larger sample sizes.<br>
</li>
<li>When estimating a parameter, choose a statistic with low or no bias and minimum variability.</li>
</ul>
</section>
<section id="section-5c-sample-proportions" class="level3">
<h3 class="anchored" data-anchor-id="section-5c-sample-proportions">Section 5C: Sample Proportions</h3>
<ul>
<li>When we want information about the population proportion <span class="math inline">\(p\)</span> of successes, we often select an SRS and use the sample proportion <span class="math inline">\(\hat{p}\)</span> to estimate the unknown parameter <span class="math inline">\(p\)</span>. The <strong>sampling distribution of the sample proportion <span class="math inline">\(\hat{p}\)</span></strong> describes how the statistic <span class="math inline">\(\hat{p}\)</span> varies in all possible samples of the same size from the population.
<ul>
<li><strong>Center</strong>: The mean of the sampling distribution of <span class="math inline">\(\hat{p}\)</span> is <span class="math inline">\(\mu_{\hat{p}} = p\)</span>. The mean describes the average value of <span class="math inline">\(\hat{p}\)</span> in all possible samples of a certain size from a population.<br>
</li>
<li><strong>Variability</strong>: The standard deviation of the sampling distribution of <span class="math inline">\(\hat{p}\)</span> is approximately <span class="math inline">\(\sigma_{\hat{p}} = \sqrt{\frac{p(1-P)}{n}}\)</span> when the <span class="math inline">\(10\%\)</span> condition is met: <span class="math inline">\(n &lt; 0.10N\)</span>. The standard deviation measures how far the values of <span class="math inline">\(\hat{p}\)</span> typically vary from <span class="math inline">\(p\)</span> in all possible samples of a certain size from a population.<br>
</li>
<li><strong>Shape</strong>: The sampling distribution of <span class="math inline">\(\hat{p}\)</span> is approximately normal when the Large Counts condition is met: <span class="math inline">\(np\)</span> and <span class="math inline">\(n(1 - p)\)</span> are both at least <span class="math inline">\(10\)</span>.</li>
</ul></li>
<li>Choose independent SRSs of size <span class="math inline">\(n_1\)</span>, from Population 1 of size <span class="math inline">\(N_1\)</span>, with proportion of successes <span class="math inline">\(p_1\)</span> and of size <span class="math inline">\(n_2\)</span> from Population 2 of size <span class="math inline">\(N_2\)</span> with proportion of successes <span class="math inline">\(p_2\)</span>. The <strong>sampling distribution of the difference in sample proportions <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span></strong> has the following properties:
<ul>
<li><strong>Center</strong>: The mean of the sampling distribution is <span class="math inline">\(\mu_{\hat{p}_1 - \hat{p}_2} = p_1 - p_2\)</span>.</li>
<li><strong>Variability</strong>: The standard deviation of the sampling distribution is approximately <span class="math inline">\(\sigma_{\hat{p}_1 - \hat{p}_2}=\sqrt{\frac{p_1(1-p_1)}{n_1}+\frac{p_2(1-p_2)}{n_2}}\)</span> as long as the samples are independent and the <span class="math inline">\(10\%\)</span> condition is met for both samples: <span class="math inline">\(n_1 &lt; 0.10N_1\)</span>, and <span class="math inline">\(n_2 &lt; 0.10N_2\)</span>.</li>
<li><strong>Shape</strong>: Approximately normal if the Large Counts condition is met for both samples: <span class="math inline">\(n_1p_1\)</span>, <span class="math inline">\(n_1(1 - p_1)\)</span>, <span class="math inline">\(n_2p_2\)</span>, and <span class="math inline">\(n_2(1 - p_2)\)</span> are all at least <span class="math inline">\(10\)</span>.</li>
</ul></li>
<li>When the Large Counts condition is met, you can use a normal distribution to calculate probabilities involving the sampling distribution of <span class="math inline">\(\hat{p}\)</span> or the sampling distribution of <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span>.</li>
</ul>
</section>
<section id="section-5d-sample-means" class="level3">
<h3 class="anchored" data-anchor-id="section-5d-sample-means">Section 5D: Sample Means</h3>
<ul>
<li>When we want information about the population mean <span class="math inline">\(\mu\)</span> for some quantitative variable, we often select an SRS and use the sample mean <span class="math inline">\(\bar{x}\)</span> to estimate the unknown parameter <span class="math inline">\(\mu\)</span>. The <strong>sampling distribution of the sample mean <span class="math inline">\(\bar{x}\)</span></strong> describes how the statistic <span class="math inline">\(\bar{x}\)</span> varies in all possible samples of the same size from the population.<br>
</li>
<li><strong>Center</strong>: The mean of the sampling distribution of <span class="math inline">\(\bar{x}\)</span> is <span class="math inline">\(\mu_{\bar{x}}=\mu\)</span>, where <span class="math inline">\(\mu\)</span> is the mean of the population. The mean describes the average value of <span class="math inline">\(\bar{x}\)</span> in all possible samples of a certain size from a population.<br>
</li>
<li><strong>Variability</strong>: The standard deviation of the sampling distribution of <span class="math inline">\(\bar{x}\)</span> is approximately <span class="math inline">\(\sigma_{\bar{x}}=\frac{\sigma}{\sqrt{n}}\)</span> for an SRS of size <span class="math inline">\(n\)</span> if the population has standard deviation <span class="math inline">\(\sigma\)</span>. This formula can be used when the <span class="math inline">\(10\%\)</span> condition is met: <span class="math inline">\(n \lt 0.10N\)</span>. The standard deviation measures how far the values of <span class="math inline">\(\bar{x}\)</span> typically vary from <span class="math inline">\(\mu\)</span> in all possible samples of a certain size from a population.</li>
<li><strong>Shape</strong>: If the population distribution is approximately normal, then so is the sampling distribution of the sample mean <span class="math inline">\(\bar{x}\)</span>. If the population distribution is non-normal, the <strong>central limit theorem (CLT)</strong> states that when <span class="math inline">\(n\)</span> is sufficiently large, the sampling distribution of <span class="math inline">\(\bar{x}\)</span> is approximately normal. For most non-normal population distributions, it is safe to use a normal distribution to calculate probabilities involving <span class="math inline">\(\bar{x}\)</span> when <span class="math inline">\(n \ge 30\)</span>.<br>
</li>
<li>Suppose we select independent SRSs of size <span class="math inline">\(n_1\)</span> from Population 1 of size <span class="math inline">\(N_1\)</span> with mean <span class="math inline">\(\mu_1\)</span> and standard deviation <span class="math inline">\(\sigma_1\)</span> and of size <span class="math inline">\(n_2\)</span> from Population 2 of size <span class="math inline">\(N_2\)</span> with mean <span class="math inline">\(\mu_2\)</span> and standard deviation <span class="math inline">\(\sigma_2\)</span>. The sampling distribution of <span class="math inline">\(\bar{x}_1 -\bar{x}_2\)</span> has the following properties:
<ul>
<li><strong>Center</strong>: The mean of the sampling distribution is <span class="math inline">\(\mu_{\bar{x}_1 -\bar{x}_2} = \mu_1 - \mu_2\)</span><br>
</li>
<li><strong>Variability</strong>: The standard deviation of the sampling distribution is approximately <span class="math inline">\(\sigma_{\bar{x}_1 -\bar{x}_2} = \sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}\)</span><br>
</li>
<li><strong>Shape</strong>: Approximately normal if both population distributions are approximately normal or if both sample sizes are large (<span class="math inline">\(n_1 \ge 30 \text{ and } n_2 \ge 30\)</span>).<br>
</li>
</ul></li>
<li>When the population distribution(s) is/are approximately normal or the sample size(s) is/are large (<span class="math inline">\(n \ge 30\)</span>), you can use a normal distribution to calculate probabilities involving <span class="math inline">\(\bar{x}\)</span> or <span class="math inline">\(\bar{x}_1 -\bar{x}_2\)</span>.</li>
</ul>
</section>
<section id="unit-5-review" class="level3">
<h3 class="anchored" data-anchor-id="unit-5-review">Unit 5 Review</h3>
<section id="section-5a-normal-distributions-revisited-1" class="level4">
<h4 class="anchored" data-anchor-id="section-5a-normal-distributions-revisited-1">Section 5A: Normal Distributions, Revisited</h4>
<p>In this section, you learned that a <strong>continuous random variable</strong> can take any value in a specified interval on the number line (unlike discrete random variables, which have gaps between possible values). Continuous random variables often result from measuring something, such as time or weight. The probability distribution of a continuous random variable is described by a <strong>density curve</strong> that is always on or above the horizontal axis and has total area 1 underneath it. The area under the density curve and above any specified interval of values on the horizontal axis gives the probability that the random variable falls within that interval. A <strong>normal random variable</strong> is a continuous random variable whose probability distribution is described by a normal curve. Any <strong>linear combination</strong> of independent, normal random variables is also a normal random variable.</p>
</section>
<section id="section-5b-what-is-a-sampling-distribution-1" class="level4">
<h4 class="anchored" data-anchor-id="section-5b-what-is-a-sampling-distribution-1">Section 5B: What is a Sampling Distribution?</h4>
<p>In this section, you learned the “big ideas” about sampling distributions. The first big idea is the difference between a statistic and a parameter. A <strong>parameter</strong> is a number that described some characteristic of a population. A <strong>statistic</strong> estimates the value of a parameter using a sample from the population. Making the distinction between a statistic and a parameter will be crucial throughout the rest of the course.</p>
<p>The second big idea is that statistics vary and have distributions. For example, the mean weight for a sample of high school students is a variable that will change from sample to sample. The <strong>sampling distribution of a statistic</strong> gives the distribution of the statistic in all possible samples of the same size from the same population. Knowing the sampling distribution of a statistic tells us how far we can expect a statistic to vary from the parameter value and what values of the statistic should be considered unusual.</p>
<p>The third big idea is how to describe a sampling distribution. As in Unit 1, you need to address shape, center, and variability. If the center (mean) of the sampling is the same as the value of the parameter being estimated, then the statistic is called an <strong>unbiased estimator</strong>. That is, an estimator is unbiased if it doesn’t consistently underestimate or consistently overestimate the parameter. Ideally, the variability of a sampling distribution willbe very small, meaning that the statistic provides precise estimates of the parameter. Larger sample sizes result in sampling distributions with less variability.</p>
<p>Finally, be very careful with your language. There is an important difference between the distribution of a population, the distribution of a sample, and the sampling distribution of a statistic. When you are writing your answers, be sure to indicate which distribution you are referring to. Don’t make ambiguous statements like “the distribution will become less variable.”</p>
</section>
<section id="section-5c-sample-proportions-1" class="level4">
<h4 class="anchored" data-anchor-id="section-5c-sample-proportions-1">Section 5C: Sample Proportions</h4>
<p>In this section, you learned about the shape, center, and variability of the <strong>sampling distribution of a sample proportion <span class="math inline">\(\hat{p}\)</span></strong>. The mean of the sampling distribution of <span class="math inline">\(\hat{p}\)</span> is <span class="math inline">\(\mu_{\hat{p}} = p\)</span>, the population proportion. As a result, the sample proportion <span class="math inline">\(\hat{p}\)</span> is an unbiased estimator of the population proportion <span class="math inline">\(p\)</span>. When the sample size is less than <span class="math inline">\(10\%\)</span> of the population size (the <span class="math inline">\(10\%\)</span> condtion), the standard deviation of the sampling distribution of the sample proportion is approximately <span class="math inline">\(\sigma_{\hat{p}} = \sqrt{\frac{p(1-p)}{n}}\)</span>. The standard deviation measures how far the sample proportion <span class="math inline">\(\hat{p}\)</span> typically varies from the propulation proportion <span class="math inline">\(p\)</span>. When <span class="math inline">\(np \ge 10\)</span> and <span class="math inline">\(n(1-p) \ge 10\)</span> (the Large Counts condition), the shape of the sampling distribution of <span class="math inline">\(\hat{p}\)</span> willbe approximately normal. When the Large Counts condition is met, you can use normal distributions to calculate probabilities involving <span class="math inline">\(\hat{p}\)</span>.</p>
<p>You also learned about the <strong>sampling distribution of a <em>difference</em> in sample proportions <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span></strong>. The mean of the sampling distribution of <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span> is <span class="math inline">\(\mu_{\hat{p}_1 - \hat{p}_2} = p_1 - p_2\)</span>. The standard deviation of the sampling distribution is approximately <span class="math inline">\(\sigma_{\hat{p}_1 - \hat{p}_2} = \sqrt{\frac{p_1(1-p_1)}{n_1}+\frac{p_2(1-p_2)}{n_2}}\)</span> when the <span class="math inline">\(10\%\)</span> condition is met for both samples and the samples are independent. The shape of the sampling distribution of <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span> will be approximately normal when the Large Counts condition is met for both samples. When the Large Counts condition is met, you can use normal distributions to calculate probabilities involving <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span>.</p>
</section>
<section id="section-5d-sample-means-1" class="level4">
<h4 class="anchored" data-anchor-id="section-5d-sample-means-1">Section 5D: Sample Means</h4>
<p>In this section, you learned about the shape, center, and variability of the <strong>sampling distribution of a sample mean <span class="math inline">\(\bar{x}\)</span></strong>. The mean of the sampling distribution <span class="math inline">\(\bar{x}\)</span> is <span class="math inline">\(\mu_{\bar{x}}=\mu\)</span>, the population mean. As a result, the sample mean <span class="math inline">\(\bar{x}\)</span> is an unbiased estimator of the population mean <span class="math inline">\(\mu\)</span>. When the sample size is less than <span class="math inline">\(10\%\)</span> of the population size (the <span class="math inline">\(10\%\)</span> condition), the standard deviation of the sampling distribution of the sample mean is approximately <span class="math inline">\(\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}}\)</span>. The standard deviation measures how far the sample mean <span class="math inline">\(\bar{x}\)</span> typically varies from the population mean <span class="math inline">\(\mu\)</span>.</p>
<p>When the population is approximately normally distributed, the shape of the sampling distribution of <span class="math inline">\(\bar{x}\)</span> will also be approximately normal for any sample size. When the population distribution is non-normal, the <strong>Central Limit Theorem (CLT)</strong> says that the sampling distribution of <span class="math inline">\(\bar{x}\)</span> will become approximately normal when the sample size is sufficiently large. You can use a normal distribution to calculate probabilities involving the sampling distribution of <span class="math inline">\(\bar{x}\)</span> if the population distribution is approximately normal or the sample size is at least 30.</p>
<p>You also learned about the <strong>sampling distribution of a <em>difference</em> in sample means <span class="math inline">\(\bar{x_1}-\bar{x_2}\)</span></strong>. The mean of the sampling distribution of <span class="math inline">\(\bar{x_1}-\bar{x_2}\)</span> is <span class="math inline">\(\mu_{\bar{x_1}-\bar{x_2}} = \mu_1 - \mu_2\)</span>. The standard deviation ofthe sampling distribution is approximately <span class="math inline">\(\sigma{\bar{x_1}-\bar{x_2}} = \sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}\)</span> when the <span class="math inline">\(10\%\)</span> conditon is met for both samples and the samples are independent. The shape of the sampling distribution of <span class="math inline">\(\bar{x_1}-\bar{x_2}\)</span> will be approximately normal when both population distributions are approximately normal or both sample sizes are at least 30.</p>
</section>
</section>
<section id="comparing-sampling-distributions" class="level3">
<h3 class="anchored" data-anchor-id="comparing-sampling-distributions">Comparing Sampling Distributions</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 35%">
<col style="width: 35%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Center</strong></td>
<td style="text-align: left;"><span class="math display">\[\mu_{\hat{p}}=p\]</span></td>
<td style="text-align: left;"><span class="math display">\[\mu_{\bar{x}}=\mu\]</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Variability</strong></td>
<td style="text-align: left;"><span class="math display">\[\sigma_{\hat{p}} = \sqrt{\frac{p(1-p)}{n}}\]</span> when the <span class="math inline">\(10\%\)</span> condition is met:<br><br> <span class="math display">\[n &lt; 0.10N\]</span></td>
<td style="text-align: left;"><span class="math display">\[\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}}\]</span> when the <span class="math inline">\(10\%\)</span> condition is met:<br><br> <span class="math display">\[n &lt; 0.10N\]</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Shape</strong></td>
<td style="text-align: left;">Approximately normal when the Large Counts condition is met:<br><br> <span class="math display">\[np \ge 10\text{ and }n(1 - p) \ge 10\]</span></td>
<td style="text-align: left;">Approximately normal when the population distribution is approximately normal or the sample size is large:<br><br> <span class="math display">\[n \ge 30\]</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Center</strong></td>
<td style="text-align: left;"><span class="math display">\[\mu_{\hat{p}_1 - \hat{p}_2} = p_1 - p_2\]</span></td>
<td style="text-align: left;"><span class="math display">\[\mu_{\bar{x_1}-\bar{x_2}} = \mu_1 - \mu_2\]</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Variability</strong></td>
<td style="text-align: left;"><span class="math display">\[\sigma_{\hat{p}_1 - \hat{p}_2} = \sqrt{\frac{p_1(1-p_1)}{n_1}+\frac{p_2(1-p_2)}{n_2}}\]</span> when the samples are independent and the <span class="math inline">\(10\%\)</span> condition is met for both samples:<br><br> <span class="math display">\[n_1 &lt; 0.10N_1\text{, and }n_2 &lt; 0.10N_2\]</span></td>
<td style="text-align: left;"><span class="math display">\[\sigma{\bar{x_1}-\bar{x_2}} = \sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}\]</span> when the samples are independent and the <span class="math inline">\(10\%\)</span> condition is met for both samples:<br><br> <span class="math display">\[n_1 &lt; 0.10N_1\text{, and }n_2 &lt; 0.10N_2\]</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Shape</strong></td>
<td style="text-align: left;">Approximately normal when the Large Counts condition is met for both samples:<br><br> <span class="math display">\[n_1p_1, n_1(1 - p_1), n_2p_2\text{, and }n_2(1 - p_2)\text{ are all }\ge 10\]</span></td>
<td style="text-align: left;">Approximately normal when both population distributions are approximately normal or both sample sizes are at least 30:<br><br> <span class="math display">\[n_1 &gt; 30\text{, and }n_2 &gt; 30\]</span></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="unit-6-inference-for-categorical-data-proportions-1" class="level1">
<h1>Unit 6: Inference for Categorical Data: Proportions</h1>
<section id="part-i-inference-for-one-sample-1" class="level2">
<h2 class="anchored" data-anchor-id="part-i-inference-for-one-sample-1">Part I: Inference for One Sample</h2>
<section id="introduction-6" class="level3">
<h3 class="anchored" data-anchor-id="introduction-6">Introduction</h3>
<p>How long does a battery last on the newest iPhone, on average? What proportion of college undergraduates attended all of their classes last week? Do a majority of U.S. adults keep their New Year’s resolutions? These are examples of the types of questions we can answer using the statistical inference methods you’ll learn about in this chapter.</p>
<p>It isn’t practical to determine the lifetime of <em>every</em> iPhone battery, to ask <em>all</em> undergraduates about their attendance, or to survey the <em>entire</em> population of U.S. adults. Instead, we choose a random sample of individuals (batteries, undergraduates, U.S. adults) to represent the population and collect data from those individuals. Using the information learned in Unit 3, we can generalize our results to the population of interest if we randomly select the sample. However, we cannot be certain that our solutions are correct–a different sample would likely yield a different estimate. Fortunately, we can use what we learned about sampling distributions in Unit 5 to help account for the chance variation due to random selection.</p>
<p>In this unit, we begin the formal study of statistical inference–using information from a sample to draw conclusions about a population parameter such as <span class="math inline">\(p\)</span> or <span class="math inline">\(\mu\)</span>. This is an important transition from Unit 5, where you were given information about a population and asked questions about the distribution of a sample statistic, such as the sample proportion <span class="math inline">\(\hat{p}\)</span> or the sample mean <span class="math inline">\(\bar{x}\)</span>. This unit also combines many of the skills you’ve gained in the previous units–selecting statistical methods, using probability and simulation, and statistical argumentation.</p>
</section>
<section id="section-6a-confidence-intervals-the-basics" class="level3">
<h3 class="anchored" data-anchor-id="section-6a-confidence-intervals-the-basics">Section 6A: Confidence Intervals: The Basics</h3>
<ul>
<li>To estimate an unknown population parameter, start with a statistic that will provide a reasonable guess. The chosen statistic is a <strong>point estimator</strong> for the parameter. The specific value of the point estimator that we calculate from sample data gives a <strong>point estimate</strong> for the parameter.<br>
</li>
<li>A <strong>confidence interval</strong>, or <strong>interval estimate</strong>, gives a set of plausible values for the unknown population parameter based on sample data.<br>
</li>
<li>To interpret a <span class="math inline">\(C\%\)</span> confidence interval, say, “We are <span class="math inline">\(C\%\)</span> confident that the interval from _________ to __________ captures the [parameter in context].” Be sure that your interpretation describes a parameter and not a statistic.<br>
</li>
<li>If a proposed value for a population parameter is not included in a confidence interval for that parameter, there is convincing evidence that the proposed value is incorrect.<br>
</li>
<li>The <strong>confidence level C</strong> is the long-run capture rate (success rate) of the methhod that produces the interval.<br>
</li>
<li>To interpret a confidence level C, say, “If we were to select many random samples of the same size from the same population and construct a <span class="math inline">\(C\%\)</span> confidence interval using each sample, about <span class="math inline">\(C\%\)</span> of the intervals would capture the [parameter in context].”</li>
</ul>
</section>
<section id="section-6b-confidence-intervals-for-a-population-proportion" class="level3">
<h3 class="anchored" data-anchor-id="section-6b-confidence-intervals-for-a-population-proportion">Section 6B: Confidence Intervals for a Population Proportion</h3>
<ul>
<li>Confidence intervals for the proportion <span class="math inline">\(p\)</span> of successes in a population use the sample proportion <span class="math inline">\(\hat{p}\)</span> as the point estimate.<br>
</li>
<li>When constructing a confidence interval for a population proportion <span class="math inline">\(p\)</span>, we need to ensure that the observations in the sample can be viewed as independent and that the sampling distribution of <span class="math inline">\(\hat{p}\)</span> is approximately normal. The required conditions are:
<ul>
<li>Random: The data come from a random sample from the population of interest.
<ul>
<li><span class="math inline">\(10\%\)</span>: When sampling without replacement, <span class="math inline">\(n \lt 0.10N\)</span>.<br>
</li>
</ul></li>
<li>Large Counts: Both <span class="math inline">\(n\hat{p}\)</span> and <span class="math inline">\(n(1-\hat{p})\)</span> are at least <span class="math inline">\(10\)</span>. That is, the number of successes and the number of failures in the sample are both at least <span class="math inline">\(10\)</span>.<br>
</li>
</ul></li>
<li>The general formula for a confidence interval is <span class="math display">\[\text{point estimate}\pm\text{margin of error}\]</span> where the <strong>margin of error</strong> of an estimate describes how far, at most, we expect the point estimate to vary from the population parameter.<br>
</li>
<li>When calculating a confidence interval, it is common practice to use the form <span class="math display">\[\text{statistic}\pm\text{(critical value)(standard error of statistic)}\]</span> where the <strong>critical value</strong> is a multiplier that makes the interval wide enough to have the stated capture rate and the <strong>standard error</strong> of the statistic is an estimate of the standard deviation of the sampling distribution of the statistic used to estimate the parameter.<br>
</li>
<li>When the conditions are met, the specific formula for a <strong><span class="math inline">\(C\%\)</span> one-sample <span class="math inline">\(z\)</span> interval for a proportion</strong> is <span class="math display">\[\hat{p}\pm z^\ast\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\]</span> where <span class="math inline">\(z^\ast\)</span> is the critical value for the standard normal curve with <span class="math inline">\(C\%\)</span> of its area betweet <span class="math inline">\(-z^\ast\)</span> and <span class="math inline">\(z^\ast\)</span>.<br>
</li>
<li>When asked to construct and interpret a confidence interval, follow the <strong>four-step process</strong>:<br>
<strong>State</strong>: State the parameter you want to estimate and the confidence level.<br>
<strong>Plan</strong>: Identify the appropriate inference method and check the conditions.<br>
<strong>Do</strong>: If the conditions are met, perform calculations.<br>
<strong>Conclude</strong>: Interpret your interval in the context of the problem.<br>
</li>
<li>Other things being equal, the margin of error of a confidence interval gets smaller as:
<ul>
<li>The confidence level <span class="math inline">\(C\)</span> decreases.<br>
</li>
<li>THe sample size <span class="math inline">\(n\)</span> increases.<br>
</li>
</ul></li>
<li>The sample size needed to obtain a confidence interval with a maximum margin of error <span class="math inline">\(ME\)</span> for a population proportion involves solving <span class="math display">\[z^\ast\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\le ME\]</span> for <span class="math inline">\(n\)</span>, where <span class="math inline">\(\hat{p}\)</span> is a guessed value for the sample proportion, and <span class="math inline">\(z^\ast\)</span> is the critical value for the confidence level you want. Use <span class="math inline">\(\hat{p} = 0.5\)</span> if you don’t have a good idea about the value of <span class="math inline">\(\hat{p}\)</span>.</li>
</ul>
</section>
<section id="section-6c-significance-tests-the-basics" class="level3">
<h3 class="anchored" data-anchor-id="section-6c-significance-tests-the-basics">Section 6C: Significance Tests: The Basics</h3>
<ul>
<li>A <strong>significance test</strong> is a procedure for using observed data to decide between two competing claimes called hypotheses. The hypotheses are often statements about a parameter, like the population proportion <span class="math inline">\(p\)</span> or the population mean <span class="math inline">\(\mu\)</span>.<br>
</li>
<li>The claim that we weigh evidence <em>against</em> in a significance test is called the <strong>null hypothesis (<span class="math inline">\(H_0\)</span>)</strong>. The null hypothesis is usually a statement of no change or no difference and has the form <span class="math inline">\(H_0\)</span>: parameter = null value.<br>
</li>
<li>The claim about the population that we are trying to find evidence <em>for</em> is the <strong>alternative hypothesis (<span class="math inline">\(H_a\)</span>)</strong>.
<ul>
<li>A <strong>one-sided</strong> alternative hypothesis has the form <span class="math inline">\(H_a\text{: parameter}\lt\text{null value or }H_a\text{: paramter}\gt\text{null value}\)</span>.<br>
</li>
<li>A <strong>two-sided</strong> alternative hypothesis has the form <span class="math inline">\(H_a\text{: parameter}\ne\text{null value}\)</span>.<br>
</li>
</ul></li>
<li>The <strong><span class="math inline">\(P\)</span>-value</strong> of a test is the probability of getting evidence for the alternative hypothesis <span class="math inline">\(H_a\)</span> that is as strong as or stronger than the observed evidence when the null hypothesis <span class="math inline">\(H_0\)</span> is true.<br>
</li>
<li>Small <span class="math inline">\(P\)</span>-values are evidence against the null hypothesis and for the alternative hypothesis because they say that the observed result is unlikely to occur when <span class="math inline">\(H_0\)</span> is true. To determine if a <span class="math inline">\(P\)</span>-value should be considered small, we compare it to the <strong>significance level</strong>, such as <span class="math inline">\(\alpha=0.05\)</span>.<br>
</li>
<li>We make a conclusion in a significance test based on the <span class="math inline">\(P\)</span>-value.
<ul>
<li>If <span class="math inline">\(P\)</span>-value <span class="math inline">\(\le\alpha\)</span>: Reject <span class="math inline">\(H_0\)</span> and conclude there is convincing evidence for <span class="math inline">\(H_a\)</span> (in context).<br>
</li>
<li>If <span class="math inline">\(P\)</span>-value <span class="math inline">\(\gt\alpha\)</span>: Fail to reject <span class="math inline">\(H_0\)</span> and conclude there is not convincing evidence for <span class="math inline">\(H_a\)</span> (in context).</li>
</ul></li>
</ul>
</section>
<section id="section-6d-significance-tests-for-population-proportions" class="level3">
<h3 class="anchored" data-anchor-id="section-6d-significance-tests-for-population-proportions">Section 6D: Significance Tests for Population Proportions</h3>
<ul>
<li>Tests about the proportion <span class="math inline">\(p\)</span> of successes in a population are based on the sample proportion <span class="math inline">\(\hat{p}\)</span>.<br>
</li>
<li>To perform a <strong>one-sample <span class="math inline">\(z\)</span> test for a proportion</strong>, we need to verify that the observations in the sample can be viewed as independent and that the sampling distribution of <span class="math inline">\(\hat{p}\)</span> is approximately normal. The required conditions are:
<ul>
<li>Random: The data come from a random sample from the population of interest.
<ul>
<li><span class="math inline">\(10\%\)</span>: When sampling without replacement, <span class="math inline">\(n&lt;0.10N\)</span>.<br>
</li>
</ul></li>
<li>Large Counts: Both <span class="math inline">\(np_0\)</span> and <span class="math inline">\(n(1-p_0)\)</span> are at least <span class="math inline">\(10\)</span>, where <span class="math inline">\(p_0\)</span> is the proportion specified by the null hypothesis.<br>
</li>
</ul></li>
<li>In general, the formula for the <strong>standardized test statistic</strong> is <span class="math display">\[\text{standardized test statistic} = \frac{\text{statistic} - \text{parameter}}{\text{standard error of statistic}}\]</span></li>
<li>For a one-sample <span class="math inline">\(z\)</span> test for a proportion specifically, the standardized test statistic is <span class="math display">\[z = \frac{\hat{p}-p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}}\]</span></li>
<li>When the Large Counts condition is met, the standardized test statistic has approximately a standard normal distribution. You can use technology or Table A to find the <span class="math inline">\(P\)</span>-value.<br>
</li>
<li>Follow the <strong>four-step process</strong> when you perform a significance test:<br>
<strong>State</strong>: State the hypotheses, parameter(s), and significance level.<br>
<strong>Plan</strong>: Identify the appropriate inference method and check the conditions.<br>
<strong>Do</strong>: If the conditions are met, perform calculations:
<ul>
<li>Calculate the test statistic.<br>
</li>
<li>Find the <span class="math inline">\(P\)</span>-value.</li>
</ul></li>
</ul>
<p><strong>Conclude</strong>: Make a conclusion about the hypotheses in the context of the problem.<br>
* Confidence intervals provide additional information that significance tests do not–namely, a set of plausible values for the population proportion <span class="math inline">\(p\)</span>. A two-sided test of <span class="math inline">\(H_0\)</span>: <span class="math inline">\(p=p_0\)</span> at significance level <span class="math inline">\(\alpha\)</span> usually gives the same conclusion as a <span class="math inline">\(100(1-\alpha)\%\)</span> confidence interval.<br>
* When we make a conclusion in a significance test, there are two kinds of mistakes we can make. * A <strong>Type I error</strong> occurs if we reject <span class="math inline">\(H_0\)</span> when <span class="math inline">\(H_0\)</span> is true. In other words, the data give convincing evidence for <span class="math inline">\(H_a\)</span>, but <span class="math inline">\(H_a\)</span> isn’t true.<br>
* A <strong>Type II error</strong> occurs if we fail to reject <span class="math inline">\(H_0\)</span> when <span class="math inline">\(H_a\)</span> is true. In other words, the data don’t give convincing evidence for <span class="math inline">\(H_a\)</span>, but <span class="math inline">\(H_a\)</span> is true.<br>
* The probability of making a Type I error is equal to the significance level <span class="math inline">\(\alpha\)</span>. There is a trade-off between P(Type I error) and the P(Type II error): as one increases, the other decreases, assuming other factors remain the same. It is important to consider the possible consequences of each type of error before choosing a significance level.<br>
* The <strong>power</strong> of a test is the probability that the test will find convincing evidence for <span class="math inline">\(H_a\)</span> when a specific alternative value ofthe parameter is true. In other words, the power of a test is the probability of avoiding a Type II error. For a specific alternative, <span class="math inline">\(\text{power}=1-P(\text{Type II error})\)</span>.<br>
* We can increase the power of a significance test (decrease the probability of a Type II error) by increasing the sample size, increasing the significance level, or increasing the difference that is important to detect between the null and alternative parameter values (known as the <em>effect size</em>). Reducing the standard error with better data collection methods can also increase power.</p>
</section>
<section id="unit-6-part-i-review" class="level3">
<h3 class="anchored" data-anchor-id="unit-6-part-i-review">Unit 6, Part I Review</h3>
<section id="section-6a-confidence-intervals-the-basics-1" class="level4">
<h4 class="anchored" data-anchor-id="section-6a-confidence-intervals-the-basics-1">Section 6A: Confidence Intervals: The Basics</h4>
<p>In this section, you learned that a <strong>point estimate</strong> is the single best guess for the value of a population parameter. You also learned that a <strong>confidence interval</strong>, also known as an <strong>interval estimate</strong>, provides an interval of plausible values for a parameter based on sample data. To interpret a confidence interval, say, “We are <span class="math inline">\(C\%\)</span> confident that the interval from ______ to _______ captures the [parameter in context],” where <span class="math inline">\(C\)</span> is the confidence level of the interval. You can use a confidence interval to evaluate a claim about the value of a population parameter.</p>
<p>The <strong>confidence level</strong> <span class="math inline">\(C\)</span> describes the percentage of confidence intervals that we expect to capture the value of the parameter in repeated sampling. To interpret a <span class="math inline">\(C\%\)</span> confidence level, say, “If we took many samples of the same size from the same population and used them to construct <span class="math inline">\(C\%\)</span> confidence intervals, about <span class="math inline">\(C\%\)</span> of those intervals would capture the [parameter in context].”</p>
</section>
<section id="section-6b-confidence-intervals-for-a-population-proportion-1" class="level4">
<h4 class="anchored" data-anchor-id="section-6b-confidence-intervals-for-a-population-proportion-1">Section 6B: Confidence Intervals for a Population Proportion</h4>
<p>In this section, you learned how to construct and interpret confidence intervals for a population proportion. Three conditions must be met to ensure that the observations in the sample are independent and that the sampling distribution of <span class="math inline">\(\hat{p}\)</span> is approximately normal. First, the data used to calculate the interval must come from a random sample from the population of interest (the Random condition). When the sample is selected without replacement from the population, the sample size should be less than <span class="math inline">\(10\%\)</span> of the population size (the <span class="math inline">\(10\%\)</span> condition). Finally, the observed number of successes <span class="math inline">\(n\hat{p}\)</span> and observed number of failures <span class="math inline">\(n(1-\hat{p})\)</span> must both be at least 10 (the Large Counts condition).</p>
<p>Confidence intervals are formed by adding and subtracting the <strong>margin of error</strong> from the point estimate (value of the statistic): <span class="math display">\[\text{CI} = \text{point estimate} \pm \text{margin of error}\]</span> The margin of error has two components: (<span class="math inline">\(1\)</span>) the <strong>standard error</strong> of the statistic, which is an estimate of the standard deviation of the sampling distribution of the statistic, and (<span class="math inline">\(2\)</span>) the <strong>critical value</strong> based on the confidence level of the interval. <span class="math display">\[\text{CI} = \text{statistic} \pm \text{(critical value)(standard error of statistic)}\]</span> The specific formula for calculating a confidence interval for a population proportion is <span class="math display">\[\hat{p}\pm z^\ast\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\]</span> where <span class="math inline">\(\hat{p}\)</span> is the sample proportion, <span class="math inline">\(z^\ast\)</span> is the critical value, and <span class="math inline">\(n\)</span> is the sample size. To find <span class="math inline">\(z^\ast\)</span> use technology or Table A to determin the values of <span class="math inline">\(z^\ast\)</span> and <span class="math inline">\(-z^\ast\)</span> that capture the middle <span class="math inline">\(C\%\)</span> of the standard normal distribution, where <span class="math inline">\(C\)</span> is the confidence level.</p>
<p>The <strong>four-step process</strong> (State, Plan, Do, Conclude) is perfectly suited for problems that ask you to construct and interpret a confidence interval:<br>
- <strong>State</strong>: State the parameter you want to estimate and the confidence level.<br>
- <strong>Plan</strong>: Identify the appropriate inference method and check the conditions.<br>
- <strong>Do</strong>: If the conditions are met, perform calculations.<br>
- <strong>Conclude</strong>: Interpret your interval in the context of the problem.</p>
<p>The size of the margin of error is determined by several factors, including the confidence level <span class="math inline">\(C\)</span> and the sample size <span class="math inline">\(n\)</span>. Increasing the sample size <span class="math inline">\(n\)</span> makes the standard error of our statistic smaller, decreasing the margin of error. Increasing the confidence level <span class="math inline">\(C\)</span> makes the margin of error larger, to ensure that the capture rate of the interval increases to <span class="math inline">\(C\%\)</span>. Remember that the margin of error only accounts for sampling variability–it does not account for any bias in the data collection process.</p>
<p>Finally, an important part of planning a study is determining the size of the sample to be selected. The necessary sample size is based on the confidence level, the proportion of successes, and the desired margin of error. To calculate the minimum sample size, solve the following inequality for <span class="math inline">\(n\)</span>, where <span class="math inline">\(\hat{p}\)</span> <span class="math inline">\(\hat{p}\)</span> is a guessed value for the sample proportion: <span class="math display">\[z^\ast\sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \le ME\]</span> If you do not have an approximate value of <span class="math inline">\(\hat{p}\)</span> from a previous study or a pilot study, use <span class="math inline">\(\hat{p}=0.5\)</span> to determine the sample size that is guaranteed to yield a value less than or equal to the desired margin of error.</p>
</section>
<section id="section-6c-significance-tests-the-basics-1" class="level4">
<h4 class="anchored" data-anchor-id="section-6c-significance-tests-the-basics-1">Section 6C: Significance Tests: The Basics</h4>
<p>In this section, you learned the basic ideas of <strong>significance testing</strong>. Start by stating the hypotheses that you want to test. The <strong>null hypothesis</strong> (<span class="math inline">\(H_0\)</span>) is typically a statement of “no difference” and the <strong>alternative hypothesis</strong> (<span class="math inline">\(H_a\)</span>) describes what we expect is true. Remember that hypotheses are always about population parameters, not sample statistics.</p>
<p>When sample data provide evidence for the alternative hypothesis, there are two possible explanations: (<span class="math inline">\(1\)</span>) the null hypothesis is true, and data supporting the alternative hypothesis occurred just by chance, or (<span class="math inline">\(2\)</span>) the alternative hypothesis is true, and the data are consistent with an alternative value of the parameter. In a significance test, we evaluate Explanation 1 by assuming the null hypothesis is true and calculating the probability of getting evidence for the alternative hypothesis as strong as or stronger than the observed data. This probability is called a <strong><span class="math inline">\(P\)</span>-value</strong>.</p>
<p>To determine if the <span class="math inline">\(P\)</span>-value is small enough to reject <span class="math inline">\(H_0\)</span>, compare it to a predetermined <strong>significance level</strong> such as <span class="math inline">\(\alpha=0.05\)</span>. If the <span class="math inline">\(P\)</span>-value <span class="math inline">\(\le\alpha\)</span>, reject <span class="math inline">\(H_0\)</span>–there is convincing evidence that the alternative hypothesis is true.</p>
</section>
<section id="section-6d-significance-tests-for-a-population-proportion" class="level4">
<h4 class="anchored" data-anchor-id="section-6d-significance-tests-for-a-population-proportion">Section 6D: Significance Tests for a Population Proportion</h4>
<p>In this section, you learned the details of performing a significance test about a population proportion <span class="math inline">\(p\)</span>. After stating hypotheses and before doing calculations, check the conditions to verify that the observations in the sample are independent and that the observations in the sample are independent and that the sampling distribution of <span class="math inline">\(\hat{p}\)</span> is approximately normal. The Random condition requires that the sample be randomly selected from the population. The <span class="math inline">\(10\%\)</span> condition requires that the sample size be less than <span class="math inline">\(10\%\)</span> of the population size when sampling without replacement from the population. Finally, the Large Counts condition says that both <span class="math inline">\(np_0\)</span> and <span class="math inline">\(n(1-p_0)\)</span> must be at least <span class="math inline">\(10\)</span>, where <span class="math inline">\(p_0\)</span> is the value of <span class="math inline">\(p\)</span> in the null hypothesis.</p>
<p>The <strong>standardized test statistic</strong> measures how far the observed value of <span class="math inline">\(\hat{p}\)</span> is from the null hypothesis value, in standard error units. <span class="math display">\[\text{standardized test statistic} = \frac{\text{statistic} - \text{parameter}}{\text{standard error of statistic}}\]</span> <span class="math display">\[z = \frac{\hat{p}-p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}}\]</span> Find the <span class="math inline">\(P\)</span>-value by calculating the probability of getting a <span class="math inline">\(z\)</span> statistic this large or larger in the direction specified by the alternative hypothesis <span class="math inline">\(H_a\)</span> in the standard normal distribution. If you are performing a <strong>two-sided test</strong>, make sure to find the area in both tails of the standard normal distribution.</p>
<p>Whenever you are asked if there is convincing evidence for a claim about a population parameter, you are expected to respond using the familiar four-step process:<br>
<strong>State</strong>: State the hypotheses, parameter(s), and significance level.<br>
<strong>Plan</strong>: Identify the appropriate inference method and check the conditions.<br>
<strong>Do</strong>: If the conditions are met, perform calculations:<br>
* Calculate the test statistic.<br>
* Find the <span class="math inline">\(P\)</span>-value.</p>
<p><strong>Conclude</strong>: Make a conclusion about the hypotheses in the context of the problem.</p>
<p>You can also use a confidence interval to make a conclusion for a two-sided test. If the null parameter value is one of the plausible values in the interval, there isn’t convincing evidence that the alternative hypothesis is true. However, if the interval contains only values consistent with the alternative hypothesis, there is convincing evidence that the alternative hypothesis is true. Besides helping you draw a conclusion, the interval tells you which alternative parameter values are plausible.</p>
<p>Because conclusions are based on sample data, there is a possibility that the conclusion to a significance test will be incorrect. You can make two types of errors: A <strong>Type I error</strong> occurs if you find convincing evidence for the alternative hypothesis when, in reality, the null hypothesis is true. A <strong>Type II error</strong> occurs when you don’t find convincing evidence that the alternative hypothesis is true. The probability of making a Type I error is equal to the significance level (<span class="math inline">\(\alpha\)</span>) of the test. Decreasing the probability of a Type I error increases the probability of a Type II error, and increasing the probability of a Type I error decreases the probability of a Type II error.</p>
<p>The probability that you avoid making a Type II error when an alternative value of the parameter is true is called the <strong>power</strong> of the test. Power is good–if the alternative hypothesis is true, you want to maximize the probability of finding convincing evidence that it is true. We can increase the power of a significance test by increasing the sample size, by increasing the significance level, and by reducing the standard error with wise data collection methods. The power of a test will also be greater when the alternative value of the parameter is farther away from the null hypothesis value.</p>
</section>
</section>
<section id="inference-for-a-population-proportion" class="level3">
<h3 class="anchored" data-anchor-id="inference-for-a-population-proportion">Inference for a Population Proportion</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 35%">
<col style="width: 35%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Name (TI-83/84)</strong></td>
<td style="text-align: left;">One-sample <span class="math inline">\(z\)</span> interval for <span class="math inline">\(p\)</span> (1-PropZInt)</td>
<td style="text-align: left;">One-sample <span class="math inline">\(z\)</span> test for <span class="math inline">\(p\)</span> (1-PropZTest)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Null Hypothesis</strong></td>
<td style="text-align: left;"><em>Not applicable.</em></td>
<td style="text-align: left;"><span class="math inline">\(H_0:p=p_0\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Conditions</strong></td>
<td style="text-align: left;"><strong>Random</strong>: The data come from a random sample for the population of interest. <br><br><strong><span class="math inline">\(10\%\)</span></strong>: When sampling without replacement, <span class="math inline">\(n \lt 0.10N\)</span>. <br><br><strong>Large Counts</strong>: Both <span class="math inline">\(n\hat{p}\)</span> and <span class="math inline">\(n(1-\hat{p})\)</span> are at least <span class="math inline">\(10\)</span>.<br><br> That is, the number of successes and the number of failures in the sample are both at least 10.</td>
<td style="text-align: left;"><strong>Random</strong>: The data come from a random sample for the population of interest. <br><br><strong><span class="math inline">\(10\%\)</span></strong>: When sampling without replacement, <span class="math inline">\(n \lt 0.10N\)</span>. <br><br><strong>Large Counts</strong>: Both <span class="math inline">\(np_0\)</span> and <span class="math inline">\(n(1-p_0)\)</span> are at least <span class="math inline">\(10\)</span><br><br> where <span class="math inline">\(p_0\)</span> is the proportion specified by the null hypothesis.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Formula</strong></td>
<td style="text-align: left;"><span class="math display">\[\hat{p} \pm z^\ast\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\]</span><br> Critical value <span class="math inline">\(z^\ast\)</span> from the standard normal distribution</td>
<td style="text-align: left;"><span class="math display">\[z = \frac{\hat{p}-p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}}\]</span><br> <span class="math inline">\(P\)</span>-value from the standard normal distribution</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="part-ii-inference-for-two-samples-1" class="level2">
<h2 class="anchored" data-anchor-id="part-ii-inference-for-two-samples-1">Part II: Inference for Two Samples</h2>
<section id="introduction-7" class="level3">
<h3 class="anchored" data-anchor-id="introduction-7">Introduction</h3>
<p>In Part I of Unit 6, you learned about confidence intervals and significance tests for a population proportion <span class="math inline">\(p\)</span>. Many interesting statistical questions involve <em>comparing</em> the proportion of successes in two populations. Whatis the difference between the proportion of Democrats and the proportion of Republicans who favor the death penalty? Has there been a change in the proportion of U.S. adults who watch the Super Bowl in the last 5 years? In both of these cases, we are interested in the difference <span class="math inline">\(p_1-p_2\)</span>, where <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> are the proportions of success in Population 1 and Population 2.</p>
<p>Other statistical questions involved comparing the effectiveness of two treatments in an experiment. For example, is a new medication more effective than a current medication for relieving headaches? What is the difference in the survival rate for two cancer treatments? In these cases, we are also interested in the difference <span class="math inline">\(p_1-p_2\)</span>. But in the settings, <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> are the true proportions of success for individuals like the ones in the experiment who receive Treatment <span class="math inline">\(1\)</span> or Treatment <span class="math inline">\(2\)</span>.</p>
<p>For their response bias project (page 312), Sarah and Miranda investigated whether the characteristics of the interviewer can affect the response to a survey question. At the Tucson Mall, they asked 60 shoppers the question, “Do you like tattoos?” When interviewing 30 of the shoppers, Sarah and Miranda wore long-sleeved shirts that covered their tattoos. For the remaining 30 shoppers, the interviewers wore tank tops that revealed their tattoos. The choice of long sleeve or tank top was determined at random for each subject. Sarah and Miranda suspected that more people would answer “Yes” when their tattoos were visible.</p>
<p>What happened in the experiment? The two-way table summarizes the results:</p>
<center>
<p><strong>Clothing worn</strong><br></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Like tattoos?</th>
<th style="text-align: center;">Tank top</th>
<th style="text-align: center;">Long sleeves</th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Yes</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">32</td>
</tr>
<tr class="even">
<td style="text-align: left;">No</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">28</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Total</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">60</td>
</tr>
</tbody>
</table>
</center>
<p>The difference (Tank top - Long sleeves) in the proportions of people who said they like tattoos is <span class="math inline">\(18/30-14/30 = 0.600–0.467 = 0.133\)</span>. does this difference provide convincing evidence that the appearance of the interviewer has the intended effect on the response, what would the difference be due to chance variation in the random assignment?</p>
</section>
<section id="section-6e-confidence-intervals-for-a-difference-in-population-proportions" class="level3">
<h3 class="anchored" data-anchor-id="section-6e-confidence-intervals-for-a-difference-in-population-proportions">Section 6E: Confidence Intervals for a Difference in Population Proportions</h3>
<ul>
<li>confidence intervals for the difference <span class="math inline">\(p_1-p_2\)</span> between the proportions of successes in two populations or treatments use the difference <span class="math inline">\(\hat{p}_1-\hat{p}_2\)</span> between the sample proportions as the point estimate.<br>
</li>
<li>When constructing a confidence interval for a difference in population proportions, we must check for independence in the data collection process and that the sampling distribution of <span class="math inline">\(\hat{p}_1-\hat{p}_2\)</span> is approximately normal. The required conditions are
<ul>
<li>Random: the data come from two independent random samples or from two groups in a randomized experiment.
<ul>
<li><span class="math inline">\(10\%\)</span>: when sampling without replacement, <span class="math inline">\(n_1 &lt; 0.10N_1\text{ and }n_2 &lt; 0.10N_2\)</span>.</li>
</ul></li>
<li>Large Counts: the counts of “successes” and “failures” in each sample or group – <span class="math inline">\(n_1\hat{p}_1, n_1(1 - \hat{p}_1), n_2\hat{p}_2\text{, and }n_2(1 - \hat{p}_2)\)</span> - are all at least <span class="math inline">\(10\)</span>.</li>
</ul></li>
<li>When conditions are met, a <span class="math inline">\(C\%\)</span> confidence interval for <span class="math inline">\(p_1-p_2\)</span> is <span class="math display">\[(\hat{p}_1-\hat{p}_2)\pm z^\ast\sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1}+\frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}\]</span> where <span class="math inline">\(z^\ast\)</span>is the critical value for the standard normal curve with <span class="math inline">\(C\%\)</span> of its area between <span class="math inline">\(-z^\ast\)</span> and <span class="math inline">\(z^\ast\)</span>. This is called a <strong>two-sample <span class="math inline">\(z\)</span> interval for a difference in proportions</strong>.<br>
</li>
<li>When performing inference for experiments, make sure to include “for individuals like the ones in the study” and avoid talking about the sample proportions when you are defining parameters and making your conclusion.</li>
<li>Be sure to follow the four step process whenever you construct an interpret a confidence interval for the difference between two proportions.</li>
</ul>
</section>
<section id="section-6f-significance-tests-for-a-difference-in-population-proportions" class="level3">
<h3 class="anchored" data-anchor-id="section-6f-significance-tests-for-a-difference-in-population-proportions">Section 6F: Significance Tests for a Difference in Population Proportions</h3>
<ul>
<li>Tests for the difference <span class="math inline">\(p_1-p_2\)</span> between the proportions of successes in two populations or treatments are based on the difference <span class="math inline">\(\hat{p}_1-\hat{p}_2\)</span> between the sample proportions.<br>
</li>
<li>The usual null hypothesis for a significance test about the difference between two population proportions is $ H_0: p_1 - p_2 = 0$ or <span class="math inline">\(H_0: p_1=p_2\)</span>. The alternative hypothesis says what kind of difference we expect.</li>
<li>When testing a claim about a difference in proportions, we must check for independence in the data collection process and that the sampling distribution of <span class="math inline">\(\hat{p}_1-\hat{p}_2\)</span> is approximately normal. The required conditions are
<ul>
<li>Random: The data come from two independent random samples or from two groups in a randomized experiment.
<ul>
<li><span class="math inline">\(10\%\)</span>: When sampling without replacement, <span class="math inline">\(n_1 &lt; 0.10N_1\text{ and }n_2 &lt; 0.10N_2\)</span>.</li>
</ul></li>
<li>Large Counts: the expected counts of successes and failures in each sample or group – <span class="math inline">\(n_1\hat{p}_C, n_1(1 - \hat{p}_C), n_2\hat{p}_C\text{, and }n_2(1 - \hat{p}_C)\)</span> - are all at least <span class="math inline">\(10\)</span>, where <span class="math inline">\(\hat{p}_C\)</span> is the combined (pooled) sample proportion. To calculate <span class="math inline">\(\hat{p}_C\)</span>, combine the two samples and divide the total number of successes by the total sample size: <span class="math display">\[\hat{p}_C = \frac{X_1+X_2}{n_1+n_2}\]</span><br>
</li>
</ul></li>
<li>When conditions are met, the <strong>two-sample <span class="math inline">\(z\)</span> test for a difference in proportions</strong> uses the standardized test statistic <span class="math display">\[z=\frac{(\hat{p}_1-\hat{p}_2) - 0}{\sqrt{\hat{p}_C(1-\hat{p}_C)(\frac{1}{n_1}+\frac{1}{n_2})}}\]</span> with <span class="math inline">\(P\)</span>-values calculated from the standard normal distribution.<br>
</li>
<li>Be sure to follow the four-step process whenever you perform a significance test about a difference in proportions.</li>
</ul>
</section>
<section id="unit-6-part-ii-review" class="level3">
<h3 class="anchored" data-anchor-id="unit-6-part-ii-review">Unit 6, Part II Review</h3>
<section id="section-6e-confidence-intervals-for-a-difference-in-population-proportions-1" class="level4">
<h4 class="anchored" data-anchor-id="section-6e-confidence-intervals-for-a-difference-in-population-proportions-1">Section 6E: Confidence Intervals for a Difference in Population Proportions</h4>
<p>In this section, you learned how to construct confidence in her force for a difference between two population proportions. To verify independence and data collection and that the sampling distribution of <span class="math inline">\(\hat{p}_1-\hat{p}_2\)</span> Is approximately normal, we checked three conditions. The Random condition says that the data must be from two independent random samples or two groups in a randomized experiment. The <span class="math inline">\(10\%\)</span> condition says that each sample size should be less than <span class="math inline">\(10\%\)</span> of the corresponding population size when sampling without replacement. The Large Counts condition says that the number of successes and the number of failures from each sample should be at least <span class="math inline">\(10\)</span> - that is, <span class="math inline">\(n_1\hat{p}_1, n_1(1 - \hat{p}_1), n_2\hat{p}_2\text{, and }n_2(1 - \hat{p}_2)\text{ are }\ge10\)</span>.</p>
<p>A confidence interval for a difference between two proportions provides an interval of plausible values for the difference in the population proportions. The formula is <span class="math display">\[(\hat{p}_1-\hat{p}_2)\pm z^\ast\sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1}+\frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}\]</span></p>
<p>The logic of confidence intervals, including how to interpret the confidence interval and the confidence level, is the same as when estimating a single population proportion. Likewise, you can use a confidence interval for a difference in proportions to evaluate claims about the population proportions. For example, if <span class="math inline">\(0\)</span> is not included in a confidence interval for <span class="math inline">\(p_1-p_2\)</span>, there is convincing evidence that the population proportions are different.</p>
</section>
<section id="section-6f-significance-tests-for-a-difference-in-population-proportions-1" class="level4">
<h4 class="anchored" data-anchor-id="section-6f-significance-tests-for-a-difference-in-population-proportions-1">Section 6F: Significance Tests for a Difference in Population Proportions</h4>
<p>In this section, you learned how to perform significance tests for a difference between two proportions. As in any test, you start by stating hypotheses. In AP Statistics, we focus on the null hypothesis of $ H_0: p_1 - p_2 = 0$ (or, equivalently, <span class="math inline">\(H_0: p_1=p_2\)</span>). The alternative hypothesis can be one-sided (<span class="math inline">\(\lt\)</span>,<span class="math inline">\(\gt\)</span>) or two-sided (<span class="math inline">\(\ne\)</span>).</p>
<p>As with confidence intervals for <span class="math inline">\(p_1-p_2\)</span>, we check the conditions to verify independence in the data collection process and that the sampling distribution of <span class="math inline">\(\hat{p}_1-\hat{p}_2\)</span> is approximately normal. The Random condition says that the data must be from two independent random samples or two groups in a randomized experiment. The <span class="math inline">\(10\%\)</span> condition says that each sample size should be less than <span class="math inline">\(10\%\)</span> of the corresponding population size when sampling without replacement. The Large Counts condition says that the <em>expected</em> numbers of successes and failures in each sample should be at least <span class="math inline">\(10\)</span>. For a test of $ H_0: p_1 - p_2 = 0$, we estimate the common value <span class="math inline">\(p\)</span> of the parameters <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> using the combined (pooled) proportion of successes in the two samples: <span class="math inline">\(\hat{p}_C = \frac{X_1+X_2}{n_1+n_2}\)</span>. Consequently, the Large Counts condition requires us to check that <span class="math inline">\(n_1\hat{p}_C, n_1(1 - \hat{p}_C), n_2\hat{p}_C\text{, and }n_2(1 - \hat{p}_C)\)</span> are all at least <span class="math inline">\(10\)</span>.</p>
<p>For a test of $ H_0: p_1 - p_2 = 0$, the standardized test statistic is <span class="math display">\[z=\frac{(\hat{p}_1-\hat{p}_2) - 0}{\sqrt{\hat{p}_C(1-\hat{p}_C)(\frac{1}{n_1}+\frac{1}{n_2})}}\]</span> When conditions are met, <span class="math inline">\(P\)</span>-values can be obtained from the standard normal distribution.</p>
<p>A significance test for a difference between two proportions uses the same logic as the significance test for one population proportion from Section 6D. Likewise, the way we interpret <span class="math inline">\(P\)</span>-values and power and the way we describe Type I and Type II errors is very similar to what you learned in Section 6D.</p>
</section>
</section>
<section id="inference-for-a-difference-in-proportions" class="level3">
<h3 class="anchored" data-anchor-id="inference-for-a-difference-in-proportions">Inference for a Difference in Proportions</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 35%">
<col style="width: 35%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Name (TI-83/84)</strong></td>
<td style="text-align: left;">Two-sample <span class="math inline">\(z\)</span> interval for <span class="math inline">\(p_1-p_2\)</span> (2-PropZInt)</td>
<td style="text-align: left;">Two-sample <span class="math inline">\(z\)</span> test for <span class="math inline">\(p_1-p_2\)</span> (2-PropZTest)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Null Hypothesis</strong></td>
<td style="text-align: left;"><em>Not applicable.</em></td>
<td style="text-align: left;">$ H_0: p_1 - p_2 = 0$</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Conditions</strong></td>
<td style="text-align: left;"><strong>Random</strong>: The data come from two independent random samples or from two groups in a randomized experiment. <br><br><strong><span class="math inline">\(10\%\)</span></strong>: When sampling without replacement, <span class="math inline">\(n_1 &lt; 0.10N_1\text{, and }n_2 &lt; 0.10N_2\)</span>. <br><br><strong>Large Counts</strong>: The counts of “successes” and “failures” in each sample or group–<span class="math inline">\(n_1\hat{p}_1, n_1(1 - \hat{p}_1), n_2\hat{p}_2\text{, and }n_2(1 - \hat{p}_2)\)</span>–are all at least <span class="math inline">\(10\)</span>.<br><br></td>
<td style="text-align: left;"><strong>Random</strong>: The data come from two independent random samples or from two groups in a randomized experiment. <br><br><strong><span class="math inline">\(10\%\)</span></strong>: When sampling without replacement, <span class="math inline">\(n_1 &lt; 0.10N_1\text{, and }n_2 &lt; 0.10N_2\)</span>. <br><br><strong>Large Counts</strong>: The expected counts of “successes” and “failures” in each sample or group–<span class="math inline">\(n_1\hat{p}_C, n_1(1 - \hat{p}_C), n_2\hat{p}_C\text{, and }n_2(1 - \hat{p}_C)\)</span>–are all at least <span class="math inline">\(10\)</span>,<br><br> where <span class="math inline">\(\hat{p}_C = \frac{X_1+X_2}{n_1+n_2}\)</span>.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Formula</strong></td>
<td style="text-align: left;"><span class="math display">\[(\hat{p}_1-\hat{p}_2)\pm z^\ast\sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1}+\frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}\]</span><br> Critical value <span class="math inline">\(z^\ast\)</span> from the standard normal distribution</td>
<td style="text-align: left;"><span class="math display">\[z=\frac{(\hat{p}_1-\hat{p}_2) - 0}{\sqrt{\hat{p}_C(1-\hat{p}_C)(\frac{1}{n_1}+\frac{1}{n_2})}}\]</span><br> <span class="math inline">\(P\)</span>-value from the standard normal distribution</td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
<section id="unit-7-inference-for-quantitative-data-means-1" class="level1">
<h1>Unit 7: Inference for Quantitative Data: Means</h1>
<section id="introduction-8" class="level3">
<h3 class="anchored" data-anchor-id="introduction-8">Introduction</h3>
<p>In Unit 6, you learned the basics of confidence intervals and significance tests, along with the details of estimating and testing claims about population proportions. In Unit 7, we revisit the big ideas–including possible errors that can occurr when performing statistical inference–but focus on estimating and testing claims about population means. There’s one key difference between these two units: inference about proportions involves <em>categorical</em> data; inference about means involves <em>quantitative</em> data. However, one thing remains the same: you will need to apply the skills of selecting statistical methods, using probability and simulation, and statistical argumentation to perform inference successfully.</p>
</section>
<section id="section-7a-confidence-intervals-for-a-population-mean-or-mean-difference" class="level3">
<h3 class="anchored" data-anchor-id="section-7a-confidence-intervals-for-a-population-mean-or-mean-difference">Section 7A: Confidence Intervals for a Population Mean or Mean Difference</h3>
<ul>
<li>Confidence intervals for the population mean <span class="math inline">\(\mu\)</span> use the sample mean <span class="math inline">\(\bar{x}\)</span> as the point estimate.<br>
</li>
<li>Because the population standard deviation <span class="math inline">\(\sigma\)</span> is usually unknown, we use the sample standard deviation <span class="math inline">\(s_x\)</span> to estimate <span class="math inline">\(\sigma\)</span> when constructing a confidence interval for a population mean. Doing so requires the use of a <strong><span class="math inline">\(t\)</span>-distribution</strong> with <span class="math inline">\(n-1\)</span> <em>degrees of freedom</em> (df) to calculate a <span class="math inline">\(t^\ast\)</span> critical value rather than the standard normal distribution and a <span class="math inline">\(z^\ast\)</span> critical value.</li>
<li>A <span class="math inline">\(t\)</span>-dstribution is described by a symmetric, single-peaked, bell-shaped density curve centered at <span class="math inline">\(0\)</span>. Any <span class="math inline">\(t\)</span>-distribution is completely specified by its degrees of freedom (df) and has more area in its tails than the standard normal distribution.<br>
</li>
<li>When constructing a confidence interval for a population mean <span class="math inline">\(\mu\)</span>, we need to ensure that the observations in the sample can be viewed as independent and that the observations in the sample can be viewed as independent and that the sampling distribution of <span class="math inline">\(\bar{x}\)</span> is approximately normal. The required conditions are:
<ul>
<li>Random: The data come from a random sample from the population of interest.
<ul>
<li><span class="math inline">\(10\%\)</span>: When sampling without replacement, <span class="math inline">\(n \lt 0.10N\)</span>.<br>
</li>
</ul></li>
<li>Normal/Large Sample: The population distribution is approximately normal or the sample size is large (<span class="math inline">\(n \ge 30\)</span>). If the population has unkown shape and <span class="math inline">\(n \lt 30\)</span>, a graph of the sample data shows no strong skewness or outliers.<br>
</li>
</ul></li>
<li>The <strong>standard error of the sample mean <span class="math inline">\(\bar{x}\)</span></strong> is an estimate of the standard deviation of the sampling distribution of <span class="math inline">\(\bar{x}\)</span>: <span class="math display">\[s_{\bar{x}}=\frac{s_x}{\sqrt{n}}\]</span> This value estimates how much the sample mean <span class="math inline">\(\bar{x}\)</span> typically varies from the population mean <span class="math inline">\(\mu\)</span> in random samples of size <span class="math inline">\(n\)</span>.<br>
</li>
<li>When conditions are met, a <span class="math inline">\(C\%\)</span> confidence interval for the population mean <span class="math inline">\(\mu\)</span> is given by <span class="math display">\[\bar{x}\pm t^\ast\frac{s_x}{\sqrt{n}}\]</span> where <span class="math inline">\(t^\ast\)</span> is the critical value for a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(\text{df} = n-1\)</span> and <span class="math inline">\(C\%\)</span> of its area between <span class="math inline">\(-t^\ast\)</span> and <span class="math inline">\(t^\ast\)</span>. This is known as a <strong>one-sample t interval for a mean</strong>.<br>
</li>
<li>When estimating a population mean, the margin of error will be smaller when the sample size increases and larger when the confidence level increases, assuming that everything else remains the same.<br>
</li>
<li><strong>Paired data</strong> result from recording two values of the same quantitative variable for each individual or for each pair of similar individuals.<br>
</li>
<li>To analyze paired data, start by computing the difference for each pair. Then make a graph of the differences. Uses the mean difference <span class="math inline">\(\bar{x}_{\text{Diff}}\)</span> and the standard deviation of the differences <span class="math inline">\(s_{\text{Diff}}\)</span> as summary statistics.<br>
</li>
<li>Confidence intervals for a population mean difference <span class="math inline">\(\mu_{\text{Diff}}\)</span> use the sample mean difference <span class="math inline">\(\bar{x}_{\text{Diff}}\)</span> as the point estimate.<br>
</li>
<li>Befor estimating <span class="math inline">\(\mu_{\text{Diff}}\)</span>, we need to check for independence of the individual differences and confirm that the sampling distribution of <span class="math inline">\(\bar{x}_{\text{Diff}}\)</span> is approximately normal. The required conditions are:
<ul>
<li>Random: Paired data come from a random sample from the population of interest or from a randomized experiment.
<ul>
<li><span class="math inline">\(10\%\)</span>: When sampling without replacement, <span class="math inline">\(n_{\text{Diff}}\lt 0.10N_{\text{Diff}}\)</span>.</li>
</ul></li>
<li>Normal/Large Sample: The population distribution of differences is approximately normal or the sample size is large (<span class="math inline">\(n_{\text{Diff}} \ge 30\)</span>). If the population distribution of differences has unknown shape and the number of differences in the sample is less than <span class="math inline">\(30\)</span>, a graph of the sample differences shows no strong skewness or outliers.<br>
</li>
</ul></li>
<li>When the conditions are met, a <span class="math inline">\(C\%\)</span> confidence interval for the population mean difference <span class="math inline">\(\mu_{\text{Diff}}\)</span> is <span class="math display">\[\bar{x}_{\text{Diff}} \pm t^\ast\frac{s_{\text{Diff}}}{\sqrt{n_{\text{Diff}}}}\]</span> where <span class="math inline">\(t^\ast\)</span> is the critical value for a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(\text{df} = n_{\text{Diff}}-1\)</span> and <span class="math inline">\(C\%\)</span> of its area between <span class="math inline">\(-t^\ast\)</span> and <span class="math inline">\(t^\ast\)</span>. This is called a <strong>paired <span class="math inline">\(t\)</span> interval for a mean difference</strong>(or a one-sample <span class="math inline">\(t\)</span> interval for a mean difference).<br>
</li>
<li>Follow the four-step process–State, Plan, Do, Conclude–whenever you are asked to construct and interpret a confidence interval for a population mean <span class="math inline">\(\mu\)</span> or a population mean difference <span class="math inline">\(\mu_{\text{Diff}}\)</span>.</li>
</ul>
</section>
<section id="section-7b-significance-tests-for-a-population-mean-or-mean-difference" class="level3">
<h3 class="anchored" data-anchor-id="section-7b-significance-tests-for-a-population-mean-or-mean-difference">Section 7B: Significance Tests for a Population Mean or Mean Difference</h3>
<ul>
<li>Tests about a population mean <span class="math inline">\(\mu\)</span> are based on the sample mean <span class="math inline">\(\bar{x}\)</span>.<br>
</li>
<li>A significance test about a population mean starts with stating hypotheses. The null hypothesis is <span class="math inline">\(H_0: \mu = \mu_0\)</span> and the alternative hypothesis can be one-sided (<span class="math inline">\(H_a: \mu \lt \mu_0 \text{ or } H_a: \mu \gt \mu_0\)</span>) or two-sided (<span class="math inline">\(H_a: \mu \ne \mu_0\)</span>).<br>
</li>
<li>To perform a test of <span class="math inline">\(H_0: \mu = \mu_0\)</span>, we need to ensure that the observations in the sample can be viewed as independent and that the sampling distribution of <span class="math inline">\(\bar{x}\)</span> is approximately normal. The required conditions are:
<ul>
<li>Random: The data come from a random sample from the population of interest.
<ul>
<li><span class="math inline">\(10\%\)</span>: when sampling without replacement, <span class="math inline">\(n \lt 0.10N\)</span>.<br>
</li>
</ul></li>
<li>Normal/Large Sample: The population distribution is approximately normal or the sample size is large (<span class="math inline">\(n \ge 30\)</span>). If the population has unkown shape and <span class="math inline">\(n \lt 30\)</span>, a graph of the sample data shows no strong skewness or outliers.<br>
</li>
</ul></li>
<li>The standardized test statistic for a <strong>one-sample <span class="math inline">\(t\)</span> test for a mean</strong> is <span class="math display">\[t = \frac{\bar{x}-\mu_0}{\frac{s_x}{\sqrt{n}}}\]</span> When the conditions are met, find the <span class="math inline">\(P\)</span>-value by calculating the probability of getting a <span class="math inline">\(t\)</span> statistic this large or larger in the direction specified by the alternative hypothesis <span class="math inline">\(H_a\)</span> in a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(\text{df} = n-1\)</span>.<br>
</li>
<li>Confidence intervals provide additional information that significance tests do not–namely, a set of plausible values for the parameter <span class="math inline">\(\mu\)</span>. A <span class="math inline">\(95\%\)</span> confidence interval for <span class="math inline">\(\mu\)</span> gives consistent results with a two-sided test of <span class="math inline">\(H_0: \mu_{\text{Diff}}=0\)</span>. Be sure to specify the order of subtraction when defining the parameter.<br>
</li>
<li>Significance tasks about a population mean difference <span class="math inline">\(\mu_{\text{Diff}}\)</span> are based on the sample mean difference <span class="math inline">\(\bar{x}_{\text{Diff}}\)</span>.<br>
</li>
<li>Before testing a claim about <span class="math inline">\(\mu_{\text{Diff}}\)</span>, we need to check for independence of the differences and that the sampling distribution of <span class="math inline">\(\bar{x}_{\text{Diff}}\)</span> is approximately normal. The required conditions are:
<ul>
<li>Random: Paired data come from a random sample from the population of interest or from a randomized experiment.
<ul>
<li><span class="math inline">\(10\%\)</span>: when sampling without replacement, <span class="math inline">\(n_{\text{Diff}} \lt 0.10N_{\text{Diff}}\)</span>.<br>
</li>
</ul></li>
<li>Normal/Large Sample: The population distribution is approximately normal or the sample size is large (<span class="math inline">\(n \ge 30\)</span>). If the population has unkown shape and <span class="math inline">\(n \lt 30\)</span>, a graph of the sample data shows no strong skewness or outliers.<br>
</li>
</ul></li>
<li>The standardized test statistic for a <strong>paired <span class="math inline">\(t\)</span> test for a mean difference</strong> (also called a one-sample <span class="math inline">\(t\)</span> test for a mean difference)is <span class="math display">\[t = \frac{\bar{x}_{\text{Diff}}-0}{\frac{s_{\text{Diff}}}{\sqrt{n_{\text{Diff}}}}}\]</span> When the conditions are met, find the <span class="math inline">\(P\)</span>-value using the <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(\text{df} = n_{\text{Diff}}-1\)</span>.<br>
</li>
<li>Follow the four-step process–State, Plan, Do, Conclude–whenever you are asked to perform a significance test about a population mean <span class="math inline">\(\mu\)</span> or a population mean difference <span class="math inline">\(\mu_{\text{Diff}}\)</span>.</li>
</ul>
</section>
<section id="section-7c-confidence-intervals-for-a-difference-in-population-means" class="level3">
<h3 class="anchored" data-anchor-id="section-7c-confidence-intervals-for-a-difference-in-population-means">Section 7C: Confidence Intervals for a Difference in Population Means</h3>
<ul>
<li>Confidence intervals for the difference <span class="math inline">\(\mu_1-\mu_2\)</span> between the means of two populations or the true mean responses to two treatments use the difference <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span> between sample means as the point estimate.<br>
</li>
<li>Before constructing a confidence interval for <span class="math inline">\(\mu_1-\mu_2\)</span>, we need to check for independence and confirm that the sampling distribution of <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span> is approximately normal. The required conditions are:
<ul>
<li>Random: The data come from two independent random samples or from two groups in a randomized experiment.
<ul>
<li><span class="math inline">\(10\%\)</span>: When sampling without replacement, <span class="math inline">\(n_1 \lt 0.10N_1 \text{ and } n_2 \lt 0.10N_2\)</span>.<br>
</li>
</ul></li>
<li>Normal/Large Sample: For each sample, the data come from an approximately normally distributed population or the sample size is large (<span class="math inline">\(n \ge 30\)</span>). For each sample, if the population distribution has unknown shape and <span class="math inline">\(n \lt 30\)</span>, a graph of the sample data shows no strong skewness or outliers.<br>
</li>
</ul></li>
<li>When the conditions are met, a <span class="math inline">\(C\%\)</span> confidence interval for <span class="math inline">\(\mu_1-\mu_2\)</span> is <span class="math display">\[(bar{x}_1-\bar{x}_2)\pm t^\ast\sqrt{\frac{s^2_1}{n_1}+\frac{s^2_2}{n_2}}\]</span> where <span class="math inline">\(t^\ast\)</span> is the critical value with <span class="math inline">\(C\%\)</span> of its area between <span class="math inline">\(-t^\ast\)</span> and <span class="math inline">\(t^\ast\)</span> for the <span class="math inline">\(t\)</span> distribution with degrees of freedom given by technology. This is called a <strong>two-sample <span class="math inline">\(t\)</span> interval for a difference in means</strong>.</li>
</ul>
</section>
<section id="section-7d-significance-tests-for-a-difference-in-population-means" class="level3">
<h3 class="anchored" data-anchor-id="section-7d-significance-tests-for-a-difference-in-population-means">Section 7D: Significance Tests for a Difference in Population Means</h3>
<ul>
<li>Tests for the difference <span class="math inline">\(\mu_1-\mu_2\)</span> between the means of two populations or treatments are based on the difference <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span> between the sample means.<br>
</li>
<li>Before testing a claim about <span class="math inline">\(\mu_1-\mu_2\)</span>, we need to check for independence in the data collection process and verify that the sampling distribution of <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span> is approximately normal. The required conditions are:
<ul>
<li>Random: The data come from two independent random samples or from two groups in a randomized experiment.
<ul>
<li><span class="math inline">\(10\%\)</span>: When sampling without replacement, <span class="math inline">\(n_1 \lt 0.10N_1 \text{ and } n_2 \lt 0.10N_2\)</span>.<br>
</li>
</ul></li>
<li>Normal/Large Sample: For each sample, the data come from an approximately normally distributed population or the sample size is large (<span class="math inline">\(n \ge 30\)</span>). For each sample, if the population distribution has unknown shape and <span class="math inline">\(n \lt 30\)</span>, a graph of the sample data shows no strong skewness or outliers.<br>
</li>
</ul></li>
<li>To test <span class="math inline">\(H_0: \mu_1-\mu_2 = 0\)</span>, use a <strong>two-sample <span class="math inline">\(t\)</span> test for a difference in means</strong>. The standardized test statistic is <span class="math display">\[t=\frac{(\bar{x}_1-\bar{x}_2)-0}{ \sqrt{\frac{s^2_1}{n_1}+\frac{s^2_2}{n_2}}}\]</span> <span class="math inline">\(P\)</span>-values are calculated using the <span class="math inline">\(t\)</span> distribution with degrees of freedom obtained by technology.<br>
</li>
<li>Be sure to follow the four-step process whenever you perform a significance test for a difference between two population means.<br>
</li>
<li>The proper inference method depends on how the data were produced. For paired data involving one quantitative variable, use paired <span class="math inline">\(t\)</span> procedures to perform inference about <span class="math inline">\(\mu_{\text{Diff}}\)</span>. For quantitative data that come from independent random samples from two populations of interest or from two groups in a randomized experiment, use two-sample <span class="math inline">\(t\)</span> procedures to perform inference about <span class="math inline">\(\mu_1-\mu_2\)</span>.<br>
</li>
<li>Very small deviations from the null hypothesis can be highly significant (small <span class="math inline">\(P\)</span>-value) when a test is based on a large sample. A statistically significant result may not be practically important.<br>
</li>
<li>Many tests run at once will likely produce some significant results by chance alone, even if all the null hypotheses are true. Beware of <span class="math inline">\(P\)</span>-hacking.</li>
</ul>
</section>
<section id="unit-7-review" class="level3">
<h3 class="anchored" data-anchor-id="unit-7-review">Unit 7, Review</h3>
<section id="section-7a-confidence-intervals-for-a-population-mean-or-mean-difference-1" class="level4">
<h4 class="anchored" data-anchor-id="section-7a-confidence-intervals-for-a-population-mean-or-mean-difference-1">Section 7A: Confidence Intervals for a Population Mean or Mean Difference</h4>
<p>In this section, you learned how to construct an interpret confidence intervals for a population mean <span class="math inline">\(\mu\)</span>. To verify independence in data collection and that the sampling distribution of <span class="math inline">\(\bar{x}\)</span> is approximately normal, we check three conditions. The Random condition says that the data come from a random sample from the population of interest. the <span class="math inline">\(10\%\)</span> condition says that the sample size must be less than <span class="math inline">\(10\%\)</span> of the population size when sampling without replacement. the Normal/Large Sample condition says that the population is approximately normally distributed or the sample size is at least <span class="math inline">\(30\)</span>. If the population distribution’s shape is unknown and the sample size is less than <span class="math inline">\(30\)</span>, graph the sample data and check for strong skewness or outliers. If there is no strong skewness or outliers, it is reasonable to assume that the population distribution is approximately normal.</p>
<p>Because the population standard deviation <span class="math inline">\(\sigma\)</span> is usually unknown, we used the sample standard deviation <span class="math inline">\(s_x\)</span> to estimate <span class="math inline">\(\sigma\)</span> when constructing a confidence interval for a population mean <span class="math inline">\(\mu\)</span>. Doing so requires use of a <strong><span class="math inline">\(t\)</span> distribution</strong> with <span class="math inline">\(n-1\)</span> <em>degrees of freedom</em> (df) to calculate a <span class="math inline">\(t^\ast\)</span> critical value rather than the standard normal distribution. A <span class="math inline">\(t\)</span> distribution is described by a symmetric, single-peaked, bell-shaped density curve centered at <span class="math inline">\(0\)</span>. Any <span class="math inline">\(t\)</span> distribution is completely specified by its degrees of freedom (df) and has more area in its tails than the standard normal distribution.</p>
<p>The formula for calculating a <strong>one-sample t interval for a mean</strong> is <span class="math display">\[\bar{x}\pm t^\ast\frac{s_x}{\sqrt{n}}\]</span> where <span class="math inline">\(\bar{x}\)</span> is the sample mean, <span class="math inline">\(t^\ast\)</span> is the critical value, <span class="math inline">\(s_x\)</span> is the sample standard deviation, and <span class="math inline">\(n\)</span> is the sample size. To find the critical value, use technology or Table B to determine the values of <span class="math inline">\(-t^\ast\)</span> and <span class="math inline">\(t^\ast\)</span> that capture the middle <span class="math inline">\(C\%\)</span> of a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(\text{df}=n-1\)</span>, where <span class="math inline">\(C\)</span> is the confidence level.</p>
<p>In this section, you also learned how to analyze <strong>paired data</strong>, which result from measuring the same quantitative variable twice for each individual or once for each of two very similar individuals. Start by finding the difference between the values in each pair. Then make a graph of the differences. Use the mean difference <span class="math inline">\(\bar{x}_{\text{Diff}}\)</span> and the standard deviation of the differences <span class="math inline">\(s_{\text{Diff}}\)</span> as summary statistics.</p>
<p>Three conditions need to be verified before calculating a confidence interval for a population mean difference <span class="math inline">\(\mu_{\text{Diff}}\)</span>. The Random condition says that paired data must come from a random sample from the population of interest or from a randomized experiment. The <span class="math inline">\(10\%\)</span> condition says that the sample size <span class="math inline">\(n_{\text{Diff}}\)</span> should be less than <span class="math inline">\(10\%\)</span> of the size of the corresponding population of differences <span class="math inline">\(N_{\text{Diff}}\)</span> when sampling without replacement. The Normal/Large Sample condition says that the population distribution of differences is approximately normal or the sample size is large (<span class="math inline">\(n_{\text{Diff}} \ge 30\)</span>). If the number of differences is small and the population distribution’s shape is unknown, graph the difference values to make sure there is no strong skewness or outliers.</p>
<p>Once you have computed the differences, calculations proceed as in a one-sample <span class="math inline">\(t\)</span> interval for a population mean. Find the critical value <span class="math inline">\(t^\ast\)</span> using a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(\text{df}=n_{\text{Diff}}-1\)</span>. The formula for the <strong>paired <span class="math inline">\(t\)</span> interval for a mean difference</strong> is <span class="math display">\[\bar{x}_{\text{Diff}} \pm t^\ast\frac{s_{\text{Diff}}}{\sqrt{n_{\text{Diff}}}}\]</span></p>
<p>Always use the four-step process whenever you are asked to construct and interpret a confidence interval for a population mean <span class="math inline">\(\mu\)</span> or a population mean difference <span class="math inline">\(\mu_{\text{Diff}}\)</span>.</p>
<p>The logic of confidence intervals, including how to interpret the confidence interval and the confidence level, is the same as in Unit 6. Likewise, you can use a confidence interval for <span class="math inline">\(\mu\)</span> or <span class="math inline">\(\mu_{\text{Diff}}\)</span> to evaluate a claim about the population mean or mean difference. As with other intervals, increasing confidence level leads to a larger margin of error and increasing the sample size leads to a smaller margin of error, assuming other things remain the same.</p>
</section>
<section id="section-7b-significance-tests-for-a-population-mean-or-mean-difference-1" class="level4">
<h4 class="anchored" data-anchor-id="section-7b-significance-tests-for-a-population-mean-or-mean-difference-1">Section 7B: Significance Tests for a Population Mean or Mean Difference</h4>
<p>In this section, you learned how to perform a significance test about a population mean. As in any test, you start by stating hypotheses. The null hypothesis is <span class="math inline">\(H_0: \mu=\mu_0\)</span>. The alternative hypothesis can be one-sided (<span class="math inline">\(\lt\)</span> or <span class="math inline">\(\gt\)</span>)or two-sided (<span class="math inline">\(\ne\)</span>).</p>
<p>Next, we check three conditions to verify independence in data collection and confirm that the sampling distribution of <span class="math inline">\(\bar{x}\)</span> is approximately normal. The Random, 10%, and Normal/Large Sample conditions for significant tests about <span class="math inline">\(\mu\)</span> are the same as the ones for confidence intervals for a population mean.</p>
<p>For a test of <span class="math inline">\(H_0: \mu=\mu_0\)</span> when the population standard deviation <span class="math inline">\(\sigma\)</span> is unknown, the standardized test statistic is <span class="math display">\[t = \frac{\bar{x}-\mu_0}{\frac{s_x}{\sqrt{n}}}\]</span> When the conditions are met, the <span class="math inline">\(P\)</span>-value can be obtained from a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(\text{df}=n-1\)</span>. Use the <span class="math inline">\(P\)</span>-value to make an appropriate conclusion about the hypotheses, in context. This inference method is called a <strong>one-sample <span class="math inline">\(t\)</span> test for a mean</strong>.</p>
<p>In this section, you also learned how to perform a significance test about a population mean difference <span class="math inline">\(\mu_{\text{Diff}}\)</span> in the special case paired data. It is very similar to the one sample <span class="math inline">\(t\)</span> test for a population mean except that you use the difference in each pair as the sample data the new hypothesis is usually <span class="math inline">\(H_0: \mu_{\text{Diff}}=0\)</span>. The Random, <span class="math inline">\(10\%\)</span>, and Normal/Large Sample conditions are the same for significance tests about <span class="math inline">\(\mu_{\text{Diff}}\)</span> as confidence intervals about a population mean difference.</p>
<p>For a test of <span class="math inline">\(H_0: \mu_{\text{Diff}}=0\)</span>, the standardized test statistic is <span class="math display">\[t = \frac{\bar{x}_{\text{Diff}}-0}{\frac{s_{\text{Diff}}}{\sqrt{n_{\text{Diff}}}}}\]</span> When the conditions are met, the <span class="math inline">\(P\)</span>-value can be obtained from a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(\text{df}=n_{\text{Diff}}-1\)</span>. This inference method is called a <strong>paired <span class="math inline">\(t\)</span> test for a mean difference</strong>.</p>
<p>Whenever you were asked if there is convincing evidence for a claim about a population mean or mean difference, be sure to follow the four-step process.</p>
<p>A significance test for a population mean or mean difference uses the same logic as the significance tests for a population proportion from Section 6D. Likewise, the way we interpret <span class="math inline">\(P\)</span>-values and power and the way we describe Type I and Type II errors are very similar to what you learned in Unit 6.</p>
</section>
<section id="section-7c-confidence-intervals-for-a-difference-in-population-means-1" class="level4">
<h4 class="anchored" data-anchor-id="section-7c-confidence-intervals-for-a-difference-in-population-means-1">Section 7C: Confidence Intervals for a Difference in Population Means</h4>
<p>In this section, you learned how to construct confidence intervals for the difference <span class="math inline">\(\mu_1-\mu_2\)</span> between the means of two populations or treatments. To verify independence and data collection and confirm that the sampling distribution of <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span> is approximately normal, we check three conditions. The Random condition says that the data must be from two independent random samples or two groups in a randomized experiment. The <span class="math inline">\(10\%\)</span> condition says that each sample size should be less than <span class="math inline">\(10\%\)</span> of the corresponding population size when sampling without replacement. The Normal/Large Sample condition says that for each sample, the data come from an approximately normally distributed population or the sample size is large (<span class="math inline">\(n \ge 30\)</span>). For each sample, if the population distribution has unknown shape and <span class="math inline">\(n \lt 30\)</span>, confirm that graph of the sample data shows no strong skewness or outliers.</p>
<p>Because we rarely know the two population standard deviations, we use a <span class="math inline">\(t\)</span> distribution with degrees of freedom calculated by technology to determine the critical value <span class="math inline">\(t^\ast\)</span> for a given confidence level <span class="math inline">\(C\)</span>. Be sure to choose the technology’s unpooled option when constructing a confidence interval for a difference between two means. Also, note that the df is usually not a whole number. The formula for the <strong>two-sample <span class="math inline">\(t\)</span> interval for a difference in means</strong> is <span class="math display">\[(\bar{x}_1-\bar{x}_2)\pm t^\ast\sqrt{\frac{s^2_1}{n_1}+\frac{s^2_2}{n_2}}\]</span> The logic of confidence intervals, including how to interpret the confidence interval and the confidence level, is the same as when estimating a single population mean. Likewise, you can use a confidence interval for a difference in means to evaluate claims about the population means. For example, if <span class="math inline">\(0\)</span> is not included in a confidence interval for <span class="math inline">\(\mu_1-\mu_2\)</span>, there is convincing evidence that the population means are different.</p>
</section>
<section id="section-7d-significance-tests-for-a-difference-in-population-means-1" class="level4">
<h4 class="anchored" data-anchor-id="section-7d-significance-tests-for-a-difference-in-population-means-1">Section 7D: Significance Tests for a Difference in Population Means</h4>
<p>In this section, you learned how to perform significance tests for the difference between the means of two populations or treatments. As in any test, you start by stating hypotheses. The null hypothesis is usually <span class="math inline">\(H_0: \mu_1-\mu_2=0\)</span> (or equivalently, <span class="math inline">\(H_0: \mu_1=\mu_2\)</span>). The alternative hypothesis can be one-sided (<span class="math inline">\(\lt\)</span> or <span class="math inline">\(\gt\)</span>)or two-sided (<span class="math inline">\(\ne\)</span>).</p>
<p>Next, we need to verify independence and data collection and confirm that the sampling distribution of <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span> is approximately normal. The Random, <span class="math inline">\(10\%\)</span>, and Normal/Large Sample conditions for significance tests about a difference in means <span class="math inline">\(\mu_1-\mu_2\)</span> are the same as the conditions for confidence intervals about a difference in means.</p>
<p>For a test of <span class="math inline">\(H_0: \mu_1-\mu_2=0\)</span>, the standardized test statistic is <span class="math display">\[t=\frac{(\bar{x}_1-\bar{x}_2)-0}{ \sqrt{\frac{s^2_1}{n_1}+\frac{s^2_2}{n_2}}}\]</span> When the conditions are met, <span class="math inline">\(P\)</span>-values can be obtained using a <span class="math inline">\(t\)</span> distribution with degrees of freedom calculated by technology. This inference method is called a <strong>two-sample <span class="math inline">\(t\)</span> test for a difference in means</strong>. Be sure to choose the technology’s unpooled option when performing a significance test for a difference between two population means.</p>
<p>To decide whether a paired <span class="math inline">\(t\)</span> procedure for a population mean difference <span class="math inline">\(\mu_{\text{Diff}}\)</span> or a two-sample <span class="math inline">\(t\)</span> procedure for a difference between two means <span class="math inline">\(\mu_1-\mu_2\)</span> is appropriate in a given setting, consider how the data were produced.</p>
<p>Remember to use significance tests wisely. Also, remember that statistically significant results aren’t always practically important. Finally, be aware that the probability of making at least one Type I error goes up dramatically when conducting multiple tests.</p>
</section>
</section>
<section id="inference-for-means" class="level3">
<h3 class="anchored" data-anchor-id="inference-for-means">Inference for Means</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 26%">
<col style="width: 26%">
<col style="width: 26%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Parameter</strong></td>
<td style="text-align: left;"><span class="math inline">\(\mu\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mu_{\text{Diff}}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mu_1-\mu_2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Conditions</strong></td>
<td style="text-align: left;"><strong>Random</strong>: The data come from a random sample from the population of interest.<br><br> <strong><span class="math inline">\(10\%\)</span></strong>: When sampling without replacement, <span class="math inline">\(n \lt 0.10N\)</span>.<br><br> <strong>Normal/Large Sample</strong>: The population distribution is approximately normal or the sample size is large (<span class="math inline">\(n \ge 30\)</span>). If the population has unkown shape and <span class="math inline">\(n \lt 30\)</span>, a graph of the sample data shows no strong skewness or outliers.</td>
<td style="text-align: left;"><strong>Random</strong>: Paired data come from a random sample from the population of interest or from a randomized experiment.<br><br><strong><span class="math inline">\(10\%\)</span></strong>: when sampling without replacement, <span class="math inline">\(n_{\text{Diff}} \lt 0.10N_{\text{Diff}}\)</span>.<br><br><strong>Normal/Large Sample</strong>: The population distribution of differences is approximately normal or the sample size is large (<span class="math inline">\(n_{\text{Diff}} \ge 30\)</span>). If the population distribution of differences has unknown shape and <span class="math inline">\(n_{\text{Diff}} \lt 30\)</span>, a graph of the sample differences shows no strong skewness or outliers.</td>
<td style="text-align: left;"><strong>Random</strong>: The data come from two independent random samples or from two groups in a randomized experiment.<br><br><strong><span class="math inline">\(10\%\)</span></strong>: When sampling without replacement, <span class="math inline">\(n_1 \lt 0.10N_1 \text{ and } n_2 \lt 0.10N_2\)</span>.<br><br><strong>Normal/Large Sample</strong>: For each sample, the data come from an approximately normally distributed population or the sample size is large (<span class="math inline">\(n \ge 30\)</span>). For each sample, if the population distribution has unknown shape and <span class="math inline">\(n \lt 30\)</span>, a graph of the sample data shows no strong skewness or outliers.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Parameter</strong></td>
<td style="text-align: left;"><span class="math inline">\(\mu\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mu_{\text{Diff}}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mu_1-\mu_2\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Name (TI-83/84)</strong></td>
<td style="text-align: left;">One-sample <span class="math inline">\(t\)</span> interval for a mean<br>(TInterval)</td>
<td style="text-align: left;">Paired <span class="math inline">\(t\)</span> interval for a mean difference<br>(TInterval)</td>
<td style="text-align: left;">Two-sample <span class="math inline">\(t\)</span> interval for a difference in means<br>(2-SampTInt)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Formula</strong></td>
<td style="text-align: left;"><span class="math display">\[\bar{x}\pm t^\ast\frac{s_x}{\sqrt{n}}\]</span> Critical value from <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(\text{df} = n-1\)</span></td>
<td style="text-align: left;"><span class="math display">\[\bar{x}_{\text{Diff}} \pm t^\ast\frac{s_{\text{Diff}}}{\sqrt{n_{\text{Diff}}}}\]</span> Critical value from <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(\text{df} = n_{\text{Diff}}-1\)</span></td>
<td style="text-align: left;"><span class="math display">\[(\bar{x}_1-\bar{x}_2)\pm t^\ast\sqrt{\frac{s^2_1}{n_1}+\frac{s^2_2}{n_2}}\]</span> Critical value from <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(\text{df}\)</span> from technology</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Parameter</strong></td>
<td style="text-align: left;"><span class="math inline">\(\mu\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mu_{\text{Diff}}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mu_1-\mu_2\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Name (TI-83/84)</strong></td>
<td style="text-align: left;">One-sample <span class="math inline">\(t\)</span> test for a mean<br>(T-Test)</td>
<td style="text-align: left;">Paired <span class="math inline">\(t\)</span> test for a mean difference<br>(T-Test)</td>
<td style="text-align: left;">Two-sample <span class="math inline">\(t\)</span> test for a difference in means<br>(2-SampTTest)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Null Hypothesis</strong></td>
<td style="text-align: left;"><span class="math inline">\(H_0: \mu=\mu_0\)</span></td>
<td style="text-align: left;"><span class="math inline">\(H_0: \mu_{\text{Diff}}=0\)</span></td>
<td style="text-align: left;"><span class="math inline">\(H_0: \mu_1-\mu_2=0\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Formula</strong></td>
<td style="text-align: left;"><span class="math display">\[t = \frac{\bar{x}-\mu_0}{\frac{s_x}{\sqrt{n}}}\]</span> <span class="math inline">\(P\)</span>-value from <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(\text{df} = n-1\)</span></td>
<td style="text-align: left;"><span class="math display">\[t = \frac{\bar{x}_{\text{Diff}}-0}{\frac{s_{\text{Diff}}}{\sqrt{n_{\text{Diff}}}}}\]</span> <span class="math inline">\(P\)</span>-value from <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(\text{df} = n_{\text{Diff}}-1\)</span></td>
<td style="text-align: left;"><span class="math display">\[t=\frac{(\bar{x}_1-\bar{x}_2)-0}{ \sqrt{\frac{s^2_1}{n_1}+\frac{s^2_2}{n_2}}}\]</span> <span class="math inline">\(P\)</span>-value from <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(\text{df}\)</span> from technology</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="unit-8-inference-for-categorical-data-chi-square-1" class="level1">
<h1>Unit 8: Inference for Categorical Data: Chi-Square</h1>
<section id="introduction-9" class="level3">
<h3 class="anchored" data-anchor-id="introduction-9">Introduction</h3>
<p>In Section 6C,you learned how to perform tests for the proportion of successes in a single population. These tests were based on a single categorical variable with values that were divided into two-categories: success and failure. Sometimes we want to perform a test for the distribution of a categorical variable with two <em>or more</em> categories. The <em>chi-square test for goodness of fit</em> allows us to determine whether a hypothesized distribution seems valid. This test is useful in fields such as genetics, where the laws of probability give the expected proportion of outcomes in each category.</p>
<p>In Section 2A, you learned how to analyze the relationship between two categorical variables. For instance, we used two-way tables and segmented bar graphs to show there was an association between environmental club membership and snowmobile use for a random sample of winter visitors to Yellowstone National Park. Do these data provide convincing evidence of an association between these two variables in the population of winter visitors to Yellowstone? We can answer this question with a <em>chi-square test for independence</em>.</p>
<p>Tests for independence use data that can be summarized in a two-way table. It is also possible to use the information in a two-way table to compare the distribution of a categorical variable for two or <em>more</em> populations or treatments, where the variable can have two <em>or more</em> categories. We can decide whether the distribution of a categorical variable differs for two or more populations or treatments using a <em>chi-square test for homogeneity</em>. This test will help us answer questions like “Does background music influence customer purchases?”</p>
</section>
<section id="section-8a-chi-square-tests-for-goodness-of-fit" class="level3">
<h3 class="anchored" data-anchor-id="section-8a-chi-square-tests-for-goodness-of-fit">Section 8A: Chi-Square Tests for Goodness of Fit</h3>
<ul>
<li>The <strong>chi-square test for goodness of fit</strong> tests the null hypothesis that a categorical variable has a specified distribution in the population of interest. The alternative hypothesis is that the variable does not have the specified distribution in the population of interest.<br>
</li>
<li>This test compares the <strong>observed count</strong> in each category with the counts that would be expected if <span class="math inline">\(H_0\)</span> were true. The <strong>expected count</strong> for any category is found by multiplying the sample size by the proportion specified by the null hypothesis for that category.<br>
</li>
<li>To ensure that the observations in the sample can be viewed as independent and that the sampling distribution of <span class="math inline">\(\chi^2\)</span> test statistic can be modeled by a chi-square distribution, the following conditions must be met:
<ul>
<li>Random: The data come from a random sample from the population of interest.
<ul>
<li><span class="math inline">\(10\%\)</span>: When sampling without replacement, <span class="math inline">\(n \lt 0.10N\)</span>.<br>
</li>
</ul></li>
<li>Large Counts: All expected counts are at least 5.<br>
</li>
</ul></li>
<li>The <strong>chi-square test statistic <span class="math inline">\(\chi^2\)</span></strong> is <span class="math display">\[\chi^2=\sum{\frac{(\text{Observed - Expected})^2}{\text{Expected}}}\]</span> where Observed is the observed count, Expected is the expected count, and the sum is over all possible categories.<br>
</li>
<li>Large values of <span class="math inline">\(\chi^2\)</span> are evidence against <span class="math inline">\(H_0\)</span> and in favor of <span class="math inline">\(H_a\)</span>. The <span class="math inline">\(P\)</span>-value is the area to the right of <span class="math inline">\(\chi^2\)</span> under the chi-square density curve with degrees of freedom .</li>
<li>If the test finds a s<span class="math inline">\(\text{df = number of categories - 1}\)</span>tatistically significant result, consider doing a <em>follow-up analysis</em> that identifies the largest contributions to the chi-square test statistic and compares the observed and expected counts in the identified categories.</li>
</ul>
</section>
<section id="section-8b-chi-square-tests-for-independence-or-homogeneity" class="level3">
<h3 class="anchored" data-anchor-id="section-8b-chi-square-tests-for-independence-or-homogeneity">Section 8B: Chi-Square Tests for Independence or Homogeneity</h3>
<ul>
<li>We use the <strong>chi-square test for independence</strong> to test for an association between two-categorical variables in a population of interest.<br>
</li>
<li>The hypotheses for a test of independence are
<ul>
<li><span class="math inline">\(H_0\)</span>: There is no association between two categorical variables in the population of interest.<br>
</li>
<li><span class="math inline">\(H_a\)</span>: There is an association between two categorical variables in the population of interest.<br>
</li>
</ul></li>
<li>When performing a chi-square test for independence, we need to check that the observations in the sample can be viewed as independent and that the sampling distribution of the <span class="math inline">\(\chi^2\)</span> test statistic can be modeled by a chi-sqaure distribution. The required conditions are
<ul>
<li>Random: The data come from a random sample from the population of interest.
<ul>
<li><span class="math inline">\(10\%\)</span>: When sampling without replacement, <span class="math inline">\(n &lt; 0.10N\)</span>.<br>
</li>
</ul></li>
<li>Large Counts: All expected counts must be at least 5.<br>
</li>
</ul></li>
<li>The <strong>expected count</strong> in any cell of a two-way table when <span class="math inline">\(H_0\)</span> is true is <span class="math display">\[\text{expected count} = \frac{\text{(row total)}\text{(column total)}}{\text{table total}}\]</span><br>
</li>
<li>The chi-square test statistic is <span class="math display">\[\chi^2=\sum{\frac{(\text{Observed - Expected})^2}{\text{Expected}}}\]</span> where the sum is over all cells in the two-way table (not including the totals).<br>
</li>
<li>Calculate the <span class="math inline">\(P\)</span>-value by finding the area to the right of <span class="math inline">\(\chi^2\)</span> in a chi-square distribution with <span class="math inline">\(\text{df = (number of rows - 1)(number of columns - 1)}\)</span>.<br>
</li>
<li>We use the <strong>chi-square test for homogeneity</strong> to compare the distribution of a single categorical variable for each of several populations or treatments.<br>
</li>
<li>The hypotheses for a test for homogeneity are
<ul>
<li><span class="math inline">\(H_0\)</span>: There is no difference in the distributions of a categorical variable in the populations of interest or for the treatments in an experiment.<br>
</li>
<li><span class="math inline">\(H_a\)</span>: There is a difference in the distributions of a categorical variable in the populations of interest or for the treatments in an experiment.<br>
</li>
</ul></li>
<li>When performing a chi-square test for homogeneity, we need to check for independence and verify that the sampling distribution of the <span class="math inline">\(\chi^2\)</span> test statistic can be modeled by a chi-square distribution. The required conditions are
<ul>
<li>Random: The data come from independent random samples or groups in a randomized experiment.
<ul>
<li><span class="math inline">\(10\%\)</span>: When sampling without replacement, <span class="math inline">\(n &lt; 0.10N\)</span> for each sample.<br>
</li>
</ul></li>
<li>Large Counts: All expected counts must be at least 5.<br>
</li>
</ul></li>
<li>If a test for independence or a test for homogeneity finds a statistically significant result, consider doing a <em>follow up analysis</em> that looks for the largest components of the chi-square test statistic and compares the observed and expected counts in the corresponding cells.</li>
</ul>
</section>
<section id="unit-8-review" class="level3">
<h3 class="anchored" data-anchor-id="unit-8-review">Unit 8 Review</h3>
<section id="section-8a-chi-square-tests-for-goodness-of-fit-1" class="level4">
<h4 class="anchored" data-anchor-id="section-8a-chi-square-tests-for-goodness-of-fit-1">Section 8A: Chi-Square Tests for Goodness of Fit</h4>
<p>In this section, you learned how to perform a <strong>chi-square test for goodness of fit</strong>. The null hypothesis is that a single categorical variable follows a specified distribution in a population of interest. The alternative hypothesis is that the variable does not follow the specified distribution in the population of interest.</p>
<p>The <strong>chi-square test statistic <span class="math inline">\(\chi^2\)</span></strong> measures the difference between the observed distribution of a categorical variable and its hypothesized distribution. To calculate the chi-square test statistic, use the following formula with observed and expected counts: <span class="math display">\[\chi^2=\sum{\frac{(\text{Observed - Expected})^2}{\text{Expected}}}\]</span> To calculate the expected counts, multiply the sample size by the proportion specified by the null hypothesis for each category. Larger values of the chi-square test statistic provide more convincing evidence that the distribution of the categorical variable differs from the hypothesized distribution in the population of interest.</p>
<p>To ensure that the observations in the sample can be viewed as independent and that the sampling distribution of the chi-square test statistic can be modeled by a chi-square distribution, we check three conditions. The Random condition says that the data are from a random sample from the population of interest. The <span class="math inline">\(10\%\)</span> condition says that the sample size should be less than <span class="math inline">\(10\%\)</span> of the population size when sampling without replacement. The Large Counts condition says that the <em>expected</em> counts for each category must be at least five. In a test for goodness to fit, use a chi-square distribution with degrees of freedom = number of categories <span class="math inline">\(- 1\)</span>.</p>
<p>When the results of a test for goodness to fit are significant, consider doing a <em>follow up analysis</em>. Identify which categories of the variable made the largest contributions to the chi-square test statistic and whether the observed values in those categories were larger or smaller than expected.</p>
<p>A sky square test for goodness of it uses the same logic as the previously introduced significant tests. Likewise, the way we interpret <span class="math inline">\(P\)</span>-values and power and the way we describe Type I and Type II errors is very similar to what you learned earlier.</p>
</section>
<section id="section-8b-chi-square-tests-for-independence-or-homogeneity-1" class="level4">
<h4 class="anchored" data-anchor-id="section-8b-chi-square-tests-for-independence-or-homogeneity-1">Section 8B: Chi-Square Tests for Independence or Homogeneity</h4>
<p>In this section, you learn two different tests to analyze categorical data that are summarized in a two-way table. A <strong>chi-square test for independence</strong> looks for an association between two categorical variables in a single population. A <strong>chi-square test for homogeneity</strong> compares the distribution of a single categorical variable for two or more populations or treatments.</p>
<p>In a chi-square test for independence, the null hypothesis is that there is no association between two categorical variables in one population (or that the two variables are independent in the population). The alternative hypothesis is that there is an association between the two variables (or that the two variables are not independent).</p>
<p>As with other significance tests, we need to verify that the observations in the sample can be viewed as independent and that the sampling distribution of the chi-square test statistic can be modeled by a chi-square distribution. The conditions for a chi-square test for independence are exactly the same as the conditions for a chi-square test for goodness of fit. However, there is a new method for calculating the expected counts: <span class="math display">\[\text{expected count} = \frac{\text{(row total)}\text{(column total)}}{\text{table total}}\]</span> To calculate the P value, compute the chi-square test statistic and use a high square distribution with degrees of freedom = (number of rows - 1)(number of columns - 1).</p>
<p>In a chi-square test for homogeneity, the null hypothesis is that there is no difference in the distribution of a categorical variable for two or more populations or treatments.the alternative hypothesis is that there is a difference in the distributions.</p>
<p>The conditions are slightly different, but the purpose for checking them is still the same: to ensure independence in the data collection process and verify that the sampling distribution of the chi-square test statistic can be modeled by a chi-square distribution. The Random Condition is that the data come from independent random samples from groups in a randomized experiment. The <span class="math inline">\(10\%\)</span> condition applies for each sample when sampling without replacement, but not for experiments when there is no sampling without replacement from a population. Finally the large counts condition remains the same – the expected counts must be at least five in each cell of the two-way table. The methods for calculating expected counts, the chi-square test statistic, the degrees of freedom, and the <span class="math inline">\(P\)</span>-value are exactly the same in a test for homogeneity as in the test for independence.</p>
<p>As with tests for goodness to fit, when the results of a test for homogeneity or a test for independence are significant, consider doing a follow up analysis. Identify which cells in the two-way table made the largest contributions to the chi-square test statistic and whether the observed counts in those cells were larger or smaller than expected.</p>
<p>The chi-square tests for independence and homogeneity use the same logic at the previously discussed significance tests. Likewise, the way we interpret <span class="math inline">\(P\)</span>-values and power and the way we describe Type I and Type II errors is very similar to what you learned in previous units.</p>
</section>
</section>
<section id="comparing-the-three-chi-square-tests" class="level3">
<h3 class="anchored" data-anchor-id="comparing-the-three-chi-square-tests">Comparing the Three Chi-Square Tests</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 26%">
<col style="width: 26%">
<col style="width: 26%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Number of samples/treatments</strong></td>
<td style="text-align: left;"><span class="math inline">\(1\)</span></td>
<td style="text-align: left;"><span class="math inline">\(1\)</span></td>
<td style="text-align: left;"><span class="math inline">\(2\)</span> or more</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Null Hypothesis</strong></td>
<td style="text-align: left;">The stated distribution of a categorical variable in the population of interest is correct.</td>
<td style="text-align: left;">There is no association between two categorical variables in the population of interest.</td>
<td style="text-align: left;">There is no difference in the distribution of a categorical variable for several populations or treatments.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Random condition</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">The data come from a random sample from the population of interest.</td>
<td style="text-align: left;">The data come from independent random samples or groups in a randomized experiment.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong><span class="math inline">\(10\%\)</span> condition</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">When sampling without replacement, <span class="math inline">\(n &lt; 0.10N\)</span> for each sample.</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Large Counts condition</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">All expected counts <span class="math inline">\(\ge 5\)</span>.</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Expected Counts</strong></td>
<td style="text-align: left;"><span class="math inline">\(np_i\)</span><br><br>where <span class="math inline">\(p_i\)</span> is the proportion specified by the null hypothesis for a particular category</td>
<td style="text-align: left;"><span class="math display">\[\frac{\text{(row total)}\text{(column total)}}{\text{table total}}\]</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Formula for test statistic</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math display">\[\chi^2=\sum{\frac{(\text{Observed - Expected})^2}{\text{Expected}}}\]</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Degrees of freedom</strong></td>
<td style="text-align: left;"># categories - 1</td>
<td style="text-align: left;">(# rows - 1)(# columns - 1)</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Name (TI-83/84)</strong></td>
<td style="text-align: left;"><span class="math inline">\(\chi^2\)</span>GOF-test</td>
<td style="text-align: left;"><span class="math inline">\(\chi^2\)</span>-test</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>